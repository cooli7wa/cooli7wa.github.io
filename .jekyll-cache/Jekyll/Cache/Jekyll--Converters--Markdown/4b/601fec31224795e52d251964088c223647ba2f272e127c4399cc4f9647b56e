I"æ<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

<p><a href="http://neuralnetworksanddeeplearning.com/chap2.html">åŸæ–‡åœ°å€</a></p>

<h3 id="warm-up-a-fast-matrix-based-approach-to-computing-the-output-from-a-neural-network">Warm up: a fast matrix-based approach to computing the output from a neural network</h3>

<h3 id="the-two-assumptions-we-need-about-the-cost-function">The two assumptions we need about the cost function</h3>

<p>ä¸¤ä¸ªå‡è®¾ï¼š</p>
<ul>
  <li>
\[C=\frac{1}{2n}\sum_x \left \| y(x)-a^{L}(x) \right \|^{2}â€‹\]

    <p>the cost function can be written as an average \(C=\frac{1}{n}\sum_{x}C_{x}\)over cost functions Cx for individual training examples, x.</p>

\[C_{x}=\frac{1}{2}\left\| y-a^{L} \right\|^{2}\]
  </li>
  <li>
    <p>the cost is that it can be written as a function of the outputs from the neural network</p>

    <p><img src="{{site.baseurl}}/images/md/chap2_cost_1.png" style="zoom:80%" /></p>

\[C_{x}=\frac{1}{2}\left \| y-a^{L}\right \|^{2}=\frac{1}{2}\sum_{j}(y_{j}-a_{L}^{j})^{2}\]
  </li>
</ul>

<p>æ€»ä½“çš„Costï¼Œä¾èµ–äºæ‰€æœ‰çš„è¾“å…¥xï¼ˆå¯¹äºï¼‰å’Œæ¯ä¸ªè¾“å…¥xçš„æ‰€æœ‰çš„output a</p>

<h3 id="the-hadamard-product-st">The Hadamard product, sâŠ™t</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre></td><td class="rouge-code"><pre><span class="c1"># ä¸çŸ©é˜µä¹˜æ³•ä¸åŒ
# è¿™ä¸ªçš„å†™æ³•æ˜¯
</span><span class="n">a</span><span class="o">*</span><span class="n">b</span>
<span class="c1"># çŸ©é˜µä¹˜æ³•çš„å†™æ³•æ˜¯
</span><span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<h3 id="the-four-fundamental-equations-behind-backpropagation">The four fundamental equations behind backpropagation</h3>

<p>ä¸€äº›åŸºç¡€ç‚¹ï¼š</p>
<ul>
  <li>
\[\delta_{j}^{l}\equiv \frac{\partial C}{\partial z_{j}^{l}}\]

    <p>the error in the \(j_{th}\) neuron in the \(l_{th}\) layer
add a little\(\bigtriangleup z_{j}^{l}\), so output \(\sigma(z_{j}^{l}) \rightarrow\sigma(z_{j}^{l}+\bigtriangleup z_{j}^{l})\)</p>

    <p>æœ€ç»ˆçš„outputä¸º\(\frac{\partial C}{\partial z_{j}^{l}}\bigtriangleup z_{j}^{l}\)</p>
  </li>
  <li>\(z_{j}^{L}\)å¹¶ä¸æ˜¯ç¥ç»å…ƒè¾“å‡ºï¼Œ\(\sigma(z_{j}^{L})\)æ‰æ˜¯</li>
</ul>

<h4 id="ç¬¬ä¸€ä¸ªåŸºç¡€å…¬å¼an-equation-for-the-error-in-the-output-layer">ç¬¬ä¸€ä¸ªåŸºç¡€å…¬å¼(An equation for the error in the output layer)ï¼š</h4>

<p>ä¸‹é¢çš„æ¨å¯¼æ˜¯é’ˆå¯¹outputå±‚æ¥è¯´</p>

\[\delta_{j}^{L}=\frac{\partial C}{\partial z_{j}^{L}}\rightarrow \sum_{k}\frac{\partial C}{\partial a_{k}^{L}}\frac{\partial a_{k}^{L}}{\partial z_{j}^{L}}\rightarrow \frac{\partial C}{\partial a_{j}^{L}}\frac{\partial a_{j}^{L}}{\partial z_{j}^{L}}\rightarrow \frac{\partial C}{\partial a_{j}^{L}}\sigma'(z_{j}^{L})\]

<p>Of course, the output activation  \(a_{k}^{L}\) of the \(k^{th}\) neuron depends only on the weighted input \(z_{j}^{L}\) for the \(j^{th}\) neuron when k=j.</p>

<p>ç°åœ¨è®¨è®ºçš„æ˜¯è¾“å‡ºå±‚çš„çŠ¶æ€ï¼Œæ‰€ä»¥\(a_{j}\)ï¼ˆä¹Ÿå°±æ˜¯\(\sigma\)ï¼‰åªä¸\(z_{j}\)æœ‰å…³ã€‚</p>

\[C=\frac{1}{2}\sum_{j}(y_{j}-a_{j}^{L})^{2} \rightarrow \partial C/\partial a_{j}^{L}=(a_{j}^{L}-y_{j})\]

\[(y_{j}-a_{j}^{L})(y_{j}-a_{j}^{L})'= (a_{j}^{L}-y_{j})\]

\[\delta^{L}=\bigtriangledown_{a}C\odot \sigma '(z^{L})\]

<p>å½“lossæ˜¯å‡æ–¹å·®çš„æ—¶å€™ï¼Œå¯ä»¥åŒ–ç®€ä¸ºä¸‹é¢è¿™ä¸ª</p>

\[\delta^{L}=(a^{L}-y)\odot \sigma '(z^{L})\]

<p>è¿™é‡Œçš„âŠ™éœ€è¦æ³¨æ„ä¸‹ï¼Œè¿™ä¸ªä¸æ˜¯ä¸€èˆ¬çš„çŸ©é˜µä¹˜æ³•</p>

<h4 id="ç¬¬äºŒä¸ªå…¬å¼an-equation-for-the-error-Î´lÎ´l-in-terms-of-the-error-in-the-next-layer">ç¬¬äºŒä¸ªå…¬å¼ï¼ˆAn equation for the error Î´lÎ´l in terms of the error in the next layerï¼‰</h4>

\[\delta^{L}=((w^{l+1})^{T}\delta ^{l+1})\odot \sigma '(z^{L})\]

<p>æ¨å¯¼çš„è¿‡ç¨‹æ–‡ç« ä¸‹é¢ä»‹ç»å¾ˆè¯¦ç»†</p>

<h4 id="ç¬¬ä¸‰ä¸ªå…¬å¼an-equation-for-the-rate-of-change-of-the-cost-with-respect-to-any-bias-in-the-network">ç¬¬ä¸‰ä¸ªå…¬å¼ï¼ˆAn equation for the rate of change of the cost with respect to any bias in the networkï¼‰</h4>

\[\frac{\partial C}{\partial b^l_j}=\delta _j^l\]

<h4 id="ç¬¬å››ä¸ªå…¬å¼an-equation-for-the-rate-of-change-of-the-cost-with-respect-to-any-weight-in-the-network">ç¬¬å››ä¸ªå…¬å¼ï¼ˆAn equation for the rate of change of the cost with respect to any weight in the networkï¼‰</h4>

\[\frac{\partial C}{\partial w_{jk}^{l}}=a_{k}^{l-1}\delta_{j}^{l}\rightarrow \frac{\partial C}{\partial w}=a_{in}\delta _{out}\]

<p>A nice consequence of Equation (32) is that when the activation \(a_{in}\) is small, \(a_{in}\)â‰ˆ0, the gradient term âˆ‚C/âˆ‚w will also tend to be small.</p>

<p>In this case, weâ€™ll say the weight learns slowly, meaning that itâ€™s not changing much during gradient descent.</p>

<p>å½“\(a_{in}\)å¾ˆå°çš„æ—¶å€™ï¼Œå‚æ•°çš„å­¦ä¹ å¾ˆæ…¢</p>

<p>so the lesson is that a weight in the final layer will learn slowly if the output neuron is either low activation (â‰ˆ0) or high activation (â‰ˆ1). In this case itâ€™s common to say the output neuron has saturated and, as a result, the weight has stopped learning (or is learning slowly).</p>

<p>å½“ä½¿ç”¨sigmoidæ¿€æ´»å‡½æ•°çš„æ—¶å€™ï¼Œå¦‚æœç¥ç»å…ƒçš„è¾“å‡ºæ¥è¿‘0æˆ–1çš„æ—¶å€™ï¼Œå­¦ä¹ ä¹Ÿå¾ˆæ…¢</p>

<p>Summing up, weâ€™ve learnt that a weight will learn slowly if either the input neuron is low-activation, or if the output neuron has saturated, i.e., is either high- or low-activation.</p>

<p><img src="{{site.baseurl}}/images/md/chap2_4bp.png" alt="" /></p>

<p><img src="{{site.baseurl}}/images/md/chap2_train_in_minibatch.png" alt="" /></p>

<h3 id="fully-matrix-based-approach-to-backpropagation-over-a-mini-batch">Fully matrix-based approach to backpropagation over a mini-batch</h3>

<p>è¿™æ˜¯ä¸€ç§æå‡é€Ÿåº¦çš„æ–¹å¼ï¼Œåœ¨tensorflowé‡Œé¢ï¼Œåº”è¯¥å·²ç»åœ¨ä½¿ç”¨äº†ã€‚</p>

<p>å¯ä»¥è¯•è¯•åœ¨tensorflowé‡Œé¢ï¼Œè¿™ä¸ªé€Ÿåº¦å¿«å¤šå°‘ï¼Ÿ</p>

<p>å†™äº†ä¸€ä»½ä»£ç ï¼Œè·‘èµ·æ¥ä½¿ç”¨GPU+tensorflowï¼Œé€Ÿåº¦ä¹Ÿæ²¡æœ‰æå‡å¤ªå¤šï¼ˆæ¯”å¦‚10å€ï¼‰</p>

<p>0:04:00.804773 åŸæ¥çš„</p>

<p>0:02:45.739480 ç°åœ¨çš„</p>

<p>ä»£ç ï¼šmnist_tensor_mine/src/tensor_network.py</p>

<h3 id="in-what-sense-is-backpropagation-a-fast-algorithm">In what sense is backpropagation a fast algorithm?</h3>

<p>å¦‚ä¹‹å‰è§†é¢‘é‡Œè®²è§£çš„ä¸€æ ·ï¼Œåå‘ä¼ æ’­ï¼Œå°†éœ€è¦è®¡ç®—é‡å¤§ä¸ºç¼©å‡ï¼Œä¸æ­£å‘ä¼ æ’­åŸºæœ¬è®¡ç®—é‡ç›¸ç­‰ã€‚</p>

<p>ä½†æ˜¯è¿™ä¸ªæ˜¯æœ‰å‰æçš„ï¼Œå°±æ˜¯ç°åœ¨è®¡ç®—çš„æ˜¯ï¼Œå¤§é‡çš„è¾“å…¥ï¼Œä¸€ä¸ªè¾“å‡ºï¼ˆæ¯”å¦‚å°±æ˜¯æ˜¯å¦ç¬¦åˆé¢„æœŸï¼‰ï¼Œåå‘ä¼ æ’­æ˜¯é€‚ç”¨çš„ï¼Œä½†æ˜¯æœ‰çš„æ¨¡å‹æ˜¯ï¼Œä¸€ä¸ªè¾“å…¥ï¼Œå¤§é‡çš„è¾“å‡ºï¼Œè¿™æ—¶ï¼Œæ­£å‘ä¼ æ’­æ‰æ˜¯åˆé€‚çš„</p>

<h3 id="backpropagation-the-big-picture">Backpropagation: the big picture</h3>

<h3 id="è½¬æ¢æˆtensorflowä¹‹åçš„æµ‹è¯•æƒ…å†µ">è½¬æ¢æˆtensorflowï¼Œä¹‹åçš„æµ‹è¯•æƒ…å†µ</h3>

<p>æœ‰ä¸€äº›å‡†å¤‡è¿‡ç¨‹ï¼š</p>
<ul>
  <li>å‚ç…§<a href="http://wiki.jikexueyuan.com/project/tensorflow-zh/tutorials/mnist_pros.html">è¿™ç¯‡æ–‡æ¡£</a>æ¥åšï¼Œæ•°æ®æ ¼å¼éœ€è¦å¤„ç†</li>
</ul>

<h3 id="batch_sizeçš„å½±å“">Batch_sizeçš„å½±å“</h3>

<p><img src="{{site.baseurl}}/images/md/chap2_batch_size_effect.jpg" alt="" /></p>

<ul>
  <li>Batch_Size å¤ªå°ï¼Œç®—æ³•åœ¨ 200 epoches å†…ä¸æ”¶æ•›ã€‚</li>
  <li>éšç€ Batch_Size å¢å¤§ï¼Œå¤„ç†ç›¸åŒæ•°æ®é‡çš„é€Ÿåº¦è¶Šå¿«ã€‚</li>
  <li>éšç€ Batch_Size å¢å¤§ï¼Œè¾¾åˆ°ç›¸åŒç²¾åº¦æ‰€éœ€è¦çš„ epoch æ•°é‡è¶Šæ¥è¶Šå¤šã€‚</li>
  <li>ç”±äºä¸Šè¿°ä¸¤ç§å› ç´ çš„çŸ›ç›¾ï¼Œ Batch_Size å¢å¤§åˆ°æŸä¸ªæ—¶å€™ï¼Œè¾¾åˆ°æ—¶é—´ä¸Šçš„æœ€ä¼˜ã€‚</li>
  <li>ç”±äºæœ€ç»ˆæ”¶æ•›ç²¾åº¦ä¼šé™·å…¥ä¸åŒçš„å±€éƒ¨æå€¼ï¼Œå› æ­¤ Batch_Size å¢å¤§åˆ°æŸäº›æ—¶å€™ï¼Œè¾¾åˆ°æœ€ç»ˆæ”¶æ•›ç²¾åº¦ä¸Šçš„æœ€ä¼˜ã€‚</li>
</ul>
:ET