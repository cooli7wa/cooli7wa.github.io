I"¿r<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

<p><a href="http://neuralnetworksanddeeplearning.com/chap3.html">åŸæ–‡åœ°å€</a></p>

<ul>
  <li>a better choice of cost function, known as the cross-entropy cost function</li>
  <li>four so-called â€œregularizationâ€ methods (L1 and L2 regularization, dropout, and artificial expansion of the training data)ï¼ŒL1 L2æ­£åˆ™åŒ–ã€dropoutã€è™šå‡æ‰©å±•æ•°æ®</li>
  <li>a better method for initializing the weights in the network</li>
  <li>a set of heuristics to help choose good hyper-parameters for the network</li>
</ul>

<p>The philosophy is that the best entree to the plethora of available techniques is in-depth study of a few of the most important.</p>

<p><strong>å¯¹ç°åœ¨è¿‡å¤šçš„æŠ€æœ¯ï¼Œæœ€å¥½çš„æ–¹å¼äº†ï¼Œæ·±å…¥ç ”ç©¶å‡ ä¸ªæœ€é‡è¦çš„æŠ€æœ¯</strong></p>

<h3 id="the-cross-entropy-cost-function">The cross-entropy cost function</h3>

<p>losså‡½æ•°æ˜¯å‡æ–¹å·®ï¼Œæ¿€æ´»å‡½æ•°æ˜¯sigmoidçš„æ—¶å€™</p>

\[\frac{\partial C}{\partial w}=(a-y)\sigma'(z)x=a\sigma'(z)\]

\[\frac{\partial C}{\partial b}=(a-y)\sigma'(z)=a\sigma'(z)\]

<p><img src="/images/md/chap1_sigmoid_1.png" alt="" /></p>

<p>å½“outputæ¥è¿‘0æˆ–1çš„æ—¶å€™ï¼Œå‡½æ•°å¾ˆå¹³å¦ï¼Œæ¢¯åº¦å°±å¾ˆå°ï¼Œå­¦ä¹ å¾ˆæ…¢</p>

<h3 id="introducing-the-cross-entropy-cost-function">Introducing the cross-entropy cost function</h3>

\[c=-\frac{1}{n}\sum_{x}[ylna+(1-y)ln(1-a)]\]

<p>ä»€ä¹ˆæ ·çš„äº¤å‰ç†µå¯ä»¥åšä¸ºæŸå¤±å‡½æ•°ï¼Ÿ</p>

<ul>
  <li>å‡½æ•°ç»“æœæ˜¯éè´Ÿçš„</li>
  <li>å½“å®é™…è¾“å‡ºç»“æœå’ŒæœŸå¾…çš„ç»“æœæ¥è¿‘çš„æ—¶å€™ï¼Œå‡½æ•°è¾“å‡ºåº”è¯¥æ¥è¿‘0
ï¼ˆå‡è®¾a=y=0æˆ–a=y=1ï¼‰</li>
</ul>

<p>ä»ä¸Šé¢è¿™ä¸¤ç‚¹çœ‹ï¼Œäº¤å‰ç†µæ˜¯åˆé€‚çš„</p>

<p>äº¤å‰ç†µä¸ºä»€ä¹ˆå¯ä»¥é˜²æ­¢è®­ç»ƒé€Ÿåº¦æ…¢ï¼Ÿ</p>

\[\frac{\partial C}{\partial w_{j}}=\frac{1}{n}\sum_{x}x_{j}(\sigma(z)-y)\]

\[\frac{\partial C}{\partial b}=\frac{1}{n}\sum_{x}(\sigma(z)-y)\]

<p>\(\sigma(z)-y\)å°±æ˜¯errorï¼Œæ‰€ä»¥errorè¶Šå¤§ï¼Œå­¦ä¹ é€Ÿç‡å°±è¶Šå¤§ï¼Œè¿™ä¸ªç¬¦åˆäººå­¦ä¹ çš„ç‰¹ç‚¹ã€‚</p>

<p>å¦å¤–äº¤å‰ç†µçš„cost/epochæ›²çº¿ï¼Œæ›´åŠ é™¡å³­</p>

<p>ä»€ä¹ˆæ—¶å€™ä½¿ç”¨äº¤å‰ç†µï¼Ÿ</p>

<p>å¦‚æœæ¿€æ´»å‡½æ•°æ˜¯sigmoidçš„è¯ï¼Œé‚£ä¹ˆæŸå¤±å‡½æ•°å°±åº”è¯¥æ˜¯äº¤å‰ç†µï¼Œå› ä¸ºå¯ä»¥æ”¾ç½®è®­ç»ƒè¿‡æ…¢çš„é—®é¢˜ã€‚å¯¹äºå…¶ä»–çš„æ¿€æ´»å‡½æ•°ï¼Œå¹¶æ²¡è¯´ã€‚</p>

\[C=-\frac{1}{n}\sum_{x}\sum_{j}[y_{j}lna^{L}_{j}+(1-y_{j})ln(1-a^{L}_{j})]\]

<p>è¿™ä¸ªå…¬å¼å’Œåˆ«çš„åœ°æ–¹çš„å‡½æ•°é‡Œç”¨çš„ä¸ä¸€æ ·ï¼Œè¿™é‡Œæ˜¯ä¸¤é¡¹ç›¸åŠ ï¼Œéƒ½å½“æˆäº†æ¦‚ç‡</p>

<p>å‡æ–¹å·®å¹¶ä¸æ˜¯æ€»æ˜¯ä¼šé€ æˆå­¦ä¹ é€Ÿç‡æ…¢çš„é—®é¢˜ï¼Œå½“æœ€åä¸€å±‚çš„ç¥ç»å…ƒæ˜¯çº¿æ€§çš„ï¼ˆä¹Ÿå°±æ˜¯æ²¡æœ‰sigmoidçš„æ¿€æ´»å‡½æ•°ï¼‰ï¼Œè¿™æ—¶çš„åå¯¼æ•°å°±ä¸ä¼šæœ‰è¿™ä¸ªé—®é¢˜ã€‚</p>

<p>æ‰€ä»¥æ¥çœ‹ï¼Œå­¦ä¹ é€Ÿç‡æ…¢çš„é—®é¢˜ï¼Œè²Œä¼¼åªåœ¨sigmoidé…åˆå‡æ–¹å·®çš„æ—¶å€™å‡ºç°ã€‚æ‰€ä»¥å‡æ–¹å·®ä¹Ÿä¸æ˜¯ç”¨åœ¨å“ªé‡Œéƒ½ä¸åˆé€‚ã€‚</p>

<h3 id="using-the-cross-entropy-to-classify-mnist-digits">Using the cross-entropy to classify MNIST digits</h3>

<p>cross-entropyå¯¹æ¯”quadratic cost</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
</pre></td><td class="rouge-code"><pre>åŸæ–‡
784, 30, 10
30, 10ï¼Œ0.5
    95.42 -&gt; 95.49

784, 100, 10
30, 10ï¼Œ0.5
    96.59 -&gt; 96.82
error: 3.41 -&gt; 3.18ï¼Œä¸‹é™äº†1/12ï¼Œä¸‹é™å¾ˆå¤šäº†

è‡ªå·±
784, 30, 10
30, 10ï¼Œ0.5
    95.19 -&gt; 95.36

784, 100, 10
30, 10ï¼Œ0.5
    96.51 -&gt; 96.63
</pre></td></tr></tbody></table></code></pre></div></div>
<p>éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œè¿™é‡Œæ²¡æœ‰ä»”ç»†è°ƒå‚ï¼Œæ‰€ä»¥é€šè¿‡è¿™ä¸ªç»“æœç›´æ¥è¯´ï¼Œcross-entropyæ¯”quadraticè¦å¥½ï¼Œä¸ä¸¥è°¨ã€‚ä¸è¿‡ä½œè€…ä¹Ÿè¯´ï¼Œå®é™…ä¸Šç¡®å®è¦å¥½</p>

<p>ä¸ºä»€ä¹ˆå…³æ³¨æŸå¤±å‡½æ•°ï¼Ÿ</p>

<p>the more important reason is that neuron saturation is an important problem in neural nets</p>

<p>ä¸»è¦åŸå› æ˜¯ï¼Œç¥ç»å…ƒçš„é¥±å’Œåº¦æ˜¯ä¸€ä¸ªç‰¹åˆ«é‡è¦çš„é—®é¢˜ï¼Œè¿™é‡Œå€¼å¾—èŠ±åŠ›æ°”æ¥ç ”ç©¶ï¼Œè¿™ä¸ªä¹Ÿåº”è¯¥æ˜¯æ·±åº¦å­¦ä¹ çš„ä¸»è¦æ–¹é¢ã€‚</p>

<h3 id="what-does-the-cross-entropy-mean-where-does-it-come-from">What does the cross-entropy mean? Where does it come from?</h3>

\[\sigma'(z)=\sigma(z)(1-\sigma(z))\]

<p>è¿™ä¸ªæ˜¯ä»\(\sigma(z)=\frac{1}{1+e^{-z}}\)æ±‚å¯¼è€Œæ¥</p>

\[\frac{\partial C}{\partial w_{j}}=\frac{1}{n}\sum_{x}x_{j}(\sigma(z)-y)\]

<p>ä»ä¸Šé¢çš„å…¬å¼å¯ä»¥çœ‹å‡ºæ¥ï¼Œxä¹Ÿæ˜¯å½±å“å­¦ä¹ é€Ÿç‡çš„ä¸€ä¸ªä¸»è¦å› ç´ ï¼Œå½“xæ¥è¿‘0çš„æ—¶å€™ï¼Œé€Ÿç‡ä¹Ÿå¾ˆæ…¢</p>

<h3 id="softmax">Softmax</h3>

<p>softmaxå¯ä»¥ç”¨æ¥è§£å†³å­¦ä¹ é€Ÿç‡æ…¢çš„é—®é¢˜ï¼Ÿ</p>

<p>è¿™é‡Œçš„softmaxå¹¶ä¸æ˜¯æˆ‘ç†è§£çš„é‚£æ ·ï¼Œåªæ˜¯ä¸€ä¸ªå½’ä¸€åŒ–çš„å¤„ç†è¿‡ç¨‹ã€‚æ–‡ç« è¿™é‡Œçš„softmaxæ˜¯ä½œä¸ºä¸€ä¸ªæ¿€æ´»å‡½æ•°æ¥ç”¨çš„ï¼Œç±»ä¼¼äºsigmoidã€‚</p>

<p>å½“costä¸º\(c\equiv -lna^{L}_{y}\)ï¼ˆè¿™ä¸ªå«åšlog-likelihood costï¼‰æ—¶ï¼Œå¯ä»¥æ¨åˆ°å‡ºæ¥ä¸‹é¢çš„å…¬å¼ï¼š</p>

\[\frac{\partial C}{\partial b^{L}_{j}}=a^{L}_{j}-y_{j}\]

\[\frac{\partial C}{\partial w^{L}_{jk}}=a^{L-1}_{k}(a^{L}_{j}-y_{j})\]

<p>ä»è¿™é‡Œå¯ä»¥çœ‹å‡ºæ¥ï¼Œè¿™ä¸ªä¸sigmoid+crossentropyæ˜¯ä¸€æ ·çš„</p>

<p>æ‰€ä»¥è¯´è¿™ä¸ªä¹Ÿè§£å†³äº†é€Ÿç‡æ…¢çš„é—®é¢˜ã€‚</p>

<p>wçš„è¯æ˜è¿‡ç¨‹å¦‚ä¸‹ï¼š</p>

\[\frac{\partial C}{\partial z^{l}_{j}}=\frac{\partial C}{\partial a}\frac{\partial a}{\partial z^{l}_{j}}=-\frac{1}{a^{l}_{j}}\frac{e^{z^{j}_{l}}\sum-(e^{z^{j}_{l}})^{2}}{\sum^{2}}=..=\frac{e^{z^{j}_{l}}-\sum}{\sum}=a^{l}_{j}-1=a^{l}_{j}-y_{j}\]

<p>log-likelihood costï¼Œå¯ä»¥ä½œä¸ºæŸå¤±å‡½æ•°ï¼Œå› ä¸ºè¿˜æ˜¯é‚£ä¹ˆå‡ ç‚¹</p>

<ul>
  <li>éè´Ÿçš„ï¼Œå½“\(a^{L}_{y}\)ä¸º1çš„æ—¶å€™ï¼Œcostå°±æ˜¯0ï¼Œä¹Ÿå°±æ˜¯ç¬¬yä¸ªç¥ç»å…ƒçš„è¾“å‡ºä¸æœŸå¾…1ç›¸ç¬¦ï¼Œæ²¡æœ‰åå·®</li>
  <li>å½“å®é™…è¾“å‡ºç»“æœå’ŒæœŸå¾…çš„ç»“æœæ¥è¿‘çš„æ—¶å€™ï¼Œå‡½æ•°è¾“å‡ºåº”è¯¥æ¥è¿‘0</li>
  <li>å½“ä¸æ¥è¿‘çš„æ—¶å€™ï¼Œè¾“å‡ºåº”è¯¥è¿œç¦»0</li>
  <li>æ³¨æ„\(c\equiv -lna^{L}_{y}\)è¿™é‡Œçš„yï¼Œæ˜¯æŒ‡è¾“å‡ºå±‚ç¬¬å‡ ä¸ªç¥ç»å…ƒçš„è¾“å‡ºï¼Œæ¯”å¦‚å¯¹äºmnistï¼Œå¦‚æœè¿™é‡Œè¦è®¡ç®—å›¾åƒä¸7çš„åå·®ï¼Œé‚£ä¹ˆè¿™é‡Œçš„yå°±æ˜¯7</li>
</ul>

<p>The fact that a softmax layer outputs a probability distribution is rather pleasing. In many problems itâ€™s convenient to be able to interpret the output activation \(a^{L}_{j}\) as the networkâ€™s estimate of the probability that the correct output is j.</p>

<p>softmaxçš„è¾“å‡ºå¯ä»¥ç†è§£ä¸ºæ˜¯ä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒï¼Œæ˜¯\(a^{L}_{j}\)æ˜¯jçš„æ¦‚ç‡ã€‚</p>

<p>a network with a sigmoid output layer, the output activations \(a^{L}_{j}\) wonâ€™t always sum to 1.</p>

<p>sigmoidçš„è¾“å‡ºï¼Œå¹¶ä¸æ˜¯ç›¸åŠ æ€»ä¸º1</p>

<p>You can think of softmax as a way of rescaling the \(z^{L}_{j}\), and then squishing them together to form a probability distribution.</p>

<p>sigmoidä½¿zæˆä¸ºä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒ</p>

<p>ç°åœ¨çŸ¥é“äº†ä¸¤ç§ç»„åˆï¼š</p>

<ul>
  <li>sigmoid + crossentropy</li>
  <li>softmax + log-likelihood</li>
</ul>

<h3 id="overfitting-and-regularization">Overfitting and regularization</h3>

<p>è¿™é‡Œçš„æµ‹è¯•ï¼Œæ˜¯æŒ‰ç…§ä¸‹é¢çš„æ¯”ä¾‹æ¥åšçš„ï¼Œtrain_dataåªæœ‰1000ï¼Œå› ä¸ºæ•°æ®å°‘äº†ï¼Œepochè®¾ç½®ä¸ºäº†400ï¼Œå…¶ä»–æ²¡å˜</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre></td><td class="rouge-code"><pre><span class="o">&gt;&gt;&gt;</span> <span class="kn">import</span> <span class="nn">mnist_loader</span> 
<span class="o">&gt;&gt;&gt;</span> <span class="n">training_data</span><span class="p">,</span> <span class="n">validation_data</span><span class="p">,</span> <span class="n">test_data</span> <span class="o">=</span> \
<span class="p">...</span> <span class="n">mnist_loader</span><span class="p">.</span><span class="n">load_data_wrapper</span><span class="p">()</span>
<span class="o">&gt;&gt;&gt;</span> <span class="kn">import</span> <span class="nn">network2</span> 
<span class="o">&gt;&gt;&gt;</span> <span class="n">net</span> <span class="o">=</span> <span class="n">network2</span><span class="p">.</span><span class="n">Network</span><span class="p">([</span><span class="mi">784</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">10</span><span class="p">],</span> <span class="n">cost</span><span class="o">=</span><span class="n">network2</span><span class="p">.</span><span class="n">CrossEntropyCost</span><span class="p">)</span> 
<span class="o">&gt;&gt;&gt;</span> <span class="n">net</span><span class="p">.</span><span class="n">large_weight_initializer</span><span class="p">()</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">net</span><span class="p">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">training_data</span><span class="p">[:</span><span class="mi">1000</span><span class="p">],</span> <span class="mi">400</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">evaluation_data</span><span class="o">=</span><span class="n">test_data</span><span class="p">,</span>
<span class="p">...</span> <span class="n">monitor_evaluation_accuracy</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">monitor_training_cost</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p><img src="/images/md/chap3_overfitting_1.png" style="zoom:100%" /></p>

<p><img src="/images/md/chap3_overfitting_2.png" alt="" /></p>

<p><img src="/images/md/chap3_overfitting_3.png" alt="" /></p>

<p>è¿™é‡Œæœ‰ä¸ªé—®é¢˜</p>

<ul>
  <li>ä»accuracyæ¥çœ‹ï¼Œoverfittingæ˜¯ä»280epochå¼€å§‹çš„</li>
  <li>ä»cost on test dataæ¥çœ‹ï¼Œoverfittingæ˜¯ä»15epochå¼€å§‹çš„</li>
</ul>

<p>é‚£ä¹ˆå“ªä¸ªæ‰æ˜¯çœŸæ­£çš„overfittingèµ·å§‹ç‚¹ï¼Ÿ</p>

<p>From a practical point of view, what we really care about is improving classification accuracy on the test data, while the cost on the test data is no more than a proxy for classification accuracy. And so it makes most sense to regard epoch 280 as the point beyond which overfitting is dominating learning in our neural network.</p>

<p>æ–‡ä¸­çš„è§‚ç‚¹æ˜¯ï¼Œ280æ‰æ˜¯ï¼Œä½†æ˜¯è¿™ä¸ªè§‚ç‚¹æœ‰ç‚¹è‰ç‡ï¼Œåé¢ä½œè€…ä¹Ÿä¼šè¯´</p>

<p><img src="/images/md/chap3_overfitting_4.png" alt="" /></p>

<p>è¿™é‡Œæ­£ç¡®ç‡æ˜¯100%ï¼Œé‚£å°±æ˜¯è¯´ï¼Œç½‘ç»œè®°ä½äº†æ•´ä¸ªè®­ç»ƒæ•°æ®ï¼Œè€Œä¸æ˜¯ç†è§£</p>

<p>é¿å…overfittingçš„æ–¹å¼ï¼š</p>

<ul>
  <li>æ—¶åˆ»å…³æ³¨åœ¨æµ‹è¯•é›†ï¼ˆéªŒè¯é›†ï¼‰ä¸Šçš„æ­£ç¡®ç‡ï¼Œå¦‚æœæ­£ç¡®ç‡åœæ­¢å¢é•¿ï¼Œé‚£ä¹ˆå°±åœæ­¢è®­ç»ƒã€‚
ä½†æ˜¯ä¸¥æ ¼æ¥è¯´ï¼Œè¿™ä¸ªå¹¶ä¸èƒ½çœŸæ­£çš„è¯†åˆ«overfittingï¼Œå› ä¸ºæœ‰æ—¶æ˜¯è®­ç»ƒé›†å’Œæµ‹è¯•é›†åŒæ—¶åœæ­¢äº†å¢é•¿ã€‚ä½†è¿™ä¸ªåº”è¯¥å¯ä»¥é˜²æ­¢overfittingçš„å‘ç”Ÿã€‚
ï¼ˆè¿™é‡Œä½œè€…ä¹Ÿåœ¨è¯´ï¼Œåˆ¤æ–­ä½•æ—¶ç®—æ˜¯overfittingï¼Œè¿™ä¸ªåº”è¯¥è°¨æ…ï¼Œå› ä¸ºè®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæœ‰æ—¶å°±ä¼šå‘ç”Ÿä¸€æ®µæ—¶é—´å†…æ­£ç¡®ç‡ä¸ä¸Šå‡çš„æƒ…å†µï¼Œè¿™ç§ç±»ä¼¼å¹³å¦çš„åœ°å½¢ï¼Œä½†æ˜¯è¿‡åï¼Œå°±åˆä¼šå¼€å§‹ä¸Šå‡ã€‚ï¼‰</li>
  <li>å¢å¤§æ•°æ®é‡</li>
  <li>æ­£åˆ™åŒ–ï¼ˆåé¢ä¼šè¯´ï¼‰</li>
</ul>

<p>è¿™é‡Œå…³äºæ•°æ®é›†çš„åˆ’åˆ†æ–¹é¢ï¼Œæ˜¯è¿™æ ·å®šçš„</p>

<ul>
  <li>train</li>
  <li>validationï¼Œåœ¨è¿™ä¸ªæ•°æ®é›†ä¸Šæ¥æµ‹è¯•è°ƒæ•´è¿‡çš„è¶…å‚ï¼Œå¹¶é€‰æ‹©å…¶ä»–çš„è¶…å‚</li>
  <li>testï¼Œåœ¨è¿™ä¸ªæ•°æ®é›†ä¸Šæ¥æœ€ç»ˆæµ‹è¯•ç½‘ç»œï¼Œè¿™ä¸ªç®—æ˜¯æœ€ç»ˆçš„æµ‹è¯•é›†</li>
</ul>

<h3 id="regularization">Regularization</h3>

<p>å‡å°‘ç½‘ç»œçš„å°ºå¯¸ï¼Œä¹Ÿæ˜¯ä¸€ç§å‡å°‘è¿‡æ‹Ÿåˆçš„æ–¹å¼ï¼Œä½†æ˜¯è¿™ä¸ªæˆ‘ä»¬ä¸€èˆ¬ä¸ä¼šé‡‡ç”¨ï¼Œä½†æ˜¯è¿™æ˜¯ä¸€ç§æ€è·¯ï¼Œè¦çŸ¥é“ã€‚</p>

<p>weight decay or L2 regularizationï¼Œæ˜¯ä¸€å›äº‹</p>

<p>äº¤å‰ç†µçš„L2ï¼š</p>

\[C=-\frac{1}{n}\sum_{xj}[y_{j}lna^{L}_{j}+(1-y_{j})ln(1-a^{L}_{j})]+\frac{\lambda}{2n}\sum_{w}w^{2}\]

<p>\(\frac{\lambda}{2n}\)ï¼Œ\(\lambda&gt;0\)ï¼Œå«åšæ­£åˆ™å‚æ•°ï¼Œnæ˜¯æ ·æœ¬çš„æ•°é‡ï¼Œåé¢ä¼šè®¨è®ºå¦‚ä½•é€‰æ‹©æ­£åˆ™å‚æ•°</p>

<p>æ³¨æ„è¿™é‡Œåªæœ‰wï¼Œæ²¡æœ‰bï¼ŒåŸå› å¦‚ä¸‹ï¼š</p>

<ul>
  <li>Empirically, doing this often doesnâ€™t change the results very much</li>
  <li>At the same time, allowing large biases gives our networks more flexibility in behaviour</li>
  <li>in particular, large biases make it easier for neurons to saturate, which is sometimes desirableï¼Œé¥±å’Œæ˜¯æˆ‘ä»¬å¸Œæœ›çš„ï¼Ÿ</li>
</ul>

<p>å‡æ–¹å·®çš„L2ï¼š</p>

\[C=\frac{1}{2n}\sum_{x}\left \|y-a^{L}\right \|^{2}+\frac{\lambda}{2n}\sum_{w}w^{2}\]

<p>é€šç”¨å½¢å¼çš„L2ï¼š</p>

\[C=C_{0}+\frac{\lambda}{2n}\sum_{w}w^{2}\]

<p>Large weights will only be allowed if they considerably improve the first part of the cost function.
ï¼Ÿï¼Ÿè¿™é‡Œä¸æ˜ç™½</p>

\[\frac{\partial C}{\partial w}=\frac{\partial C_{0}}{\partial w}+\frac{\lambda}{n}w\]

\[\frac{\partial C}{\partial b}=\frac{\partial C_{0}}{\partial b}\]

\[b\rightarrow b-\eta \frac{\partial C_{0}}{\partial b}\]

\[w\rightarrow w-\eta \frac{\partial C_{0}}{\partial w}-\frac{\eta\lambda }{n}w=(1-\frac{\eta \lambda }{n})w-\eta \frac{\partial C_{0}}{\partial w}\]

<p>åæ¥çš„æµ‹è¯•é‡Œé¢ï¼Œä½¿ç”¨æ­£åˆ™éƒ½è¦å¥½è¿‡æ²¡ä½¿ç”¨æ­£åˆ™çš„ç»“æœ</p>

<p>ä¸ºä»€ä¹ˆL2æ­£åˆ™å¯ä»¥å‡å°‘overfittingï¼Œè€Œä¸”å¾—åˆ°æ›´å¥½çš„ç»“æœï¼Ÿ</p>

<ul>
  <li>å¯¹äºL2æ­£åˆ™æ¥è¯´ï¼ŒL2å‡å°äº†wï¼Œwè¶Šå°ï¼Œæ¯æ¬¡è¿­ä»£wçš„å˜åŒ–å°±è¶Šå°ï¼Œè¿™æ ·å³ä½¿æ ·æœ¬å°‘ï¼Œæ•°æ®ä¸å‡è¡¡ï¼Œå¹³å‡ä¸å®Œå…¨ï¼Œä¹Ÿå¯ä»¥å‡å°é™·å…¥åˆ°å±€éƒ¨æœ€ä¼˜è§£çš„å¯èƒ½æ€§</li>
  <li>å¦‚æœæ ·æœ¬å¤šçš„è¯ï¼Œé‚£ä¹ˆå­¦ä¹ çš„ç›®æ ‡å°±å¤šï¼Œé‚£ä¹ˆå–å¹³å‡å€¼ä¹‹åï¼Œé™·å…¥åˆ°å°‘æ ·æœ¬çš„å±€éƒ¨æœ€ä¼˜è§£é‡Œçš„å¯èƒ½æ€§å°±å°ï¼Œè¿™æ˜¯ä¸ºä»€ä¹ˆå¤šæ ·æœ¬ï¼Œå¯ä»¥å‡å°overfitting</li>
  <li>å¦‚æœæ ·æœ¬å¤šï¼Œè€Œä¸”ä½¿ç”¨L2ï¼Œé‚£ä¹ˆæ•ˆæœè‡ªç„¶å°±æ›´å¥½</li>
</ul>

<h3 id="why-does-regularization-help-reduce-overfitting">Why does regularization help reduce overfitting?</h3>

<p>One point of view is to say that in science we should go with the simpler explanation, unless compelled not to.</p>

<p>æ­£åˆ™å¯ä»¥æŠµæŠ—noise</p>

<p>ä¸‹é¢çš„ä¸¤ä¸ªä¾‹å­ï¼Œå†è¯´çš„æ˜¯ï¼Œæ­£åˆ™äº§ç”Ÿçš„å¸®åŠ©ï¼Œæ˜¯æ— æ³•ç®€å•è§£é‡Šã€‚ã€‚</p>

<p>A network with 100 hidden neurons has nearly 80,000 parameters. We have only 50,000 images in our training data. Itâ€™s like trying to fit an 80,000th degree polynomial to 50,000 data points. By all rights, our network should overfit terribly. And yet, as we saw earlier, such a network actually does a pretty good job generalizing.</p>

<p>æˆ‘ä»¬çš„è¾“å…¥æ˜¯50000ä¸ªç‚¹ï¼Œæ¨¡å‹çš„å‚æ•°ç¼ºæœ‰80000ä¸ªï¼Œæ‰€ä»¥è¿™æ ·çœ‹çš„è¯ï¼Œåº”è¯¥ä¼šè¿‡æ‹Ÿåˆä¸¥é‡ï¼Œä½†æ˜¯å®éªŒçš„ç»“æœå´æ˜¯ä¸€ä¸ªå¥½çš„ç»“æœï¼Œè¿™é‡Œæ˜¯ä¸€ä¸ªæ— æ³•è§£é‡Šçš„åœ°æ–¹ã€‚</p>

<p>â€œthe dynamics of gradient descent learning in multilayer nets has a â€˜self-regularizationâ€™ effectâ€</p>

<p>æ¢¯åº¦ä¸‹é™ï¼Œå¥½åƒè‡ªå¸¦æ­£åˆ™æ•ˆæœ</p>

<h3 id="other-techniques-for-regularization">Other techniques for regularization</h3>

<ul>
  <li>L1 regularization</li>
</ul>

\[C=C_{0}+\frac{\lambda}{n}\sum_{w}\left | w \right |\]

<p>\(\frac{\partial C}{\partial w}=\frac{\partial C_{0}}{\partial w}+\frac{\lambda}{n}sgn(w)\), sgn(w), w&gt;0:+1 w&lt;0:-1</p>

\[w\rightarrow w'=w-\frac{\eta \lambda }{n}sgn(w)-\eta \frac{\partial C_{0}}{\partial w}\]

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre></td><td class="rouge-code"><pre>L0ï¼šè®¡ç®—éé›¶ä¸ªæ•°ï¼Œç”¨äºäº§ç”Ÿç¨€ç–æ€§ï¼Œä½†æ˜¯åœ¨å®é™…ç ”ç©¶ä¸­å¾ˆå°‘ç”¨ï¼Œå› ä¸ºL0èŒƒæ•°å¾ˆéš¾ä¼˜åŒ–æ±‚è§£ï¼Œæ˜¯ä¸€ä¸ªNP-hardé—®é¢˜ï¼Œå› æ­¤æ›´å¤šæƒ…å†µä¸‹æˆ‘ä»¬æ˜¯ä½¿ç”¨L1èŒƒæ•°
L1ï¼šè®¡ç®—ç»å¯¹å€¼ä¹‹å’Œï¼Œç”¨ä»¥äº§ç”Ÿç¨€ç–æ€§ï¼Œå› ä¸ºå®ƒæ˜¯L0èŒƒå¼çš„ä¸€ä¸ªæœ€ä¼˜å‡¸è¿‘ä¼¼ï¼Œå®¹æ˜“ä¼˜åŒ–æ±‚è§£
L2ï¼šè®¡ç®—å¹³æ–¹å’Œå†å–å¹³å‡æ•°ï¼ŒL2èŒƒæ•°æ›´å¤šæ˜¯é˜²æ­¢è¿‡æ‹Ÿåˆï¼Œå¹¶ä¸”è®©ä¼˜åŒ–æ±‚è§£å˜å¾—ç¨³å®šå¾ˆå¿«é€Ÿï¼ˆè¿™æ˜¯å› ä¸ºåŠ å…¥äº†L2èŒƒå¼ä¹‹åï¼Œæ»¡è¶³äº†å¼ºå‡¸ï¼‰ã€‚
</pre></td></tr></tbody></table></code></pre></div></div>
<p>L1å’ŒL2çš„åŒºåˆ«æ˜¯ï¼ŒL1ä½¿ç”¨çš„å›ºå®šå€¼çš„è¡°å‡ï¼Œè€ŒL2ä½¿ç”¨çš„æ˜¯wçš„æ¯”ä¾‹è¡°å‡ï¼Œæ‰€ä»¥åœ¨wæ¯”è¾ƒå¤§çš„æ—¶å€™ï¼ŒL2è¡°å‡æ›´å¿«ï¼Œåœ¨wå°çš„æ—¶å€™ï¼ŒL1è¡°å‡æ›´å¿«ã€‚</p>

<p>The net result is that L1 regularization tends to concentrate the weight of the network in a relatively small number of high-importance connections, while the other weights are driven toward zero.</p>

<p>L1å€¾å‘äºä¿å­˜æ¯”è¾ƒå¤§çš„wï¼Œè€Œå…¶ä»–çš„wè¶‹äº0ã€‚</p>

<p>L1äº§ç”Ÿç¨€ç–æ€§ï¼Œå› ä¸ºå¾ˆå¤šå°wï¼Œè¢«è¡°å‡ä¸º0ã€‚</p>

<p>ç¨€ç–æ€§çš„å¥½å¤„æ˜¯å¯è§£é‡Šæ€§ï¼Œå³æ ¹æ®éé›¶ç³»æ•°æ‰€å¯¹åº”çš„åŸºçš„å®é™…æ„ä¹‰æ¥è§£é‡Šæ¨¡å‹çš„å®é™…æ„ä¹‰ï¼Œè€Œä¸”å¯ä»¥ç¼©å‡æ•°æ®é‡</p>

<p>å¦å¤–L1ï¼Œéœ€è¦æ³¨æ„w=0çš„ç‚¹ï¼Œå› ä¸º</p>

\[\left | w \right |\]

<p>åœ¨è¿™ç‚¹æ˜¯ä¸å¯å¯¼çš„ï¼Œåœ¨å®é™…ä½¿ç”¨çš„æ—¶å€™ï¼Œéœ€è¦é¢å¤–å¤„ç†ã€‚</p>

<ul>
  <li>dropout</li>
</ul>

<p>æ¯ä¸ªbatchæ˜¯ä¸€ä¸ªå¾ªç¯</p>

<ol>
  <li>æ¢å¤ä¹‹å‰dropoutçš„ç¥ç»å…ƒ</li>
  <li>éšæœºç æ‰ä¸€åŠçš„ç¥ç»å…ƒ</li>
  <li>æ­£å‘åå‘ä¼ æ’­ï¼Œæ›´æ–°å‚æ•°</li>
</ol>

<p>æœ‰ä¸€ä¸ªåœ°æ–¹éœ€è¦æ³¨æ„ï¼Œç”±äºå®é™…ç®—å‡ºæ¥çš„å‚æ•°æ˜¯åªæœ‰ä¸€åŠçš„ä¸­é—´ç¥ç»å…ƒï¼Œå½“åšè¯„ä¼°çš„æ—¶å€™ï¼Œéœ€è¦ä½¿ç”¨æ‰€æœ‰çš„ç¥ç»å…ƒï¼Œæ‰€ä»¥è®­ç»ƒæ—¶å€™å¾—åˆ°çš„å‚æ•°ï¼Œåº”è¯¥é™¤ä»¥2.</p>

<p>ä¸ºä»€ä¹ˆdropoutå¯ä»¥ç”Ÿæ•ˆï¼Ÿ</p>

<p>The reason is that the different networks may overfit in different ways, and averaging may help eliminate that kind of overfitting.</p>

<p>And so the dropout procedure is like averaging the effects of a very large number of different networks. The different networks will overfit in different ways, and so, hopefully, the net effect of dropout will be to reduce overfitting.</p>

<p>å› ä¸ºdropoutç›¸å½“äºäº§ç”Ÿäº†å¾ˆå¤šä¸åŒæ¨¡å‹çš„ç½‘ç»œï¼Œæ¯ä¸ªç½‘ç»œéƒ½å¯èƒ½è¿‡æ‹Ÿåˆæˆä¸åŒçš„æ–¹å¼ï¼Œè€Œdropoutå¹³å‡åŒ–äº†è¿™äº›æ–¹å¼ï¼Œæ‰€ä»¥å¯ä»¥ç”Ÿæ•ˆã€‚</p>

<p>Dropout has been especially useful in training large, deep networks, where the problem of overfitting is often acute.</p>

<p>å½“ç½‘ç»œå¤§ä¸”æ·±çš„æ—¶å€™ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆå°±è¶Šæ¥è¶Šé‡è¦</p>

<ul>
  <li>artificially increasing the training set size</li>
</ul>

<p><img src="/images/md/chap3_train_size_1.png" alt="" /></p>

<p><img src="/images/md/chap3_train_size_2.png" alt="" /></p>

<p>ä¸¤ä¸ªä¸åŒçš„ç®—æ³•ABï¼Œå¯èƒ½å‘ç”Ÿçš„æƒ…å†µæ˜¯ï¼Œåœ¨æ•°æ®é›†Xä¸Šï¼ŒAè¦å¥½ï¼Œåœ¨æ•°æ®é›†Yä¸Šï¼ŒBè¦å¥½ï¼Œæ‰€ä»¥å¦‚æœæœ‰äººé—®ï¼Œæ˜¯Aå¥½è¿˜æ˜¯Bå¥½ï¼Œé‚£ä¹ˆåº”è¯¥åé—®ï¼Œä½ é€‰æ‹©å“ªä¸ªæ•°æ®é›†ï¼Ÿ</p>

<p>æ‰‹å†™æ•°å­—è¯†åˆ«</p>
<ul>
  <li>åŸºç¡€æƒ…å†µï¼š98.4</li>
  <li>åŠ å…¥ä¸€äº›åŸºç¡€æ‰©å±•ï¼Œæ¯”å¦‚æ—‹è½¬ç­‰ï¼š98.9</li>
  <li>åŠ å…¥ä¸€ä¸ªç‰¹æ®Šçš„éšæœºçš„æ™ƒåŠ¨ï¼ˆæ¨¡æ‹Ÿæ‰‹å†™æ—¶å€™çš„æ™ƒåŠ¨ï¼‰ï¼š99.3</li>
</ul>

<p>Itâ€™s fine to look for better algorithms, but make sure youâ€™re not focusing on better algorithms to the exclusion of easy wins getting more or better training data.</p>

<p>å¯»æ‰¾å¥½çš„ç®—æ³•ï¼Œä¹Ÿæ‰¾å¯»æ‰¾å¥½çš„æ•°æ®ï¼Œä¸€ä¸ªå¥½çš„æ•°æ®ï¼Œä¼šä½¿è¾¾åˆ°å¥½æˆç»©å˜å¾—ç®€å•åœ°å¤šã€‚</p>

<h3 id="weight-initialization">Weight initialization</h3>

\[z=\sum_{j}w_{j}x_{j}+b\]

<p>è¿™ä¸ªå…¬å¼ï¼Œå½“1000ä¸ªinputï¼Œå…¶ä¸­500ä¸ª1,500ä¸ª0ï¼Œwå’Œbéƒ½é‡‡ç”¨æ ‡å‡†æ­£æ€åˆ†å¸ƒæ¥åˆå§‹åŒ–ï¼Œé‚£ä¹ˆï¼Œ
zç¬¦åˆN(0,501)çš„æ­£æ€åˆ†å¸ƒï¼Œè¿™ä¸ªæ­£æ€åˆ†å¸ƒå°±å¾ˆå¹³ï¼Œå¯¼è‡´çš„ç»“æœå°±æ˜¯ï¼Œzè¿œå¤§äº1æˆ–è€…è¿œå°äº-1çš„å¯èƒ½æ€§å¾ˆå¤§ï¼Œå°±æ˜¯|z|å–å¤§å€¼çš„å¯èƒ½æ€§å¾ˆå¤§ã€‚</p>

<p>è¿™æ—¶å€™ï¼Œå¦‚æœæ¿€æ´»å‡½æ•°æ˜¯sigmoidçš„è¯ï¼Œé‚£ä¹ˆ\(\sigma(z)\)å°±å¾ˆæ¥è¿‘1ï¼Œä¹Ÿå°±æ˜¯é¥±å’Œäº†ï¼Œå­¦ä¹ é€Ÿç‡å°±å¾ˆä½ï¼ˆè¿™ä¸ªåº”è¯¥æ˜¯è¯´çš„æ˜¯sigmoidçš„æƒ…å†µäº†ï¼‰ã€‚</p>

<p>We addressed that earlier problem with a clever choice of cost function. Unfortunately, while that helped with saturated output neurons, it does nothing at all for the problem with saturated hidden neurons.</p>

<p>ä¹‹å‰çš„æ›´æ¢æŸå¤±å‡½æ•°ä»å‡æ–¹å·®åˆ°äº¤å‰ç†µï¼Œåªæ˜¯è§£å†³äº†è¾“å‡ºå±‚çš„é¥±å’Œé—®é¢˜ï¼Œæ²¡æ³•è§£å†³ä¸­é—´å±‚çš„é¥±å’Œé—®é¢˜ï¼</p>

<p>å¦‚ä½•é¿å…è¿™ä¸ªé—®é¢˜å‘¢ï¼Ÿ</p>

<p>å¯ä»¥åœ¨åˆå§‹åŒ–çš„æ—¶å€™ï¼Œå°†æ ‡å‡†æ­£æ€åˆ†å¸ƒï¼Œæ¢æˆ\(N(0,1/\sqrt{n_{in}})\)ï¼Œbè¿˜æ˜¯ä½¿ç”¨çš„æ ‡å‡†æ­£æ€åˆ†å¸ƒï¼Œå› ä¸ºè¿™ä¸ªæœ‰äººå®éªŒè¯æ˜è¿‡ï¼Œæ²¡æœ‰å•¥å½±å“ã€‚</p>

<p><img src="/images/md/chap3_weight_init_1.png" alt="" /></p>

<p>å®éªŒç»“æœï¼Œå¯ä»¥çœ‹å‡ºæ¥ï¼Œè™½ç„¶æœ€åç»“æœä¸€æ ·ï¼Œä½†æ˜¯ä¸Šå‡é€Ÿåº¦è¦å¿«ï¼Œå¹¶ä¸”æå‰è¾¾åˆ°æœ€åçš„96ç²¾åº¦</p>

<p>However, in Chapter 4 weâ€™ll see examples of neural networks where the long-run behaviour is significantly better with the 1/ninâ€¾â€¾â€¾âˆš1/nin weight initialization. Thus itâ€™s not only the speed of learning which is improved, itâ€™s sometimes also the final performance.</p>

<p>åˆå§‹åŒ–çš„å€¼ï¼Œå¹¶éåªæ˜¯å½±å“å­¦ä¹ é€Ÿç‡ï¼Œä¹Ÿä¼šå½±å“åˆ°æœ€ç»ˆçš„ç»“æœã€‚</p>

<p>L2 regularization sometimes automatically gives us something similar to the new approach to weight initialization.</p>

<p>L2æ­£åˆ™å’Œä¼˜åŒ–åˆå§‹åŒ–å€¼çš„æ–¹å¼ï¼Œæ¯”è¾ƒç›¸ä¼¼ï¼Œéƒ½æ˜¯å‡å°å‚æ•°</p>

<h3 id="handwriting-recognition-revisited-the-code">Handwriting recognition revisited: the code</h3>

<p>saveå’Œloadä½¿ç”¨çš„æ˜¯jsonï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨pickle
ä½œè€…çš„æ„æ€æ˜¯ä½¿ç”¨jsonï¼Œå¯ä»¥æ–¹ä¾¿åœ¨ä»¥åæ›´æ”¹ä»£ç ä¹‹åï¼Œloadæ•°æ®
ä½†æ˜¯å®é™…ä¸Špickleä¹Ÿå¯ä»¥åšåˆ°è¿™ç‚¹ï¼Œä»¥å‰çœ‹è¿‡ç±»ä¼¼çš„åšæ³•</p>

<h3 id="how-to-choose-a-neural-networks-hyper-parameters">How to choose a neural networkâ€™s hyper-parameters?</h3>

<p>å¼€å¯monitorï¼Œè¿™ä¸ªå¾ˆé‡è¦</p>

<ul>
  <li>Broad strategy
    <ul>
      <li>å¯ä»¥ä»10ä¸ªåˆ†ç±»ï¼Œå‡å°‘åˆ°ä¸¤ä¸ªåˆ†ç±»ï¼Œæ¯”å¦‚å…ˆå°è¯•åˆ†ç±»0å’Œ1ï¼Œè¿™æ ·ä¼šå‡å°‘æ—¶é—´</li>
      <li>å¯ä»¥å‡å°‘ç½‘ç»œå±‚æ•°ï¼Œä¹Ÿå¯ä»¥å‡å°‘æ—¶é—´ï¼Œåç»­å†å¢åŠ å±‚æ•°</li>
      <li>å¯ä»¥å‡å°‘è®­ç»ƒçš„æ•°æ®é‡ï¼Œè™½ç„¶è¿™ä¸ªå¯èƒ½é€ æˆoverfittingç­‰ï¼Œä½†æ˜¯ç°åœ¨ä¸æ˜¯é¿å…è¿™ä¸ªçš„æ—¶å€™ï¼Œç°åœ¨ä¸»è¦æ˜¯æƒ³æµ‹è¯•å‚æ•°ã€‚
  å‰æœŸçš„æ—¶å€™ï¼Œèƒ½å¤Ÿå¿«é€Ÿå›é¦ˆæ˜¯æœ€é‡è¦çš„äº‹ï¼Œæ¨¡å‹ä¸€å®šè¦ç®€å•ï¼Œæ•°æ®ä¹Ÿè¦ç®€å•ï¼Œè¦è®°ä½ã€‚</li>
    </ul>
  </li>
  <li>Learning rate</li>
</ul>

<p><img src="/images/md/chap3_learning_rate_1.png" alt="" /></p>

<p>0.25åœ¨æœ€å°ç‚¹é™„è¿‘æœ‰éœ‡è¡ï¼Œ0.025åˆå¤ªæ…¢äº†</p>

<p>æœ€å¥½çš„åŠæ³•æ˜¯æ¯”å¦‚å‰20epochsä½¿ç”¨0.25ï¼Œåé¢10ä¸ªepochsä½¿ç”¨0.025</p>

<p>å¦‚ä½•ä¸€æ­¥æ­¥é€‰æ‹©rateï¼Ÿ</p>

<p>é¦–å…ˆrateï¼ŒæŒ‰ç…§ä½œè€…çš„ç»éªŒæ˜¯ï¼Œä¸è®­ç»ƒæœ¬èº«çš„é€Ÿç‡æœ‰å…³ï¼Œæ‰€ä»¥ä½œè€…ç”¨çš„æ˜¯costä½œä¸ºåˆ¤æ–­æ ‡å‡†ï¼Œè€Œå…¶ä»–çš„å‚æ•°ï¼Œæ¯”å¦‚mini-batchï¼Œå±‚æ•°ç­‰ç­‰ï¼Œéƒ½æ˜¯ç”¨çš„valiadateæ•°æ®é›†çš„å‡†ç¡®ç‡ä½œä¸ºåˆ¤æ–­æ ‡å‡†ã€‚</p>
<ol>
  <li>å…ˆé€‰æ‹©ä¸€ä¸ªç»éªŒå€¼ï¼Œæ¯”å¦‚0.01æˆ–è€…0.1</li>
  <li>æ‰¾åˆ°ä¸Šé™å€¼ã€‚çœ‹è®­ç»ƒçš„å‰å‡ ä¸ªepochï¼Œå¦‚æœcoståœ¨ä¸‹é™ï¼Œé‚£ä¹ˆå°è¯•åŠ å¤§rateï¼Œå¦‚æœä¸Šå‡æˆ–è€…éœ‡åŠ¨ï¼Œé‚£ä¹ˆå°±å‡å°‘rateï¼Œè¿™æ ·å¯ä»¥æ‰¾åˆ°ä¸€ä¸ªä¸Šé™å€¼</li>
  <li>ç”¨ä¸Šé™å€¼è®­ç»ƒï¼Œè§‚å¯Ÿå›¾ï¼Œå¦‚æœåœ¨æœ€ä½ç‚¹é™„è¿‘éœ‡è¡çš„è¯ï¼Œé‚£ä¹ˆå°±å°è¯•å‡å°‘ä¸€ç‚¹rate</li>
  <li>ç›´åˆ°æ‰¾åˆ°ä¸€ä¸ªæ—¢è¿…é€Ÿåˆä¸éœ‡è¡çš„rate</li>
</ol>

<ul>
  <li>Use early stopping to determine the number of training epochs</li>
</ul>

<p>åœ¨æµ‹è¯•é›†æˆ–è€…éªŒè¯é›†ä¸Šï¼Œå¦‚æœä¸€æ®µæ—¶é—´æ­£ç¡®ç‡ä¸ä¸Šå‡ï¼Œå°±åœæ­¢è®­ç»ƒã€‚</p>

<p>è¿™ä¸ªå¯ä»¥ç®€åŒ–epochçš„é€‰æ‹©ï¼Œå¹¶ä¸”å‡å°‘overfittingã€‚</p>

<p>ä½†æ˜¯åœ¨å‰æœŸï¼Œä½œè€…å¹¶ä¸å»ºè®®ä½¿ç”¨è¿™ç§æ–¹æ¡ˆï¼Œä½œè€…æ˜¯å¸Œæœ›overfittingå‘ç”Ÿï¼Œç„¶åä½¿ç”¨æ­£åˆ™æ¥å¤„ç†ã€‚</p>

<p>å¦‚ä½•å®ç°ï¼Ÿ</p>

<p>A better rule is to terminate if the best classification accuracy doesnâ€™t improve for quite some time.</p>

<p>æ¯”å¦‚ï¼Œä»no-improvement-in-tenå¼€å§‹ï¼Œéœ€è¦çŸ¥é“çš„æ˜¯ï¼Œæœ‰æ—¶æ¨¡å‹å°±æ˜¯ä¼šå¹³å¦ä¸€æ®µæ—¶é—´ï¼Œè¿™æ®µæ—¶é—´å†…æ²¡æœ‰æ­£ç¡®ç‡ä¸Šå‡ï¼Œæ‰€ä»¥è¿™ä¸ªåˆ¤æ–­çš„æ ‡å‡†ï¼Œè¦æ ¹æ®å®é™…æƒ…å†µæ¥å®šã€‚</p>

<ul>
  <li>Learning rate schedule</li>
</ul>

<p>å­¦ä¹ é€Ÿç‡é€’å‡ï¼Œæ€ä¹ˆé€’å‡ï¼Ÿ</p>

<p>ä¸€ç§æ–¹æ³•æ˜¯ï¼Œå½“éªŒè¯é›†çš„æ­£ç¡®ç‡ä¸‹é™çš„æ—¶å€™å¼€å§‹å‡å°å­¦ä¹ é€Ÿç‡ï¼Œå¯ä»¥æŒ‰ç…§ a factor of two or tenï¼ˆä¹Ÿå°±æ˜¯å‡å°åˆ°åŸæ¥çš„1/2æˆ–è€…1/10ï¼‰ï¼Œç›´åˆ°å‡å°åˆ°1/1024ï¼ˆor 1/1000ï¼‰ï¼Œå°±åœæ­¢å‡å°ã€‚</p>

<p>éœ€è¦çŸ¥é“çš„æ˜¯ï¼Œè·Ÿearly stopä¸€æ ·ï¼Œè¿™ä¸ªä¹Ÿå¢åŠ äº†é¢å¤–çš„å‚æ•°ï¼Œä¹Ÿå°±æ˜¯å¢åŠ äº†éœ€è¦è°ƒæ•´çš„åœ°æ–¹ï¼Œåœ¨å‰æœŸçš„æ—¶å€™ï¼Œæ²¡å¿…è¦è¿™ä¹ˆåšï¼ŒåæœŸè¿½æ±‚åˆ†æ•°çš„æ—¶å€™ï¼Œå†è¿™ä¹ˆåšã€‚</p>

<ul>
  <li>The regularization parameter,  Î»</li>
</ul>

<p>æœ€å¼€å§‹çš„æ—¶å€™ï¼ŒÎ»=0.0å³å¯ï¼Œå…ˆè°ƒæ•´Î· ,Î· è°ƒæ•´åˆ°ä¸€ä¸ªåˆé€‚çš„æ•°å€¼ä¹‹åï¼Œå¼€å§‹è°ƒæ•´ Î»ï¼Œå¯ä»¥æŒ‰ç…§10å€çš„é€Ÿç‡å¢åŠ æˆ–è€…å‡å°‘ï¼Œå½“ç¡®å®šå¥½ Î»ä¹‹åï¼Œå†å›è¿‡å¤´å»ç»§ç»­è°ƒæ•´Î·ã€‚</p>

<ul>
  <li>How I selected hyper-parameters earlier in this book</li>
  <li>Mini-batch size</li>
</ul>

<p>online_learning(batchä¸º1)ï¼Œä¼šå¯¼è‡´æ¢¯åº¦æœ‰é—®é¢˜ï¼Œä½†æ˜¯è¿™ä¸ªä¸æ˜¯ä¸€ä¸ªå¾ˆè¦ç´§çš„äº‹ï¼Œå› ä¸ºå¦‚ä¸‹ï¼š</p>

<p>Itâ€™s as though you are trying to get to the North Magnetic Pole, but have a wonky compass thatâ€™s 10-20 degrees off each time you look at it. Provided you stop to check the compass frequently, and the compass gets the direction right on average, youâ€™ll end up at the North Magnetic Pole just fine.</p>

<p>é‚£æ˜¯ä¸æ˜¯online_learningï¼Œæ˜¯æœ€å¥½çš„é€‰æ‹©äº†å‘¢ï¼Ÿå› ä¸ºæ—¢å¯ä»¥è¾¾åˆ°æœ€ä¼˜ç‚¹ï¼Œä¹Ÿé¢‘ç¹å¾—æ›´æ–°å‚æ•°ï¼Œä¸æ˜¯å¾ˆå¥½ï¼Ÿä½†æ˜¯æœ‰ä¸€ç‚¹éœ€è¦æ³¨æ„ï¼Œå› ä¸ºbatch size =1ï¼Œé‚£ä¹ˆ10000ä¸ªæ ·æœ¬çš„epochå°±å¿…é¡»å¾ªç¯10000æ¬¡ï¼Œè€Œä¸”æ²¡æœ‰ç”¨åˆ°çŸ©é˜µè®¡ç®—çš„ä¼˜åŠ¿ï¼Œè¿™æ ·å°±å¾ˆæ…¢ï¼Œå­¦ä¹ æ—¶é—´å°±ä¼šé•¿ã€‚</p>

<p>å¦‚æœbatch sizeå¤ªå¤§ï¼Œé‚£ä¹ˆå‚æ•°æ›´æ–°å°±ä¸å¤Ÿé¢‘ç¹ï¼Œä¹Ÿä¸å¥½ã€‚æ‰€ä»¥éœ€è¦ç»¼åˆèµ·æ¥è€ƒè™‘ã€‚</p>

<p>æ‰€å¹¸çš„æ˜¯ï¼Œbatch_sizeä¸å…¶ä»–å‚æ•°çš„æ²¡å•¥å…³ç³»ï¼Œæ‰€ä»¥åªéœ€å®šä¸‹å…¶ä»–å‚æ•°ï¼Œå•ç‹¬ä¼˜åŒ–è¿™ä¸ªå°±å¥½ã€‚</p>

<p>æ‰€ä»¥åº”è¯¥æ˜¯åœ¨æœ€ä¼˜ç‚¹ä¸€è‡´çš„æƒ…å†µä¸‹ï¼Œé€‰æ‹©ä¸€ä¸ªæœ€å¤§çš„batch sizeï¼Œä½¿è®­ç»ƒæ—¶é—´æœ€çŸ­ã€‚</p>

<ul>
  <li>Automated techniques</li>
</ul>

<p>è‡ªåŠ¨ä¼˜åŒ–å‚æ•°</p>

<ul>
  <li>Summing up</li>
</ul>

<p>æœ‰ä¸€äº›paperä»‹ç»äº†å¦‚ä½•é€‰æ‹©å‚æ•°</p>

<p>é€‰æ‹©å‚æ•°è¿™ä¸ªé—®é¢˜ï¼Œç°åœ¨ä¹Ÿæ²¡æœ‰è§£å†³ï¼Œæ²¡æœ‰ä¸€ä¸ªç»Ÿä¸€çš„æ–¹å¼</p>

<p>So your goal should be to develop a workflow that enables you to quickly do a pretty good job on the optimization, while leaving you the flexibility to try more detailed optimizations, if thatâ€™s important.</p>

<p>æ‰€ä»¥å°±æ˜¯å®šä¸‹ä¸€ä¸ªè‡ªå·±çš„æµç¨‹ï¼Œè¿™ä¸ªæµç¨‹åº”è¯¥å¯ä»¥åšå‡ºä¸€ä¸ªæ¯”è¾ƒå¥½çš„å‚æ•°ï¼Œç„¶åå†ç»†è‡´å¾—è°ƒæ•´ã€‚</p>

<h3 id="other-techniques">Other techniques</h3>

<h4 id="variations-on-stochastic-gradient-descentå˜ç§éšæœºæ¢¯åº¦ä¸‹é™">Variations on stochastic gradient descentï¼ˆå˜ç§éšæœºæ¢¯åº¦ä¸‹é™ï¼‰</h4>

<ul>
  <li>Hessian technique
Intuitively, the advantage Hessian optimization has is that it incorporates not just information about the gradient, but also information about how the gradient is changing.</li>
  <li>momentum</li>
</ul>

<h4 id="other-approaches-to-minimizing-the-cost-function">Other approaches to minimizing the cost function</h4>

<p>As you go deeper into neural networks itâ€™s worth digging into the other techniques, understanding how they work, their strengths and weaknesses, and how to apply them in practice.</p>

<h4 id="other-models-of-artificial-neuron">Other models of artificial neuron</h4>

<ul>
  <li>tanhï¼Œå…·ä½“å…¬å¼ä¸è®°å½•äº†ï¼Œè¿™ä¸ªä¸sigmoidç±»ä¼¼ï¼Œåªæ˜¯outputä»0,1åˆ°äº†-1,1
ä¸€èˆ¬è®¤ä¸ºtanhè¦æ¯”sigmoidè¦å¥½ï¼Œå› ä¸ºsigmoidç”±äºæ˜¯éè´Ÿçš„ï¼Œæ‰€ä»¥å¯¹ä¸€ä¸ªç¥ç»å…ƒæ¥è¯´ï¼Œæ‰€æœ‰çš„wåŒæ—¶ä¸Šå‡æˆ–è€…ä¸‹é™ï¼Œè¿™ä¸ªä¸å¤ªè´´åˆå®é™…ã€‚è€Œtanhï¼Œå°±å¯ä»¥é¿å…è¿™ä¸ªé—®é¢˜ã€‚
ä½†æ˜¯ç°åœ¨å®éªŒèµ·æ¥ï¼Œtanhæ¯”sigmoidåˆæ²¡æœ‰å¤šå°‘è¿›æ­¥æˆ–è€…è¿›æ­¥å¾ˆå°ã€‚</li>
  <li>rectified linear neuronï¼ˆå°±æ˜¯reluï¼‰
image recognitionä¸Šé¢ç”¨çš„æ¯”è¾ƒå¤šï¼Œå¹¶ä¸”æ¯”è¾ƒæœ‰æ•ˆã€‚è¿™ç§æ¿€æ´»å‡½æ•°ï¼Œæ²¡æœ‰sigmoidç±»ä¼¼å‡½æ•°çš„é¥±å’Œé—®é¢˜ã€‚</li>
</ul>

<h3 id="network2-code">network2 code</h3>

<ul>
  <li>å¢åŠ äº†cross-entropy</li>
  <li>å¢åŠ äº†æƒé‡è¡°å‡</li>
  <li>å¢åŠ äº†Save load</li>
  <li>å¢åŠ äº†monitor
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre></td><td class="rouge-code"><pre><span class="c1"># å‡æ–¹å·®
</span><span class="mf">0.5</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">a</span><span class="o">-</span><span class="n">y</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
<span class="c1"># äº¤å‰ç†µ
</span><span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="o">-</span><span class="n">y</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">a</span><span class="p">)</span><span class="o">-</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">y</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">a</span><span class="p">)))</span> <span class="c1"># nan_to_numï¼Œå¦‚æœæ˜¯nanåˆ™æ¢ä¸º0ï¼Œå¦‚æœæ˜¯infiniteï¼Œåˆ™æ¢ä¸ºinf
</span></pre></td></tr></tbody></table></code></pre></div>    </div>
  </li>
</ul>
:ET