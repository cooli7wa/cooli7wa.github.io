I"Fm<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

<p><a href="http://neuralnetworksanddeeplearning.com/chap1.html">原文地址</a></p>

<h3 id="perceptrons感知器">perceptrons（感知器）</h3>

<p>1950s-1960s by scientist Frank Rosenblatt</p>

<p><img src="/images/md/chap1_perceptron.png" alt="" /></p>

<p>数学模型：</p>

<p><img src="/images/md/chap1_perceptron_1.png" alt="" /></p>

<p>所有权重参数为w1,w2…，threshold</p>

<p>threshold，Dropping the threshold means you’re more willing to go to the festival.</p>

<p>与权重b是同一种意思，一个表示偏好的权重</p>

<p>感知机的输入只是0或1，输出也是0或1</p>

<p><img src="/images/md/chap1_perceptron_2.png" alt="" /></p>

<p>两处简化</p>

\[\sum_{j} w_{j}x_{j} \equiv w\cdot x\]

\[b \equiv -threshold\]

<p>这个bias代表的是how easy it is to get the perceptron to fire</p>

<p>perceptron的两种用途：</p>
<ul>
  <li>a method for weighing evidence to make decisions</li>
  <li>compute simple logical functions, depend on NAND gate</li>
</ul>

<p>perceptrons能实现与非门，但是并不是仅仅的只是实现另一种集成电路，与传统的基于与非门的集成电路不同的是，perceptrons能够通过自动调节weights和biases，来自动学习解决问题</p>

<h3 id="sigmoid-neurons">sigmoid neurons</h3>

<p>In fact, a small change in the weights or bias of any single perceptron in the network can sometimes cause the output of that perceptron to completely flip, say from 00 to 11.
sigmoid：Sigmoid neurons are similar to perceptrons, but modified so that small changes in their weights and bias cause only a small change in their output.</p>

<p>perceptrons的问题是，它是阶梯变化的，不线性，微小的变化不能很好的表现出来。</p>

<p>sigmoid输出0-1之间的任意数，σ(w⋅x+b)</p>

\[\sigma (z) \equiv \frac{1}{1+e^{-z}}\]

\[\sigma (z) \equiv \frac{1}{1+e^{-\sum_{j}w_{j}x_{j}-b}}\]

<p><img src="/images/md/chap1_sigmoid_1.png" alt="" /></p>

<p>sigmoid的一个比perceptrons不方便的地方是，output不是0或1，而是一个介于0-1之间的数，就指示一个对错的百分比而已</p>

<p>perceptrons和sigmoid的一些特性</p>
<ul>
  <li>perceptrons的w b乘以任意正数c，不影响输出，sigmoid就会有影响</li>
  <li>实际上sigmoid如果乘以c，且c趋于正无穷，那么sigmoid与perceptrons就一样了</li>
</ul>

<p>MLPs：multilayer perceptrons</p>

<p>我们一般使用的是feedforward网络，也就是只有前进没有后退的
还有一种是Recurrent neural nets，这里包含loops，这个loops持续一段被限制的时间，不是一直循环下去，神经元本身会受到之前的激发状态的影响或者受到之前的输入的影响，Recurrent neural nets，更贴近人类的大脑</p>

<h3 id="手写数字处理">手写数字处理</h3>

<p>output 10和4，为什么会导致识别准确率有区别？</p>

<p>我个人的理解是，hidden层是基于第一层输入图像的提取结果，那么这个结果是基于图像的。
而4这种方式，是基于数字的，在这层来做不合适。</p>

<p>在练习的部分，有提到，如果加入第三层的话，那么这层可以使用4output，因为相当于是将之前的10个数字的概率，这种基于数字的提取成另外一种数字，这样成功率应该比较高。</p>

<h3 id="梯度递减">梯度递减</h3>

<p>Why not try to maximize that number directly, rather than minimizing a proxy measure like the quadratic cost?</p>

<p>我的理解是，如果只关注结果的数字的最大化的话，那么做了些许的改变w和b，对预测结果来说，可能根本就没有变化，那么就不知道怎么继续提高分数了，这就是说这个不是一个平滑的方法。</p>

<p>而如果关注的是所有结果的数字的均方差的最小化的话，这是一个平滑的方法，可以进行学习。</p>

<p>均方差：</p>

\[\frac{1}{2n}\sum \left \| y(x)-a \right \|^{2}\]

<p>梯度下降：</p>

\[\Delta C \approx \bigtriangledown C\cdot \Delta v\]

\[\bigtriangledown C\equiv (\frac{\partial C}{\partial v_{1}},...,\frac{\partial C}{\partial v_{m}})^{T}\]

\[\Delta v=-\eta \bigtriangledown C\]

<p>stochastic gradient descent，随机梯度递减</p>

\[\bigtriangledown C = \frac{1}{n} \sum_{x} \bigtriangledown C_{x}\]

<p>In practice, to compute the gradient ∇C we need to compute the gradients ∇Cx separately for each training input, x, and then average them, ∇C=1n∑x∇Cx</p>

<p>一种极端的mini-batch为1的训练，这种应用于online on-line incremental learning，就是随机梯度下降，类似于人类学习的过程</p>

<h3 id="创建网络模型">创建网络模型</h3>

<p>从文章提供的<a href="https://github.com/mnielsen/neural-networks-and-deep-learning.git">git</a>下载的代码，在3.5下使用需要更改几个地方</p>

<p>参考这篇 <a href="/images/md/chap1_mnist_35.patch">patch</a></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre></td><td class="rouge-code"><pre><span class="k">class</span> <span class="nc">Network</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sizes</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">sizes</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">sizes</span> <span class="o">=</span> <span class="n">sizes</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">biases</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">sizes</span><span class="p">[</span><span class="mi">1</span><span class="p">:]]</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">weights</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> 
                        <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">sizes</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">sizes</span><span class="p">[</span><span class="mi">1</span><span class="p">:])]</span>
</pre></td></tr></tbody></table></code></pre></div></div>
<p>这里使用的是随机高斯分布（正态分布）来初始化weights和biases</p>

<p>测试下隐含层的神经元数量的影响：</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td><td class="rouge-code"><pre><span class="c1"># Epoch 29: 9384 / 10000
# train time: 0:03:17.5953027
# Epoch 29: 9418 / 10000
# train time: 0:03:28.126904
</span><span class="n">net</span> <span class="o">=</span> <span class="n">network</span><span class="p">.</span><span class="n">Network</span><span class="p">([</span><span class="mi">784</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">10</span><span class="p">])</span>
<span class="n">net</span><span class="p">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">training_data</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="n">test_data</span><span class="o">=</span><span class="n">test_data</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre></td><td class="rouge-code"><pre><span class="c1"># Epoch 29: 9503 / 10000
# train time: 0:04:12.572446
</span><span class="n">net</span> <span class="o">=</span> <span class="n">network</span><span class="p">.</span><span class="n">Network</span><span class="p">([</span><span class="mi">784</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">10</span><span class="p">])</span>
<span class="n">net</span><span class="p">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">training_data</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="n">test_data</span><span class="o">=</span><span class="n">test_data</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre></td><td class="rouge-code"><pre><span class="c1"># Epoch 29: 9575 / 10000
# train time: 0:05:39.106396
</span><span class="n">net</span> <span class="o">=</span> <span class="n">network</span><span class="p">.</span><span class="n">Network</span><span class="p">([</span><span class="mi">784</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">10</span><span class="p">])</span>
<span class="n">net</span><span class="p">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">training_data</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="n">test_data</span><span class="o">=</span><span class="n">test_data</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td><td class="rouge-code"><pre><span class="c1"># Epoch 29: 8537 / 10000
# train time: 0:07:48.387790
# Epoch 29: 9599 / 10000
# train time: 0:07:44.628575
</span><span class="n">net</span> <span class="o">=</span> <span class="n">network</span><span class="p">.</span><span class="n">Network</span><span class="p">([</span><span class="mi">784</span><span class="p">,</span> <span class="mi">80</span><span class="p">,</span> <span class="mi">10</span><span class="p">])</span>
<span class="n">net</span><span class="p">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">training_data</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="n">test_data</span><span class="o">=</span><span class="n">test_data</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p><strong>从结果看，隐含层神经元越多，测试结果一般越好，但是花费时间越多。</strong></p>

<p>隐含层神经元越多，意味着从input层获取到的Feature的种类就越多，对于预测越有帮助，但是由于神经元越多，参数也就越多，所以花费时间长。</p>

<p><strong>最后一次实验（对应中间层是80），结果不稳定，低的时候只有85%，可能是陷入了局部最优解，但是20的情况，我测试多次也没有出现不稳定的情况。所以看来，模型越复杂，精度可能会越好，但是越有可能不稳定（陷入局部最优、过拟合、梯度爆炸/消失等）。</strong></p>

<p>测试下batch-size的影响：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre></td><td class="rouge-code"><pre><span class="c1"># Epoch 29: 8932 / 10000
# train time: 0:04:17.280716
</span><span class="n">net</span> <span class="o">=</span> <span class="n">network</span><span class="p">.</span><span class="n">Network</span><span class="p">([</span><span class="mi">784</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">10</span><span class="p">])</span>
<span class="n">net</span><span class="p">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">training_data</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="n">test_data</span><span class="o">=</span><span class="n">test_data</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre></td><td class="rouge-code"><pre><span class="c1"># Epoch 29: 9384 / 10000
# train time: 0:03:17.5953027
</span><span class="n">net</span> <span class="o">=</span> <span class="n">network</span><span class="p">.</span><span class="n">Network</span><span class="p">([</span><span class="mi">784</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">10</span><span class="p">])</span>
<span class="n">net</span><span class="p">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">training_data</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="n">test_data</span><span class="o">=</span><span class="n">test_data</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre></td><td class="rouge-code"><pre><span class="c1"># Epoch 29: 9376 / 10000
# train time: 0:03:11.990982
</span><span class="n">net</span> <span class="o">=</span> <span class="n">network</span><span class="p">.</span><span class="n">Network</span><span class="p">([</span><span class="mi">784</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">10</span><span class="p">])</span>
<span class="n">net</span><span class="p">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">training_data</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="n">test_data</span><span class="o">=</span><span class="n">test_data</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre></td><td class="rouge-code"><pre><span class="c1"># Epoch 29: 9383 / 10000
# train time: 0:02:57.594158
</span><span class="n">net</span> <span class="o">=</span> <span class="n">network</span><span class="p">.</span><span class="n">Network</span><span class="p">([</span><span class="mi">784</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">10</span><span class="p">])</span>
<span class="n">net</span><span class="p">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">training_data</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="n">test_data</span><span class="o">=</span><span class="n">test_data</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre></td><td class="rouge-code"><pre><span class="c1"># Epoch 29: 9270 / 10000
# train time: 0:03:08.102759
</span><span class="n">net</span> <span class="o">=</span> <span class="n">network</span><span class="p">.</span><span class="n">Network</span><span class="p">([</span><span class="mi">784</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">10</span><span class="p">])</span>
<span class="n">net</span><span class="p">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">training_data</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="n">test_data</span><span class="o">=</span><span class="n">test_data</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p><strong>从结果看，随着size增加，整体来看，相同epoch的时间在下降，精度也有下降趋势。所以这么看的话，size应该有个最优值，在这点上，训练时间和精度会达到最优。</strong>
<strong>另外一个现象是，在Size=1的情况下，模型没有收敛到应该有的水平。可能是由于，只根据一个样本来更新参数，整体波动比较大的原因</strong></p>

<h3 id="toward-deep-learning">Toward deep learning</h3>

<p>In the early days of AI research people hoped that the effort to build an AI would also help us understand the principles behind intelligence and, maybe, the functioning of the human brain. But perhaps the outcome will be that we end up understanding neither the brain nor how artificial intelligence works</p>

<p>我们人类的目的是想建立一个AI来帮助我们理解智能背后的原理，比如人类的大脑，但是讽刺的是，实际的结果是，我们造出来了AI，但是发现我们既不理解大脑也不理解AI。</p>

<h3 id="minist数据集到图像">MINIST数据集到图像</h3>

<p>对于这本书里的数据集，可以在mnist_loader.py里，加入如下代码</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
</pre></td><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">restore_image</span><span class="p">():</span>
    <span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span>

    <span class="n">tr_d</span><span class="p">,</span> <span class="n">va_d</span><span class="p">,</span> <span class="n">te_d</span> <span class="o">=</span> <span class="n">load_data</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">img</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">tr_d</span><span class="p">[</span><span class="mi">0</span><span class="p">][:</span><span class="mi">1</span><span class="p">]):</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">))</span>
        <span class="n">new_img</span> <span class="o">=</span> <span class="n">Image</span><span class="p">.</span><span class="n">fromarray</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="s">'L'</span><span class="p">)</span>
        <span class="n">new_img</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">"__main__"</span><span class="p">:</span>
    <span class="n">restore_image</span><span class="p">()</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>fromarray的mode可以参考下表</p>

<p><img src="/images/md/chap1_modes.png" alt="" /></p>

<p>取到的图片（这个不是数字，应该是这个图片有预处理，是从pickle里拿出来的）</p>

<p><img src="/images/md/chap1_book_image.png" alt="" /></p>

<p>对于MNIST的真正数据集，train-images-idx3-ubyte，用如下方法提取</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
</pre></td><td class="rouge-code"><pre><span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span>
<span class="kn">import</span> <span class="nn">numpy</span>
<span class="kn">import</span> <span class="nn">gzip</span>

<span class="k">def</span> <span class="nf">_read32</span><span class="p">(</span><span class="n">bytestream</span><span class="p">):</span>
  <span class="n">dt</span> <span class="o">=</span> <span class="n">numpy</span><span class="p">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">numpy</span><span class="p">.</span><span class="n">uint32</span><span class="p">).</span><span class="n">newbyteorder</span><span class="p">(</span><span class="s">'&gt;'</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">numpy</span><span class="p">.</span><span class="n">frombuffer</span><span class="p">(</span><span class="n">bytestream</span><span class="p">.</span><span class="n">read</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dt</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">extract_images</span><span class="p">(</span><span class="n">f</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">gzip</span><span class="p">.</span><span class="n">GzipFile</span><span class="p">(</span><span class="n">fileobj</span><span class="o">=</span><span class="n">f</span><span class="p">)</span> <span class="k">as</span> <span class="n">bytestream</span><span class="p">:</span>
        <span class="n">magic</span> <span class="o">=</span> <span class="n">_read32</span><span class="p">(</span><span class="n">bytestream</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">magic</span> <span class="o">!=</span> <span class="mi">2051</span><span class="p">:</span>
            <span class="k">raise</span> <span class="nb">ValueError</span><span class="p">(</span><span class="s">'Invalid magic number %d in MNIST image file: %s'</span> <span class="o">%</span>
                             <span class="p">(</span><span class="n">magic</span><span class="p">,</span> <span class="n">f</span><span class="p">.</span><span class="n">name</span><span class="p">))</span>
        <span class="n">num_images</span> <span class="o">=</span> <span class="n">_read32</span><span class="p">(</span><span class="n">bytestream</span><span class="p">)</span>
        <span class="n">rows</span> <span class="o">=</span> <span class="n">_read32</span><span class="p">(</span><span class="n">bytestream</span><span class="p">)</span>
        <span class="n">cols</span> <span class="o">=</span> <span class="n">_read32</span><span class="p">(</span><span class="n">bytestream</span><span class="p">)</span>
        <span class="n">buf</span> <span class="o">=</span> <span class="n">bytestream</span><span class="p">.</span><span class="n">read</span><span class="p">(</span><span class="n">rows</span> <span class="o">*</span> <span class="n">cols</span> <span class="o">*</span> <span class="n">num_images</span><span class="p">)</span>
        <span class="n">data</span> <span class="o">=</span> <span class="n">numpy</span><span class="p">.</span><span class="n">frombuffer</span><span class="p">(</span><span class="n">buf</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">numpy</span><span class="p">.</span><span class="n">uint8</span><span class="p">)</span>
        <span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">num_images</span><span class="p">,</span> <span class="n">rows</span><span class="p">,</span> <span class="n">cols</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">data</span>

<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s">'train-images-idx3-ubyte.gz'</span><span class="p">,</span> <span class="s">'rb'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">images</span> <span class="o">=</span> <span class="n">extract_images</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
        <span class="n">img</span> <span class="o">=</span> <span class="n">Image</span><span class="p">.</span><span class="n">fromarray</span><span class="p">(</span><span class="n">numpy</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">images</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="p">(</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">)),</span> <span class="s">'L'</span><span class="p">)</span>
        <span class="n">img</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="s">'/home/cooli7wa/Desktop/%s.png'</span><span class="o">%</span><span class="n">i</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p><img src="/images/md/chap1_mnist_image.png" alt="" /></p>
:ET