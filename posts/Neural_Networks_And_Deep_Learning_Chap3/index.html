<!DOCTYPE html><html lang="en" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="day-prompt" content="days ago"><meta name="hour-prompt" content="hours ago"><meta name="minute-prompt" content="minutes ago"><meta name="justnow-prompt" content="just now"><meta name="generator" content="Jekyll v4.3.1" /><meta property="og:title" content="Neural Networks And Deep Learning Chap3" /><meta name="author" content="cooli7wa" /><meta property="og:locale" content="en" /><link rel="canonical" href="https://cooli7wa.github.io/posts/Neural_Networks_And_Deep_Learning_Chap3/" /><meta property="og:url" content="https://cooli7wa.github.io/posts/Neural_Networks_And_Deep_Learning_Chap3/" /><meta property="og:site_name" content="Cooli7wa" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2018-02-24T00:00:00+08:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Neural Networks And Deep Learning Chap3" /><meta name="twitter:site" content="@twitter_username" /><meta name="twitter:creator" content="@cooli7wa" /><meta name="google-site-verification" content="google_meta_tag_verification" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"cooli7wa"},"dateModified":"2018-02-24T00:00:00+08:00","datePublished":"2018-02-24T00:00:00+08:00","headline":"Neural Networks And Deep Learning Chap3","mainEntityOfPage":{"@type":"WebPage","@id":"https://cooli7wa.github.io/posts/Neural_Networks_And_Deep_Learning_Chap3/"},"url":"https://cooli7wa.github.io/posts/Neural_Networks_And_Deep_Learning_Chap3/"}</script><title>Neural Networks And Deep Learning Chap3 | Cooli7wa</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="Cooli7wa"><meta name="application-name" content="Cooli7wa"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } static get ID() { return "mode-toggle"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } let self = this; /* always follow the system prefers */ this.sysDarkPrefers.addEventListener("change", () => { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.notify(); }); } /* constructor() */ get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode === ModeToggle.DARK_MODE; } get isLightMode() { return this.mode === ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer)) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } setDark() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_KEY); sessionStorage.removeItem(ModeToggle.MODE_KEY); } /* Notify another plugins that the theme mode has changed */ notify() { window.postMessage({ direction: ModeToggle.ID, message: this.modeStatus }, "*"); } } /* ModeToggle */ const toggle = new ModeToggle(); function flipMode() { if (toggle.hasMode) { if (toggle.isSysDarkPrefer) { if (toggle.isLightMode) { toggle.clearMode(); } else { toggle.setLight(); } } else { if (toggle.isDarkMode) { toggle.clearMode(); } else { toggle.setDark(); } } } else { if (toggle.isSysDarkPrefer) { toggle.setLight(); } else { toggle.setDark(); } } toggle.notify(); } /* flipMode() */ </script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column align-items-end" lang="en"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="/images/cooli7wa.png" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">Cooli7wa</a></div><div class="site-subtitle font-italic"></div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <button class="mode-toggle btn" aria-label="Switch Mode"> <i class="fas fa-adjust"></i> </button> <span class="icon-border"></span> <a href="https://github.com/github_username" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href="https://twitter.com/twitter_username" aria-label="twitter" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['cooli7wa67','163.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>Neural Networks And Deep Learning Chap3</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>Neural Networks And Deep Learning Chap3</h1><div class="post-meta text-muted"><div> By <em> <a href="https://cooli7wa67@163.com">cooli7wa</a> </em></div><div class="d-flex"><div> <span> Posted <em class="timeago" date="2018-02-24 00:00:00 +0800" data-toggle="tooltip" data-placement="bottom" title="Sat, Feb 24, 2018, 12:00 AM +0800" >Feb 24, 2018</em> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="5224 words"> <em>29 min</em> read</span></div></div></div><div class="post-content"> <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script><p><a href="http://neuralnetworksanddeeplearning.com/chap3.html">原文地址</a></p><ul><li>a better choice of cost function, known as the cross-entropy cost function<li>four so-called “regularization” methods (L1 and L2 regularization, dropout, and artificial expansion of the training data)，L1 L2正则化、dropout、虚假扩展数据<li>a better method for initializing the weights in the network<li>a set of heuristics to help choose good hyper-parameters for the network</ul><p>The philosophy is that the best entree to the plethora of available techniques is in-depth study of a few of the most important.</p><p><strong>对现在过多的技术，最好的方式了，深入研究几个最重要的技术</strong></p><h3 id="the-cross-entropy-cost-function">The cross-entropy cost function<a href="#the-cross-entropy-cost-function" class="anchor"><i class="fas fa-hashtag"></i></a></h3></h3><p>loss函数是均方差，激活函数是sigmoid的时候</p>\[\frac{\partial C}{\partial w}=(a-y)\sigma'(z)x=a\sigma'(z)\] \[\frac{\partial C}{\partial b}=(a-y)\sigma'(z)=a\sigma'(z)\]<p><img data-src="/images/md/chap1_sigmoid_1.png" alt="" data-proofer-ignore></p><p>当output接近0或1的时候，函数很平坦，梯度就很小，学习很慢</p><h3 id="introducing-the-cross-entropy-cost-function">Introducing the cross-entropy cost function<a href="#introducing-the-cross-entropy-cost-function" class="anchor"><i class="fas fa-hashtag"></i></a></h3></h3>\[c=-\frac{1}{n}\sum_{x}[ylna+(1-y)ln(1-a)]\]<p>什么样的交叉熵可以做为损失函数？</p><ul><li>函数结果是非负的<li>当实际输出结果和期待的结果接近的时候，函数输出应该接近0 （假设a=y=0或a=y=1）</ul><p>从上面这两点看，交叉熵是合适的</p><p>交叉熵为什么可以防止训练速度慢？</p>\[\frac{\partial C}{\partial w_{j}}=\frac{1}{n}\sum_{x}x_{j}(\sigma(z)-y)\] \[\frac{\partial C}{\partial b}=\frac{1}{n}\sum_{x}(\sigma(z)-y)\]<p>\(\sigma(z)-y\)就是error，所以error越大，学习速率就越大，这个符合人学习的特点。</p><p>另外交叉熵的cost/epoch曲线，更加陡峭</p><p>什么时候使用交叉熵？</p><p>如果激活函数是sigmoid的话，那么损失函数就应该是交叉熵，因为可以放置训练过慢的问题。对于其他的激活函数，并没说。</p>\[C=-\frac{1}{n}\sum_{x}\sum_{j}[y_{j}lna^{L}_{j}+(1-y_{j})ln(1-a^{L}_{j})]\]<p>这个公式和别的地方的函数里用的不一样，这里是两项相加，都当成了概率</p><p>均方差并不是总是会造成学习速率慢的问题，当最后一层的神经元是线性的（也就是没有sigmoid的激活函数），这时的偏导数就不会有这个问题。</p><p>所以来看，学习速率慢的问题，貌似只在sigmoid配合均方差的时候出现。所以均方差也不是用在哪里都不合适。</p><h3 id="using-the-cross-entropy-to-classify-mnist-digits">Using the cross-entropy to classify MNIST digits<a href="#using-the-cross-entropy-to-classify-mnist-digits" class="anchor"><i class="fas fa-hashtag"></i></a></h3></h3><p>cross-entropy对比quadratic cost</p><div class="language-plaintext highlighter-rouge"><div class="code-header"> <span label-text="Plaintext"><i class="fas fa-code small"></i></span> <button aria-label="copy" title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
</pre><td class="rouge-code"><pre>原文
784, 30, 10
30, 10，0.5
    95.42 -&gt; 95.49

784, 100, 10
30, 10，0.5
    96.59 -&gt; 96.82
error: 3.41 -&gt; 3.18，下降了1/12，下降很多了

自己
784, 30, 10
30, 10，0.5
    95.19 -&gt; 95.36

784, 100, 10
30, 10，0.5
    96.51 -&gt; 96.63
</pre></table></code></div></div><p>需要注意的是，这里没有仔细调参，所以通过这个结果直接说，cross-entropy比quadratic要好，不严谨。不过作者也说，实际上确实要好</p><p>为什么关注损失函数？</p><p>the more important reason is that neuron saturation is an important problem in neural nets</p><p>主要原因是，神经元的饱和度是一个特别重要的问题，这里值得花力气来研究，这个也应该是深度学习的主要方面。</p><h3 id="what-does-the-cross-entropy-mean-where-does-it-come-from">What does the cross-entropy mean? Where does it come from?<a href="#what-does-the-cross-entropy-mean-where-does-it-come-from" class="anchor"><i class="fas fa-hashtag"></i></a></h3></h3>\[\sigma'(z)=\sigma(z)(1-\sigma(z))\]<p>这个是从\(\sigma(z)=\frac{1}{1+e^{-z}}\)求导而来</p>\[\frac{\partial C}{\partial w_{j}}=\frac{1}{n}\sum_{x}x_{j}(\sigma(z)-y)\]<p>从上面的公式可以看出来，x也是影响学习速率的一个主要因素，当x接近0的时候，速率也很慢</p><h3 id="softmax">Softmax<a href="#softmax" class="anchor"><i class="fas fa-hashtag"></i></a></h3></h3><p>softmax可以用来解决学习速率慢的问题？</p><p>这里的softmax并不是我理解的那样，只是一个归一化的处理过程。文章这里的softmax是作为一个激活函数来用的，类似于sigmoid。</p><p>当cost为\(c\equiv -lna^{L}_{y}\)（这个叫做log-likelihood cost）时，可以推到出来下面的公式：</p>\[\frac{\partial C}{\partial b^{L}_{j}}=a^{L}_{j}-y_{j}\] \[\frac{\partial C}{\partial w^{L}_{jk}}=a^{L-1}_{k}(a^{L}_{j}-y_{j})\]<p>从这里可以看出来，这个与sigmoid+crossentropy是一样的</p><p>所以说这个也解决了速率慢的问题。</p><p>w的证明过程如下：</p>\[\frac{\partial C}{\partial z^{l}_{j}}=\frac{\partial C}{\partial a}\frac{\partial a}{\partial z^{l}_{j}}=-\frac{1}{a^{l}_{j}}\frac{e^{z^{j}_{l}}\sum-(e^{z^{j}_{l}})^{2}}{\sum^{2}}=..=\frac{e^{z^{j}_{l}}-\sum}{\sum}=a^{l}_{j}-1=a^{l}_{j}-y_{j}\]<p>log-likelihood cost，可以作为损失函数，因为还是那么几点</p><ul><li>非负的，当\(a^{L}_{y}\)为1的时候，cost就是0，也就是第y个神经元的输出与期待1相符，没有偏差<li>当实际输出结果和期待的结果接近的时候，函数输出应该接近0<li>当不接近的时候，输出应该远离0<li>注意\(c\equiv -lna^{L}_{y}\)这里的y，是指输出层第几个神经元的输出，比如对于mnist，如果这里要计算图像与7的偏差，那么这里的y就是7</ul><p>The fact that a softmax layer outputs a probability distribution is rather pleasing. In many problems it’s convenient to be able to interpret the output activation \(a^{L}_{j}\) as the network’s estimate of the probability that the correct output is j.</p><p>softmax的输出可以理解为是一个概率分布，是\(a^{L}_{j}\)是j的概率。</p><p>a network with a sigmoid output layer, the output activations \(a^{L}_{j}\) won’t always sum to 1.</p><p>sigmoid的输出，并不是相加总为1</p><p>You can think of softmax as a way of rescaling the \(z^{L}_{j}\), and then squishing them together to form a probability distribution.</p><p>sigmoid使z成为一个概率分布</p><p>现在知道了两种组合：</p><ul><li>sigmoid + crossentropy<li>softmax + log-likelihood</ul><h3 id="overfitting-and-regularization">Overfitting and regularization<a href="#overfitting-and-regularization" class="anchor"><i class="fas fa-hashtag"></i></a></h3></h3><p>这里的测试，是按照下面的比例来做的，train_data只有1000，因为数据少了，epoch设置为了400，其他没变</p><div class="language-python highlighter-rouge"><div class="code-header"> <span label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre><td class="rouge-code"><pre><span class="o">&gt;&gt;&gt;</span> <span class="kn">import</span> <span class="nn">mnist_loader</span> 
<span class="o">&gt;&gt;&gt;</span> <span class="n">training_data</span><span class="p">,</span> <span class="n">validation_data</span><span class="p">,</span> <span class="n">test_data</span> <span class="o">=</span> \
<span class="p">...</span> <span class="n">mnist_loader</span><span class="p">.</span><span class="n">load_data_wrapper</span><span class="p">()</span>
<span class="o">&gt;&gt;&gt;</span> <span class="kn">import</span> <span class="nn">network2</span> 
<span class="o">&gt;&gt;&gt;</span> <span class="n">net</span> <span class="o">=</span> <span class="n">network2</span><span class="p">.</span><span class="n">Network</span><span class="p">([</span><span class="mi">784</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">10</span><span class="p">],</span> <span class="n">cost</span><span class="o">=</span><span class="n">network2</span><span class="p">.</span><span class="n">CrossEntropyCost</span><span class="p">)</span> 
<span class="o">&gt;&gt;&gt;</span> <span class="n">net</span><span class="p">.</span><span class="n">large_weight_initializer</span><span class="p">()</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">net</span><span class="p">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">training_data</span><span class="p">[:</span><span class="mi">1000</span><span class="p">],</span> <span class="mi">400</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">evaluation_data</span><span class="o">=</span><span class="n">test_data</span><span class="p">,</span>
<span class="p">...</span> <span class="n">monitor_evaluation_accuracy</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">monitor_training_cost</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</pre></table></code></div></div><p><img data-src="/images/md/chap3_overfitting_1.png" style="zoom:100%" data-proofer-ignore></p><p><img data-src="/images/md/chap3_overfitting_2.png" alt="" data-proofer-ignore></p><p><img data-src="/images/md/chap3_overfitting_3.png" alt="" data-proofer-ignore></p><p>这里有个问题</p><ul><li>从accuracy来看，overfitting是从280epoch开始的<li>从cost on test data来看，overfitting是从15epoch开始的</ul><p>那么哪个才是真正的overfitting起始点？</p><p>From a practical point of view, what we really care about is improving classification accuracy on the test data, while the cost on the test data is no more than a proxy for classification accuracy. And so it makes most sense to regard epoch 280 as the point beyond which overfitting is dominating learning in our neural network.</p><p>文中的观点是，280才是，但是这个观点有点草率，后面作者也会说</p><p><img data-src="/images/md/chap3_overfitting_4.png" alt="" data-proofer-ignore></p><p>这里正确率是100%，那就是说，网络记住了整个训练数据，而不是理解</p><p>避免overfitting的方式：</p><ul><li>时刻关注在测试集（验证集）上的正确率，如果正确率停止增长，那么就停止训练。 但是严格来说，这个并不能真正的识别overfitting，因为有时是训练集和测试集同时停止了增长。但这个应该可以防止overfitting的发生。 （这里作者也在说，判断何时算是overfitting，这个应该谨慎，因为训练过程中，有时就会发生一段时间内正确率不上升的情况，这种类似平坦的地形，但是过后，就又会开始上升。）<li>增大数据量<li>正则化（后面会说）</ul><p>这里关于数据集的划分方面，是这样定的</p><ul><li>train<li>validation，在这个数据集上来测试调整过的超参，并选择其他的超参<li>test，在这个数据集上来最终测试网络，这个算是最终的测试集</ul><h3 id="regularization">Regularization<a href="#regularization" class="anchor"><i class="fas fa-hashtag"></i></a></h3></h3><p>减少网络的尺寸，也是一种减少过拟合的方式，但是这个我们一般不会采用，但是这是一种思路，要知道。</p><p>weight decay or L2 regularization，是一回事</p><p>交叉熵的L2：</p>\[C=-\frac{1}{n}\sum_{xj}[y_{j}lna^{L}_{j}+(1-y_{j})ln(1-a^{L}_{j})]+\frac{\lambda}{2n}\sum_{w}w^{2}\]<p>\(\frac{\lambda}{2n}\)，\(\lambda&gt;0\)，叫做正则参数，n是样本的数量，后面会讨论如何选择正则参数</p><p>注意这里只有w，没有b，原因如下：</p><ul><li>Empirically, doing this often doesn’t change the results very much<li>At the same time, allowing large biases gives our networks more flexibility in behaviour<li>in particular, large biases make it easier for neurons to saturate, which is sometimes desirable，饱和是我们希望的？</ul><p>均方差的L2：</p>\[C=\frac{1}{2n}\sum_{x}\left \|y-a^{L}\right \|^{2}+\frac{\lambda}{2n}\sum_{w}w^{2}\]<p>通用形式的L2：</p>\[C=C_{0}+\frac{\lambda}{2n}\sum_{w}w^{2}\]<p>Large weights will only be allowed if they considerably improve the first part of the cost function. ？？这里不明白</p>\[\frac{\partial C}{\partial w}=\frac{\partial C_{0}}{\partial w}+\frac{\lambda}{n}w\] \[\frac{\partial C}{\partial b}=\frac{\partial C_{0}}{\partial b}\] \[b\rightarrow b-\eta \frac{\partial C_{0}}{\partial b}\] \[w\rightarrow w-\eta \frac{\partial C_{0}}{\partial w}-\frac{\eta\lambda }{n}w=(1-\frac{\eta \lambda }{n})w-\eta \frac{\partial C_{0}}{\partial w}\]<p>后来的测试里面，使用正则都要好过没使用正则的结果</p><p>为什么L2正则可以减少overfitting，而且得到更好的结果？</p><ul><li>对于L2正则来说，L2减小了w，w越小，每次迭代w的变化就越小，这样即使样本少，数据不均衡，平均不完全，也可以减小陷入到局部最优解的可能性<li>如果样本多的话，那么学习的目标就多，那么取平均值之后，陷入到少样本的局部最优解里的可能性就小，这是为什么多样本，可以减小overfitting<li>如果样本多，而且使用L2，那么效果自然就更好</ul><h3 id="why-does-regularization-help-reduce-overfitting">Why does regularization help reduce overfitting?<a href="#why-does-regularization-help-reduce-overfitting" class="anchor"><i class="fas fa-hashtag"></i></a></h3></h3><p>One point of view is to say that in science we should go with the simpler explanation, unless compelled not to.</p><p>正则可以抵抗noise</p><p>下面的两个例子，再说的是，正则产生的帮助，是无法简单解释。。</p><p>A network with 100 hidden neurons has nearly 80,000 parameters. We have only 50,000 images in our training data. It’s like trying to fit an 80,000th degree polynomial to 50,000 data points. By all rights, our network should overfit terribly. And yet, as we saw earlier, such a network actually does a pretty good job generalizing.</p><p>我们的输入是50000个点，模型的参数缺有80000个，所以这样看的话，应该会过拟合严重，但是实验的结果却是一个好的结果，这里是一个无法解释的地方。</p><p>“the dynamics of gradient descent learning in multilayer nets has a ‘self-regularization’ effect”</p><p>梯度下降，好像自带正则效果</p><h3 id="other-techniques-for-regularization">Other techniques for regularization<a href="#other-techniques-for-regularization" class="anchor"><i class="fas fa-hashtag"></i></a></h3></h3><ul><li>L1 regularization</ul>\[C=C_{0}+\frac{\lambda}{n}\sum_{w}\left | w \right |\]<p>\(\frac{\partial C}{\partial w}=\frac{\partial C_{0}}{\partial w}+\frac{\lambda}{n}sgn(w)\), sgn(w), w&gt;0:+1 w&lt;0:-1</p>\[w\rightarrow w'=w-\frac{\eta \lambda }{n}sgn(w)-\eta \frac{\partial C_{0}}{\partial w}\]<div class="language-plaintext highlighter-rouge"><div class="code-header"> <span label-text="Plaintext"><i class="fas fa-code small"></i></span> <button aria-label="copy" title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre><td class="rouge-code"><pre>L0：计算非零个数，用于产生稀疏性，但是在实际研究中很少用，因为L0范数很难优化求解，是一个NP-hard问题，因此更多情况下我们是使用L1范数
L1：计算绝对值之和，用以产生稀疏性，因为它是L0范式的一个最优凸近似，容易优化求解
L2：计算平方和再取平均数，L2范数更多是防止过拟合，并且让优化求解变得稳定很快速（这是因为加入了L2范式之后，满足了强凸）。
</pre></table></code></div></div><p>L1和L2的区别是，L1使用的固定值的衰减，而L2使用的是w的比例衰减，所以在w比较大的时候，L2衰减更快，在w小的时候，L1衰减更快。</p><p>The net result is that L1 regularization tends to concentrate the weight of the network in a relatively small number of high-importance connections, while the other weights are driven toward zero.</p><p>L1倾向于保存比较大的w，而其他的w趋于0。</p><p>L1产生稀疏性，因为很多小w，被衰减为0。</p><p>稀疏性的好处是可解释性，即根据非零系数所对应的基的实际意义来解释模型的实际意义，而且可以缩减数据量</p><p>另外L1，需要注意w=0的点，因为</p>\[\left | w \right |\]<p>在这点是不可导的，在实际使用的时候，需要额外处理。</p><ul><li>dropout</ul><p>每个batch是一个循环</p><ol><li>恢复之前dropout的神经元<li>随机砍掉一半的神经元<li>正向反向传播，更新参数</ol><p>有一个地方需要注意，由于实际算出来的参数是只有一半的中间神经元，当做评估的时候，需要使用所有的神经元，所以训练时候得到的参数，应该除以2.</p><p>为什么dropout可以生效？</p><p>The reason is that the different networks may overfit in different ways, and averaging may help eliminate that kind of overfitting.</p><p>And so the dropout procedure is like averaging the effects of a very large number of different networks. The different networks will overfit in different ways, and so, hopefully, the net effect of dropout will be to reduce overfitting.</p><p>因为dropout相当于产生了很多不同模型的网络，每个网络都可能过拟合成不同的方式，而dropout平均化了这些方式，所以可以生效。</p><p>Dropout has been especially useful in training large, deep networks, where the problem of overfitting is often acute.</p><p>当网络大且深的时候，防止过拟合就越来越重要</p><ul><li>artificially increasing the training set size</ul><p><img data-src="/images/md/chap3_train_size_1.png" alt="" data-proofer-ignore></p><p><img data-src="/images/md/chap3_train_size_2.png" alt="" data-proofer-ignore></p><p>两个不同的算法AB，可能发生的情况是，在数据集X上，A要好，在数据集Y上，B要好，所以如果有人问，是A好还是B好，那么应该反问，你选择哪个数据集？</p><p>手写数字识别</p><ul><li>基础情况：98.4<li>加入一些基础扩展，比如旋转等：98.9<li>加入一个特殊的随机的晃动（模拟手写时候的晃动）：99.3</ul><p>It’s fine to look for better algorithms, but make sure you’re not focusing on better algorithms to the exclusion of easy wins getting more or better training data.</p><p>寻找好的算法，也找寻找好的数据，一个好的数据，会使达到好成绩变得简单地多。</p><h3 id="weight-initialization">Weight initialization<a href="#weight-initialization" class="anchor"><i class="fas fa-hashtag"></i></a></h3></h3>\[z=\sum_{j}w_{j}x_{j}+b\]<p>这个公式，当1000个input，其中500个1,500个0，w和b都采用标准正态分布来初始化，那么， z符合N(0,501)的正态分布，这个正态分布就很平，导致的结果就是，z远大于1或者远小于-1的可能性很大，就是|z|取大值的可能性很大。</p><p>这时候，如果激活函数是sigmoid的话，那么\(\sigma(z)\)就很接近1，也就是饱和了，学习速率就很低（这个应该是说的是sigmoid的情况了）。</p><p>We addressed that earlier problem with a clever choice of cost function. Unfortunately, while that helped with saturated output neurons, it does nothing at all for the problem with saturated hidden neurons.</p><p>之前的更换损失函数从均方差到交叉熵，只是解决了输出层的饱和问题，没法解决中间层的饱和问题！</p><p>如何避免这个问题呢？</p><p>可以在初始化的时候，将标准正态分布，换成\(N(0,1/\sqrt{n_{in}})\)，b还是使用的标准正态分布，因为这个有人实验证明过，没有啥影响。</p><p><img data-src="/images/md/chap3_weight_init_1.png" alt="" data-proofer-ignore></p><p>实验结果，可以看出来，虽然最后结果一样，但是上升速度要快，并且提前达到最后的96精度</p><p>However, in Chapter 4 we’ll see examples of neural networks where the long-run behaviour is significantly better with the 1/nin‾‾‾√1/nin weight initialization. Thus it’s not only the speed of learning which is improved, it’s sometimes also the final performance.</p><p>初始化的值，并非只是影响学习速率，也会影响到最终的结果。</p><p>L2 regularization sometimes automatically gives us something similar to the new approach to weight initialization.</p><p>L2正则和优化初始化值的方式，比较相似，都是减小参数</p><h3 id="handwriting-recognition-revisited-the-code">Handwriting recognition revisited: the code<a href="#handwriting-recognition-revisited-the-code" class="anchor"><i class="fas fa-hashtag"></i></a></h3></h3><p>save和load使用的是json，也可以使用pickle 作者的意思是使用json，可以方便在以后更改代码之后，load数据 但是实际上pickle也可以做到这点，以前看过类似的做法</p><h3 id="how-to-choose-a-neural-networks-hyper-parameters">How to choose a neural network’s hyper-parameters?<a href="#how-to-choose-a-neural-networks-hyper-parameters" class="anchor"><i class="fas fa-hashtag"></i></a></h3></h3><p>开启monitor，这个很重要</p><ul><li>Broad strategy<ul><li>可以从10个分类，减少到两个分类，比如先尝试分类0和1，这样会减少时间<li>可以减少网络层数，也可以减少时间，后续再增加层数<li>可以减少训练的数据量，虽然这个可能造成overfitting等，但是现在不是避免这个的时候，现在主要是想测试参数。 前期的时候，能够快速回馈是最重要的事，模型一定要简单，数据也要简单，要记住。</ul><li>Learning rate</ul><p><img data-src="/images/md/chap3_learning_rate_1.png" alt="" data-proofer-ignore></p><p>0.25在最小点附近有震荡，0.025又太慢了</p><p>最好的办法是比如前20epochs使用0.25，后面10个epochs使用0.025</p><p>如何一步步选择rate？</p><p>首先rate，按照作者的经验是，与训练本身的速率有关，所以作者用的是cost作为判断标准，而其他的参数，比如mini-batch，层数等等，都是用的valiadate数据集的准确率作为判断标准。</p><ol><li>先选择一个经验值，比如0.01或者0.1<li>找到上限值。看训练的前几个epoch，如果cost在下降，那么尝试加大rate，如果上升或者震动，那么就减少rate，这样可以找到一个上限值<li>用上限值训练，观察图，如果在最低点附近震荡的话，那么就尝试减少一点rate<li>直到找到一个既迅速又不震荡的rate</ol><ul><li>Use early stopping to determine the number of training epochs</ul><p>在测试集或者验证集上，如果一段时间正确率不上升，就停止训练。</p><p>这个可以简化epoch的选择，并且减少overfitting。</p><p>但是在前期，作者并不建议使用这种方案，作者是希望overfitting发生，然后使用正则来处理。</p><p>如何实现？</p><p>A better rule is to terminate if the best classification accuracy doesn’t improve for quite some time.</p><p>比如，从no-improvement-in-ten开始，需要知道的是，有时模型就是会平坦一段时间，这段时间内没有正确率上升，所以这个判断的标准，要根据实际情况来定。</p><ul><li>Learning rate schedule</ul><p>学习速率递减，怎么递减？</p><p>一种方法是，当验证集的正确率下降的时候开始减小学习速率，可以按照 a factor of two or ten（也就是减小到原来的1/2或者1/10），直到减小到1/1024（or 1/1000），就停止减小。</p><p>需要知道的是，跟early stop一样，这个也增加了额外的参数，也就是增加了需要调整的地方，在前期的时候，没必要这么做，后期追求分数的时候，再这么做。</p><ul><li>The regularization parameter, λ</ul><p>最开始的时候，λ=0.0即可，先调整η ,η 调整到一个合适的数值之后，开始调整 λ，可以按照10倍的速率增加或者减少，当确定好 λ之后，再回过头去继续调整η。</p><ul><li>How I selected hyper-parameters earlier in this book<li>Mini-batch size</ul><p>online_learning(batch为1)，会导致梯度有问题，但是这个不是一个很要紧的事，因为如下：</p><p>It’s as though you are trying to get to the North Magnetic Pole, but have a wonky compass that’s 10-20 degrees off each time you look at it. Provided you stop to check the compass frequently, and the compass gets the direction right on average, you’ll end up at the North Magnetic Pole just fine.</p><p>那是不是online_learning，是最好的选择了呢？因为既可以达到最优点，也频繁得更新参数，不是很好？但是有一点需要注意，因为batch size =1，那么10000个样本的epoch就必须循环10000次，而且没有用到矩阵计算的优势，这样就很慢，学习时间就会长。</p><p>如果batch size太大，那么参数更新就不够频繁，也不好。所以需要综合起来考虑。</p><p>所幸的是，batch_size与其他参数的没啥关系，所以只需定下其他参数，单独优化这个就好。</p><p>所以应该是在最优点一致的情况下，选择一个最大的batch size，使训练时间最短。</p><ul><li>Automated techniques</ul><p>自动优化参数</p><ul><li>Summing up</ul><p>有一些paper介绍了如何选择参数</p><p>选择参数这个问题，现在也没有解决，没有一个统一的方式</p><p>So your goal should be to develop a workflow that enables you to quickly do a pretty good job on the optimization, while leaving you the flexibility to try more detailed optimizations, if that’s important.</p><p>所以就是定下一个自己的流程，这个流程应该可以做出一个比较好的参数，然后再细致得调整。</p><h3 id="other-techniques">Other techniques<a href="#other-techniques" class="anchor"><i class="fas fa-hashtag"></i></a></h3></h3><h4 id="variations-on-stochastic-gradient-descent变种随机梯度下降">Variations on stochastic gradient descent（变种随机梯度下降）<a href="#variations-on-stochastic-gradient-descent变种随机梯度下降" class="anchor"><i class="fas fa-hashtag"></i></a></h4></h4><ul><li>Hessian technique Intuitively, the advantage Hessian optimization has is that it incorporates not just information about the gradient, but also information about how the gradient is changing.<li>momentum</ul><h4 id="other-approaches-to-minimizing-the-cost-function">Other approaches to minimizing the cost function<a href="#other-approaches-to-minimizing-the-cost-function" class="anchor"><i class="fas fa-hashtag"></i></a></h4></h4><p>As you go deeper into neural networks it’s worth digging into the other techniques, understanding how they work, their strengths and weaknesses, and how to apply them in practice.</p><h4 id="other-models-of-artificial-neuron">Other models of artificial neuron<a href="#other-models-of-artificial-neuron" class="anchor"><i class="fas fa-hashtag"></i></a></h4></h4><ul><li>tanh，具体公式不记录了，这个与sigmoid类似，只是output从0,1到了-1,1 一般认为tanh要比sigmoid要好，因为sigmoid由于是非负的，所以对一个神经元来说，所有的w同时上升或者下降，这个不太贴合实际。而tanh，就可以避免这个问题。 但是现在实验起来，tanh比sigmoid又没有多少进步或者进步很小。<li>rectified linear neuron（就是relu） image recognition上面用的比较多，并且比较有效。这种激活函数，没有sigmoid类似函数的饱和问题。</ul><h3 id="network2-code">network2 code<a href="#network2-code" class="anchor"><i class="fas fa-hashtag"></i></a></h3></h3><ul><li>增加了cross-entropy<li>增加了权重衰减<li>增加了Save load<li>增加了monitor<div class="language-python highlighter-rouge"><div class="code-header"> <span label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre><td class="rouge-code"><pre><span class="c1"># 均方差
</span><span class="mf">0.5</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">a</span><span class="o">-</span><span class="n">y</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
<span class="c1"># 交叉熵
</span><span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="o">-</span><span class="n">y</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">a</span><span class="p">)</span><span class="o">-</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">y</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">a</span><span class="p">)))</span> <span class="c1"># nan_to_num，如果是nan则换为0，如果是infinite，则换为inf
</span></pre></table></code></div></div></ul></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/study/'>study</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=Neural Networks And Deep Learning Chap3 - Cooli7wa&url=https://cooli7wa.github.io/posts/Neural_Networks_And_Deep_Learning_Chap3/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=Neural Networks And Deep Learning Chap3 - Cooli7wa&u=https://cooli7wa.github.io/posts/Neural_Networks_And_Deep_Learning_Chap3/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=Neural Networks And Deep Learning Chap3 - Cooli7wa&url=https://cooli7wa.github.io/posts/Neural_Networks_And_Deep_Learning_Chap3/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" title-succeed="Link copied successfully!"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><div class="panel-heading pl-3 pt-2 mb-2">Contents</div><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="tail-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/Neural_Networks_And_Deep_Learning_Chap1/"><div class="card-body"> <em class="timeago small" date="2018-02-22 00:00:00 +0800" >Feb 22, 2018</em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Neural Networks And Deep Learning Chap1</h3><div class="text-muted small"><p> 原文地址 perceptrons（感知器） 1950s-1960s by scientist Frank Rosenblatt 数学模型： 所有权重参数为w1,w2…，threshold threshold，Dropping the threshold means you’re more willing to go to the festival. 与权重b是同一种意...</p></div></div></a></div><div class="card"> <a href="/posts/Neural_Networks_And_Deep_Learning_Chap2/"><div class="card-body"> <em class="timeago small" date="2018-02-23 00:00:00 +0800" >Feb 23, 2018</em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Neural Networks And Deep Learning Chap2</h3><div class="text-muted small"><p> 原文地址 Warm up: a fast matrix-based approach to computing the output from a neural network The two assumptions we need about the cost function 两个假设： \[C=\frac{1}{2n}\sum_x \left \| y(x)-a^{L...</p></div></div></a></div><div class="card"> <a href="/posts/%E4%BB%A5%E5%A4%AA%E5%9D%8A%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0-%E4%BA%A4%E6%98%93%E4%BD%93%E5%88%9B%E5%BB%BA/"><div class="card-body"> <em class="timeago small" date="2019-01-04 00:00:00 +0800" >Jan 4, 2019</em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>以太坊源码学习-交易体创建</h3><div class="text-muted small"><p> 这篇文章学习以太坊转账中交易生成流程。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 // internal/ethapi/api.go // SendTransaction will create a transaction from the given arguments and // tries to sign it with th...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/Neural_Networks_And_Deep_Learning_Chap2/" class="btn btn-outline-primary" prompt="Older"><p>Neural Networks And Deep Learning Chap2</p></a> <a href="/posts/Neural_Networks_And_Deep_Learning_Chap5/" class="btn btn-outline-primary" prompt="Newer"><p>Neural Networks And Deep Learning Chap5</p></a></div></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center text-muted"><div class="footer-left"><p class="mb-0"> © 2024 <a href="https://github.com/cooli7wa">Cooli7wa</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/lozad/dist/lozad.min.js,npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.16.1,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id="></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', ''); }); </script>
