[ { "title": "Stacks - Bitcoin L2", "url": "/posts/Stacks_Bitcoin_L2/", "categories": "study", "tags": "", "date": "2024-02-27 00:00:00 +0800", "snippet": "Stacks 和 Bicoin 的关系Stacks 是 Bitcoin 的 L2，不过有自己的代币（STX），这点和一般的 L2 不同。Stacks 的共识算法（PoX）Stacks 共识算法（PoX）依赖 Bitcoin 的共识（PoW）。文档说 PoX 与其他链使用的 PoB（power of burn）类似，不同一般情况不销毁 Bitcoin，而是将其奖励给维持链活性的 Stackers。Stacks 上竞争出块的矿工，需要支付 Bitcoin，根据 Bitcoin 的多少决定出块概率。PoX 通过 VRF（verifiable random function）来选择出块的矿工，此矿工可以获得 STX 奖励，相当于用 Bitcoin 换取 STX。Stacker 根据持有的 STX 多少和参与出块的程度，获得对应比例的矿工支付的 Bitcoin。Stackers 会预置一些自己的 Bitcoin 地址到地址集合，每次出块，矿工会随机选择一些地址转入 Bitcoin，一旦被选择一次，那么此地址被移除出集合。如果地址集合中的地址不够，那么还是可能会有 Burn 地址来凑数。谁来提交 Bitcoin 交易给 Stacks 验证？矿工 在 Stacks 上提交转账 Bitcoin 的交易。什么是 sBTC？BTC 怎么转的 sBTC？sBTC 和 STX 什么关系？sBTC 是 BTC 在 Stacks 上的映射，相当于解决了 BTC 的流动性问题。Bitcoin 上有一个多签的脚本地址，转给此地址的 BTC 会被转为同等金额的 sBTC，在必要的时候，可以重新转为 BTC。Stacks 有一个 Bitcoin layer 来监听事件。sBTC 和 STX 没有啥关系，STX 是 Stacks 的原生代币。其他Stacks 会将自己的账本 hash 记录在 Bitcoin 上（我没找到记录的方法，我估计是在交易脚本记录的方式），来作为验证的依据，提高自己链的可信度。一些疑问和题外话PoX 让矿工花费 BTC 来换取 STX，而且花费的越多，能得到 STX 的概率就越高。这让我有些怀疑？有矿工愿意这么做吗？将目前价值最高的数字货币换成一个新出现的货币？而且这种做法貌似也不是必须的，大多数有代币的链，都不是这么做的。Stacks 这样有点敛财的感觉呢。虽然 Stacks 的 PoX 和之前的 PoB 有所不同，PoX 大概率不会销毁 BTC（除非地址集合不足，会用 Burn 地址充数），但是还有概率会销毁 BTC 的。作为一个 L1 链 的 L2，却要影响到 L1，而且还要销毁 L1 上本就不多的代币，如果此 L2 真的发展起来，那么 L1 还怎么玩？慢慢的不就是取而代之了吗？这很让人费解。" }, { "title": "Bitcoin Taproot", "url": "/posts/Bitcoin_Taproot/", "categories": "study", "tags": "", "date": "2024-02-23 00:00:00 +0800", "snippet": "Taproot 是 Bitcoin 2021 的一次重要升级，再上一次还是 2017 年的 SegWit。Taproot 带来的好处是更快、更便宜，也提升了隐私性、扩展性和可编程性。包含三个 BIP，BIP340、BIP341、BIP342。BIP 340 - Schnorr Signatures引入了 Schnorr 签名算法，替代之前的 ECDSA。这个算法是 Claus Schnorr 在 1991 年提出的，好处是什么呢？签名更小、验证速度更快、抗攻击性更好。 签名更小，体现在可以将多个签名聚合成一个签名。另外因为是聚合签名，所以更难分辨出单个签名用户，带了个更好的隐私性。对于抵抗虚假签名方面，也更出色。BIP 341 - Taproot主要是引入了两个特性 MAST 和 P2TR。MAST (Merkelized Alternative Script Trees)，类似于之前的 P2SH 使用的单个脚本，MAST 支持多个脚本，同样是在转账交易里植入脚本默克尔树的根，提取这笔钱就需要提供对应的脚本。P2TR (Pay-to-Taproot) ，类似于之前的 P2SH 和 P2PK，新增的一种花费 Bitcoin 的方法而已。BIP 342 - Tapscript这个使 Bitcoin 的脚本语言支持 Schnorr 算法、MAST 和 P2TR。参考https://chain.link/education-hub/schnorr-signature#:~:text=Schnorr%20allows%20for%20smaller%20signature,the%20sum%20of%20its%20keys.https://trustmachines.co/learn/bitcoin-taproot-upgrade-basic-breakdown/" }, { "title": "Bitcoin Address", "url": "/posts/Bitcoin_Address/", "categories": "study", "tags": "", "date": "2024-02-23 00:00:00 +0800", "snippet": "遗留（Legacy）/支付公钥哈希（P2PKH）地址今天，这类型的地址在交易中使用最多的空间，因此也是最昂贵的地址类型。不过这类地址很容易识别，因为这些地址都是以「1」开头的。支付脚本哈希 Pay-to-Script-Hash（P2SH）地址与传统以「1」开头的地址相比，P2SH 地址不是公钥的哈希，而是涉及某些技术脚本的哈希，可用于要求多重签名的转账事宜等，甚至可以利用隔离见证节省交易费用，发送到 P2SH 地址比使用旧地址的钱包便宜约 26%。隔离见证地址（SegWit）Bech32 地址Segwit 地址也称为 Bech32 地址，它们的特性是以 bc1q 开头。这种类型的比特币地址减少了交易中存储的信息量，它们不在交易中存储签名和脚本，而是在见证中，因此，相对 P2SH 地址，Segwit 地址可以节省大约 16% 的交易费用，相对传统地址，节省 38% 以上的费用。由于这种成本节约，它是最常用的比特币交易地址。主根（Taproot）地址为了提高区块空间的效率并改善费用，SegWit 在地址的构造方式上引入了一些变化。因此在 SegWit 地址的基础之上，开发出了以「bc1p」开头的 Taproot 地址，翻译为主根地址，这类地址进一步减小了存储空间，提高了交易效率，并提供了更好的隐私性。https://www.theblockbeats.info/news/37101" }, { "title": "Tornado设计", "url": "/posts/Tornado%E8%AE%BE%E8%AE%A1/", "categories": "study", "tags": "", "date": "2022-11-25 00:00:00 +0800", "snippet": "Tornado 是什么Tornado 是一个去中心化的混币器，用来匿名传送加密货币。什么是混币器？混币器是为了匿名传送加密货币，一般在区块链上交易货币，都是可以查到货币的来源和目标地址的。混币器通过将多人的交易混合在一起，交易中的来源和目标地址不再是简单的一一对应关系，而是多对多的关系。这样就算知道目标地址获得了货币，也不知道货币的来源地址是什么。混币器的分类？有中心化的和非中心化的。中心化的一般是依靠区块链外的第三方机构，交易信息和货币会先发给第三方，由第三方将多笔混淆之后在发送给区块链。好处是实现很简单，坏处是不安全，第三方可能作恶或者泄漏交易信息。去中心化的一般是通过一些技术手段，直接在链上处理交易，也能达到匿名的目的。Tornado 在以太坊上就是一个普通的智能合约，只不过合约内应用了一些譬如零知识证明等技术手段。另外，想做到链上交易的匿名，除了 Tornado 这种类似链上附加的方式来做，还可以在链本身做，比如 Zcash。混币器保护了用户的隐私，但也常常被用来洗黑钱。今年 8 月，美国财政部外国资产控制办公室（OFAC）制裁了 Tornado Cash，将 Tornado 相关的 38 个以太坊地址添加到“特别指定国民”名单中，禁止美国实体及个人与其交互。同时 GitHub 也删除了 Tornado 所有代码，封禁了所有给 Tornado 贡献过代码的账号（不过现在又恢复了，参考第一个链接）。Tornado 设计的核心角色： user。货币交易的参与者，包括发送货币和接受货币的。 relayer。可选的角色，如果想更高的匿名性，那么可以将所有交易发送给 relayer，然后 relayer 以自己的名义发送交易到链上。 contract。以太坊上的 Tornado 合约。其他： Prove 和 Verify。零知识证明的流程，括号内元素为需要的输入参数。deposit 存入货币假设准备发送货币的是用户 A。用户 A 先随机选择两个数，一个是 k（nullifier），一个是 r（secret），这两个数类似于取款密码的作用，然后对两个数拼接后做哈希，的到 commitment（后面会用到）。为什么是两个数？如果只是作为密码，那么一个 r 就够了，Tornado 是为了防止多次提款，所以加入了另一个数 k，在后面取款流程中，如果成功会记录 k 的哈希在合约内，以后如果重复取款，就会发现。用户 A 发送 deposit 交易到 Tornado 的链上合约内（假设没有 relayer），参数包括 commitment 和要存入的货币。这里要存入的货币，是有一定限制的。在 Tornado 的合约内，是包含几个货币池子的，有 0.1、1、10、100 ETH 几种。分池子就是为了匿名，比如 1 ETH 池子内都是一个个的 1 ETH 大小的待取货币，你存了一个，我存了一个，放到池子里就分不出来了。如果你存了 1.1，我存了 1.2，那么放到一起，还是能分出来。那么合约收到 deposit 的交易后会做什么呢？合约会检查对应池子的默克尔树，如果有空位，那么就将 commitment 插入到空位，然后重新计算对应的默克尔树，将旧的 merkle root 记录在合约的 merkle root history 内，新的 merkle root 和 commitment 对应的 merkle path，返回给用户。withdraw 提取货币假设提取货币的是用户 B，B 需要从 A 那要几样东西才能提取货币： 零知识证明 Prove。这个 Prove 的计算公式如图中所示，是为了“证明我知道 secret 和 nullifier 是什么，但是不告诉你具体的数值“，另外还指定了要将货币提取给哪个地址和手续费（如有）。 merkle root。包含上一步存入的 commitment 的默克尔树根。 merkle root 的选择，有点技巧，如果直接用上一步返回的，那么就能将 root 和 commitment 联系起来，因为每次存入 commitment，root 都会变，那么在 withdraw 的时候提供的 root 就可以定位到 commitment，这个肯定是不行的。所以这里是包含 commitment 的就可以，因为合约状态是公开的，所以肯定是可以找到这样的 root。 nullifier hash。nullifier 的哈希。用来防治重复提币。B 构建一个 withdraw 的交易发送到 Tornado 合约，参数包含 Prove、root、nullifer hash、address、fee 等。合约会在 merkle root history 里确定存在参数提到的 root，然后进行零知识证明的 Verify 流程，如果通过代表有资格提取，但是还没完，还要将 nullifier hash 在合约内的 hash list 内进行查找，如果存在代表是重复提现，那么就中止了，如果不存在，那么合约会将 fee 发送到 relayer 地址，将剩余的 (N - fee) ETH 给参数中的接收地址 address，同时在合约的 nullifier hash list 内记录这笔交易的 nullifier hash。安全提示官方的文档里还介绍了一些安全提示，挺好的，这里就直接翻译下贴在这里： Using Relayer or not, you still need to keep up common Internet anonymity like using vpn, proxies, Tor in order to hide the IP address you act from. Since you are using browser an Incognito Tab feature is also useful. 使用 VPN、代理、Tor 等方式来保护自己的 IP。 Make sure you clear cookies for dapps before using your new address, because if a dapp sees both old and new address with the same cookies it will know that addresses are from the same owner. 清理自己的浏览器 cookies。 The note contains data that can be used to link your deposit with withdraw. It is a good idea to make sure that your note data is securely destroyed after the withdrawal is complete. 清理电脑缓存数据。 Wait until there are a few deposits after yours. If your deposit and withdrawal are right next to each other, the observer can guess that this might be the same person. We recommend waiting until there are at least 5 deposits 等你的 deposit 后面又增加了几个别人的 deposit 后，再提现。 Wait until some time has passed after your deposit. Even if there are multiple deposits after yours they all might be made by the same person that is trying to spam deposits and make user falsely believe that there is a large anonymity set. We recommend waiting at least 24h to make sure that there were deposits made by multiple people during that time. 最好还要多等一段时间，比如 24 小时。 一些想法关于技术Tornado 的设计简单有效，从设计思想和受欢迎程度上都看得出来，“简单有效”或许才是真正搞技术的人的追求，而搞营销的人追求的是复杂难懂，同时不可避免的漏洞百出。。关于人文Tornado 作为一个开源的项目，又是部署在公链上，貌似没有人可以干涉它的运行，实际上呢，看看美国对它的制裁吧。所处的环境，使用的人，都是受限的，就没有真正自由的软件，根不是自由的，叶子就不会是自由的。参考https://github.com/tornadocash/tornado-corehttps://tornado.cash/audits/TornadoCash_whitepaper_v1.4.pdfhttps://tornado-cash.medium.com/introducing-private-transactions-on-ethereum-now-42ee915babe0https://mirror.xyz/qiwihui.eth/vvAVr6a5bFxehzz5kyRG0p5e_xSM61e9RaYgjVMNbCU" }, { "title": "以太坊源码学习-数据结构", "url": "/posts/%E4%BB%A5%E5%A4%AA%E5%9D%8A%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/", "categories": "study", "tags": "", "date": "2019-01-04 00:00:00 +0800", "snippet": "这篇学习下以太坊中使用的数据结构。header// Header represents a block header in the Ethereum blockchain.type Header struct { ParentHash common.Hash `json:&quot;parentHash&quot; gencodec:&quot;required&quot;` UncleHash common.Hash `json:&quot;sha3Uncles&quot; gencodec:&quot;required&quot;` Coinbase common.Address `json:&quot;miner&quot; gencodec:&quot;required&quot;` Root common.Hash `json:&quot;stateRoot&quot; gencodec:&quot;required&quot;` TxHash common.Hash `json:&quot;transactionsRoot&quot; gencodec:&quot;required&quot;` ReceiptHash common.Hash `json:&quot;receiptsRoot&quot; gencodec:&quot;required&quot;` Bloom Bloom `json:&quot;logsBloom&quot; gencodec:&quot;required&quot;` Difficulty *big.Int `json:&quot;difficulty&quot; gencodec:&quot;required&quot;` Number *big.Int `json:&quot;number&quot; gencodec:&quot;required&quot;` GasLimit uint64 `json:&quot;gasLimit&quot; gencodec:&quot;required&quot;` GasUsed uint64 `json:&quot;gasUsed&quot; gencodec:&quot;required&quot;` Time *big.Int `json:&quot;timestamp&quot; gencodec:&quot;required&quot;` Extra []byte `json:&quot;extraData&quot; gencodec:&quot;required&quot;` MixDigest common.Hash `json:&quot;mixHash&quot;` Nonce BlockNonce `json:&quot;nonce&quot;`} ParentHash，父区块的区块头的 Hash，RLP 之后 Keccak256 得到。 UncleHash，叔区块区块头的 Hash，RLP 之后 Keccak256 得到。 Coinbase，挖到区块的地址 Root，stateRoot，账户状态的 MPT 树根 TxHash，transactionsRoot，交易状态的 MPT 树根 ReceiptHash，receiptsRoot，收据状态的 MPT 树根 Bloom，logsBloom，日记 Bloom 过滤器由可索引信息（日志地址和日志主题）组成，这个信息包含在每个日志入口 Difficulty，difficulty，难度值 Number，number，区块编号, 等于当前区块的直系前辈区块数量（创始区块的区块编号为0） GasLimit，gasLimit，目前每个区块的燃料消耗上限 GasUsed，gasUsed，当前区块的所有交易使用燃料之和 Time，当前时间戳 Extra，额外的数据 MixDigest，mixHash，混合哈希, 与一个与随机数 (nonce)相关的 256 位哈希计算, 用于证明针对当前区块已经完成了足够的计算 Nonce，随机数block// Block represents an entire block in the Ethereum blockchain.type Block struct { header *Header uncles []*Header transactions Transactions // caches hash atomic.Value size atomic.Value // Td is used by package core to store the total difficulty // of the chain up to and including the block. td *big.Int // These fields are used by package eth to track // inter-peer block relay. ReceivedAt time.Time ReceivedFrom interface{}} header，区块头指针 uncles，叔区块的区块头指针 transactons，所有的交易 hash，区块头进行 RLP 之后求 hash 值 size，区块头 RLP 之后的大小 td，total difficulty，到目前为止总的难度值 ReceivedAt，接收到区块的时间 ReceivedFrom，从哪个节点接收到的区块transactionstype Transaction struct { data txdata // caches hash atomic.Value size atomic.Value from atomic.Value}type txdata struct { AccountNonce uint64 `json:&quot;nonce&quot; gencodec:&quot;required&quot;` Price *big.Int `json:&quot;gasPrice&quot; gencodec:&quot;required&quot;` GasLimit uint64 `json:&quot;gas&quot; gencodec:&quot;required&quot;` Recipient *common.Address `json:&quot;to&quot; rlp:&quot;nil&quot;` // nil means contract creation Amount *big.Int `json:&quot;value&quot; gencodec:&quot;required&quot;` Payload []byte `json:&quot;input&quot; gencodec:&quot;required&quot;` // Signature values V *big.Int `json:&quot;v&quot; gencodec:&quot;required&quot;` R *big.Int `json:&quot;r&quot; gencodec:&quot;required&quot;` S *big.Int `json:&quot;s&quot; gencodec:&quot;required&quot;` // This is only used when marshaling to JSON. Hash *common.Hash `json:&quot;hash&quot; rlp:&quot;-&quot;`} hash，本交易的 RLP 的哈希 size，本交易的 RLP 的大小 from，交易的发起者 data AccountNonce，账户的 nonce 值，账户模型中为了消除重复交易 Price，gasPrice，交易的 gas 价格 GasLimint，gas 上限 Recipient，收款方地址 Amoount，转账金额 Payload，合约代码或者输入参数 V，R，S，属于签名数据，通过这三个可以推算出公钥，也就可以算出 from Hash，这个不太清楚什么用处？ " }, { "title": "以太坊源码学习-处理区块", "url": "/posts/%E4%BB%A5%E5%A4%AA%E5%9D%8A%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0-%E5%A4%84%E7%90%86%E5%8C%BA%E5%9D%97/", "categories": "study", "tags": "", "date": "2019-01-04 00:00:00 +0800", "snippet": "这篇学习下处理区块的流程，这里包括创建和调用 evm、计算 gas 消耗、更新状态树和 receipt等。// Process processes the state changes according to the Ethereum rules by running// the transaction messages using the statedb and applying any rewards to both// the processor (coinbase) and any included uncles.//// Process returns the receipts and logs accumulated during the process and// returns the amount of gas that was used in the process. If any of the// transactions failed to execute due to insufficient gas it will return an error.func (p *StateProcessor) Process(block *types.Block, statedb *state.StateDB, cfg vm.Config) (types.Receipts, []*types.Log, uint64, error) { var ( receipts types.Receipts usedGas = new(uint64) header = block.Header() allLogs []*types.Log gp = new(GasPool).AddGas(block.GasLimit()) ) // Mutate the block and state according to any hard-fork specs if p.config.DAOForkSupport &amp;amp;&amp;amp; p.config.DAOForkBlock != nil &amp;amp;&amp;amp; p.config.DAOForkBlock.Cmp(block.Number()) == 0 { misc.ApplyDAOHardFork(statedb) } // Iterate over and process the individual transactions for i, tx := range block.Transactions() { statedb.Prepare(tx.Hash(), block.Hash(), i) receipt, _, err := ApplyTransaction(p.config, p.bc, nil, gp, statedb, header, tx, usedGas, cfg) if err != nil { return nil, nil, 0, err } receipts = append(receipts, receipt) allLogs = append(allLogs, receipt.Logs...) } // Finalize the block, applying any consensus engine specific extras (e.g. block rewards) p.engine.Finalize(p.bc, header, statedb, block.Transactions(), block.Uncles(), receipts) return receipts, allLogs, *usedGas, nil}GasPool 是区块 gas 消耗的上限。这里的主要代码是，for 循环，这里会单独处理每一个交易，调用 ApplyTransaction()，返回 receipt。Prepare 代码：// Prepare sets the current transaction hash and index and block hash which is// used when the EVM emits new state logs.func (self *StateDB) Prepare(thash, bhash common.Hash, ti int) { self.thash = thash self.bhash = bhash self.txIndex = ti}ApplyTransaction() 代码：// ApplyTransaction attempts to apply a transaction to the given state database// and uses the input parameters for its environment. It returns the receipt// for the transaction, gas used and an error if the transaction failed,// indicating the block was invalid.func ApplyTransaction(config *params.ChainConfig, bc ChainContext, author *common.Address, gp *GasPool, statedb *state.StateDB, header *types.Header, tx *types.Transaction, usedGas *uint64, cfg vm.Config) (*types.Receipt, uint64, error) { msg, err := tx.AsMessage(types.MakeSigner(config, header.Number)) if err != nil { return nil, 0, err } // Create a new context to be used in the EVM environment context := NewEVMContext(msg, header, bc, author) // Create a new environment which holds all relevant information // about the transaction and calling mechanisms. vmenv := vm.NewEVM(context, statedb, config, cfg) // Apply the transaction to the current state (included in the env) _, gas, failed, err := ApplyMessage(vmenv, msg, gp) if err != nil { return nil, 0, err } // Update the state with pending changes var root []byte if config.IsByzantium(header.Number) { statedb.Finalise(true) } else { root = statedb.IntermediateRoot(config.IsEIP158(header.Number)).Bytes() } *usedGas += gas // Create a new receipt for the transaction, storing the intermediate root and gas used by the tx // based on the eip phase, we&#39;re passing whether the root touch-delete accounts. receipt := types.NewReceipt(root, failed, *usedGas) receipt.TxHash = tx.Hash() receipt.GasUsed = gas // if the transaction created a contract, store the creation address in the receipt. if msg.To() == nil { receipt.ContractAddress = crypto.CreateAddress(vmenv.Context.Origin, tx.Nonce()) } // Set the receipt logs and create a bloom for filtering receipt.Logs = statedb.GetLogs(tx.Hash()) receipt.Bloom = types.CreateBloom(types.Receipts{receipt}) return receipt, gas, err}先将 tx 转化为了 Message 类型，然后构造了 evm 环境，调用其 ApplyMessage 函数，这时的 evm 已经包含了所有需要的数据。IntermediateRoot 这个不知道什么用处？后面就是将所有处理结果和日志存储在 receipt 内，如果是创建合约的交易，那么还会存储合约地址。gas 的消耗，如果只是转账，是固定的 21000，如果是创建合约、或者是跟合约交互，那么跟具体的操作有关，可以看下 params/protocol_params.go 这里的预设值。// AsMessage returns the transaction as a core.Message.//// AsMessage requires a signer to derive the sender.//// XXX Rename message to something less arbitrary?func (tx *Transaction) AsMessage(s Signer) (Message, error) { msg := Message{ nonce: tx.data.AccountNonce, gasLimit: tx.data.GasLimit, gasPrice: new(big.Int).Set(tx.data.Price), to: tx.data.Recipient, amount: tx.data.Amount, data: tx.data.Payload, checkNonce: true, } var err error msg.from, err = Sender(s, tx) return msg, err}// ApplyMessage computes the new state by applying the given message// against the old state within the environment.//// ApplyMessage returns the bytes returned by any EVM execution (if it took place),// the gas used (which includes gas refunds) and an error if it failed. An error always// indicates a core error meaning that the message would always fail for that particular// state and would never be accepted within a block.func ApplyMessage(evm *vm.EVM, msg Message, gp *GasPool) ([]byte, uint64, bool, error) { return NewStateTransition(evm, msg, gp).TransitionDb()}// TransitionDb will transition the state by applying the current message and// returning the result including the used gas. It returns an error if failed.// An error indicates a consensus issue.func (st *StateTransition) TransitionDb() (ret []byte, usedGas uint64, failed bool, err error) { if err = st.preCheck(); err != nil { return } msg := st.msg sender := vm.AccountRef(msg.From()) homestead := st.evm.ChainConfig().IsHomestead(st.evm.BlockNumber) contractCreation := msg.To() == nil // Pay intrinsic gas gas, err := IntrinsicGas(st.data, contractCreation, homestead) if err != nil { return nil, 0, false, err } if err = st.useGas(gas); err != nil { return nil, 0, false, err } var ( evm = st.evm // vm errors do not effect consensus and are therefor // not assigned to err, except for insufficient balance // error. vmerr error ) if contractCreation { ret, _, st.gas, vmerr = evm.Create(sender, st.data, st.gas, st.value) } else { // Increment the nonce for the next transaction st.state.SetNonce(msg.From(), st.state.GetNonce(sender.Address())+1) ret, st.gas, vmerr = evm.Call(sender, st.to(), st.data, st.gas, st.value) } if vmerr != nil { log.Debug(&quot;VM returned with error&quot;, &quot;err&quot;, vmerr) // The only possible consensus-error would be if there wasn&#39;t // sufficient balance to make the transfer happen. The first // balance transfer may never fail. if vmerr == vm.ErrInsufficientBalance { return nil, 0, false, vmerr } } st.refundGas() st.state.AddBalance(st.evm.Coinbase, new(big.Int).Mul(new(big.Int).SetUint64(st.gasUsed()), st.gasPrice)) return ret, st.gasUsed(), vmerr != nil, err}preCheck 中会 buyGas，这里会将 limit × price 的 gas 先扣除掉。后面处理过程中，会计算真实的 gas 消耗： IntrinsicGas，是固定消耗，包含基础的转账消耗（或者是创建合约消耗），和 data 数据大小对应的 gas 消耗。 调用智能合约的消耗然后剩余的 gas 会在 refundGas 中归还，最后计算挖矿奖励。unc (st *StateTransition) preCheck() error { // Make sure this transaction&#39;s nonce is correct. if st.msg.CheckNonce() { nonce := st.state.GetNonce(st.msg.From()) if nonce &amp;lt; st.msg.Nonce() { return ErrNonceTooHigh } else if nonce &amp;gt; st.msg.Nonce() { return ErrNonceTooLow } } return st.buyGas()}func (st *StateTransition) buyGas() error { mgval := new(big.Int).Mul(new(big.Int).SetUint64(st.msg.Gas()), st.gasPrice) if st.state.GetBalance(st.msg.From()).Cmp(mgval) &amp;lt; 0 { return errInsufficientBalanceForGas } if err := st.gp.SubGas(st.msg.Gas()); err != nil { return err } st.gas += st.msg.Gas() st.initialGas = st.msg.Gas() st.state.SubBalance(st.msg.From(), mgval) return nil}" }, { "title": "以太坊源码学习-交易体创建", "url": "/posts/%E4%BB%A5%E5%A4%AA%E5%9D%8A%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0-%E4%BA%A4%E6%98%93%E4%BD%93%E5%88%9B%E5%BB%BA/", "categories": "study", "tags": "", "date": "2019-01-04 00:00:00 +0800", "snippet": "这篇文章学习以太坊转账中交易生成流程。// internal/ethapi/api.go// SendTransaction will create a transaction from the given arguments and// tries to sign it with the key associated with args.To. If the given passwd isn&#39;t// able to decrypt the key it fails.func (s *PrivateAccountAPI) SendTransaction(ctx context.Context, args SendTxArgs, passwd string) (common.Hash, error) { if args.Nonce == nil { // Hold the addresse&#39;s mutex around signing to prevent concurrent assignment of // the same nonce to multiple accounts. s.nonceLock.LockAddr(args.From) defer s.nonceLock.UnlockAddr(args.From) } signed, err := s.signTransaction(ctx, &amp;amp;args, passwd) if err != nil { log.Warn(&quot;Failed transaction send attempt&quot;, &quot;from&quot;, args.From, &quot;to&quot;, args.To, &quot;value&quot;, args.Value.ToInt(), &quot;err&quot;, err) return common.Hash{}, err } return submitTransaction(ctx, s.b, signed)}如果 args.Nonce == nil，那么后面会通过 GetPoolNonce() 来获取一个 Nonce，这里需要上锁，也可以指定 Nonce。signTransaction 是构造交易和签名的流程。// signTransaction sets defaults and signs the given transaction// NOTE: the caller needs to ensure that the nonceLock is held, if applicable,// and release it after the transaction has been submitted to the tx poolfunc (s *PrivateAccountAPI) signTransaction(ctx context.Context, args *SendTxArgs, passwd string) (*types.Transaction, error) { // Look up the wallet containing the requested signer account := accounts.Account{Address: args.From} wallet, err := s.am.Find(account) if err != nil { return nil, err } // Set some sanity defaults and terminate on failure if err := args.setDefaults(ctx, s.b); err != nil { return nil, err } // Assemble the transaction and sign with the wallet tx := args.toTransaction() var chainID *big.Int if config := s.b.ChainConfig(); config.IsEIP155(s.b.CurrentBlock().Number()) { chainID = config.ChainID } return wallet.SignTxWithPassphrase(account, passwd, tx, chainID)}setDefaults 是对交易体做一些默认配置。toTransaction 是构造交易体。SignTxWithPassphrase 是签名流程。先看下 setDefaults ：// setDefaults is a helper function that fills in default values for unspecified tx fields.func (args *SendTxArgs) setDefaults(ctx context.Context, b Backend) error { if args.Gas == nil { args.Gas = new(hexutil.Uint64) *(*uint64)(args.Gas) = 90000 } if args.GasPrice == nil { price, err := b.SuggestPrice(ctx) if err != nil { return err } args.GasPrice = (*hexutil.Big)(price) } if args.Value == nil { args.Value = new(hexutil.Big) } if args.Nonce == nil { nonce, err := b.GetPoolNonce(ctx, args.From) if err != nil { return err } args.Nonce = (*hexutil.Uint64)(&amp;amp;nonce) } if args.Data != nil &amp;amp;&amp;amp; args.Input != nil &amp;amp;&amp;amp; !bytes.Equal(*args.Data, *args.Input) { return errors.New(`Both &quot;data&quot; and &quot;input&quot; are set and not equal. Please use &quot;input&quot; to pass transaction call data.`) } if args.To == nil { // Contract creation var input []byte if args.Data != nil { input = *args.Data } else if args.Input != nil { input = *args.Input } if len(input) == 0 { return errors.New(`contract creation without any data provided`) } } return nil}如果不提供 gas 限制值，那么默认是 90000.如果不提供 GasPrice，那么会通过 SuggestPrice() 获得一个推荐值。如果没有 nonce， 那么会通过 GetPoolNonce() 来得到一个。在 to == nil 的时候，是创建智能合约，那么会对 data 和 input 进行一系列检查。下面是构造交易体流程：func (args *SendTxArgs) toTransaction() *types.Transaction { var input []byte if args.Data != nil { input = *args.Data } else if args.Input != nil { input = *args.Input } if args.To == nil { return types.NewContractCreation(uint64(*args.Nonce), (*big.Int)(args.Value), uint64(*args.Gas), (*big.Int)(args.GasPrice), input) } return types.NewTransaction(uint64(*args.Nonce), *args.To, (*big.Int)(args.Value), uint64(*args.Gas), (*big.Int)(args.GasPrice), input)}func NewTransaction(nonce uint64, to common.Address, amount *big.Int, gasLimit uint64, gasPrice *big.Int, data []byte) *Transaction { return newTransaction(nonce, &amp;amp;to, amount, gasLimit, gasPrice, data)}func newTransaction(nonce uint64, to *common.Address, amount *big.Int, gasLimit uint64, gasPrice *big.Int, data []byte) *Transaction { if len(data) &amp;gt; 0 { data = common.CopyBytes(data) } d := txdata{ AccountNonce: nonce, Recipient: to, Payload: data, Amount: new(big.Int), GasLimit: gasLimit, Price: new(big.Int), V: new(big.Int), R: new(big.Int), S: new(big.Int), } if amount != nil { d.Amount.Set(amount) } if gasPrice != nil { d.Price.Set(gasPrice) } return &amp;amp;Transaction{data: d}}下面是签名的流程：// SignTxWithPassphrase signs the transaction if the private key matching the// given address can be decrypted with the given passphrase.func (ks *KeyStore) SignTxWithPassphrase(a accounts.Account, passphrase string, tx *types.Transaction, chainID *big.Int) (*types.Transaction, error) { _, key, err := ks.getDecryptedKey(a, passphrase) if err != nil { return nil, err } defer zeroKey(key.PrivateKey) // Depending on the presence of the chain ID, sign with EIP155 or homestead if chainID != nil { return types.SignTx(tx, types.NewEIP155Signer(chainID), key.PrivateKey) } return types.SignTx(tx, types.HomesteadSigner{}, key.PrivateKey)}先通过密码得到解密的密钥，然后调用 SignTx 来签名，注意的是 defer zeroKey，这里保证在函数结束的时候，会清空内存中的私钥。// SignTx signs the transaction using the given signer and private keyfunc SignTx(tx *Transaction, s Signer, prv *ecdsa.PrivateKey) (*Transaction, error) { h := s.Hash(tx) sig, err := crypto.Sign(h[:], prv) if err != nil { return nil, err } return tx.WithSignature(s, sig)}先求 hash 然后做 sign。看下 hash 的过程：// Hash returns the hash to be signed by the sender.// It does not uniquely identify the transaction.func (fs FrontierSigner) Hash(tx *Transaction) common.Hash { return rlpHash([]interface{}{ tx.data.AccountNonce, tx.data.Price, tx.data.GasLimit, tx.data.Recipient, tx.data.Amount, tx.data.Payload, })}这里是 rlpHash，即先做 RLP 转换，然后做 Keccak256 hash。这里 hash 的数据，包括：账户 nonce，gas price，gas limint，目标地址，金额，额外数据。// submitTransaction is a helper function that submits tx to txPool and logs a message.func submitTransaction(ctx context.Context, b Backend, tx *types.Transaction) (common.Hash, error) { if err := b.SendTx(ctx, tx); err != nil { return common.Hash{}, err } if tx.To() == nil { signer := types.MakeSigner(b.ChainConfig(), b.CurrentBlock().Number()) from, err := types.Sender(signer, tx) if err != nil { return common.Hash{}, err } addr := crypto.CreateAddress(from, tx.Nonce()) log.Info(&quot;Submitted contract creation&quot;, &quot;fullhash&quot;, tx.Hash().Hex(), &quot;contract&quot;, addr.Hex()) } else { log.Info(&quot;Submitted transaction&quot;, &quot;fullhash&quot;, tx.Hash().Hex(), &quot;recipient&quot;, tx.To()) } return tx.Hash(), nil}这里提交交易体到网络上，并且如果是创建智能合约的交易，那么会将智能合约地址计算出来，并记录到 log，最后返回交易体的 hash。// CreateAddress creates an ethereum address given the bytes and the noncefunc CreateAddress(b common.Address, nonce uint64) common.Address { data, _ := rlp.EncodeToBytes([]interface{}{b, nonce}) return common.BytesToAddress(Keccak256(data)[12:])}智能合约的地址是通过 创建者地址 和 nonce 计算出来的，可以保证唯一。再看下 SendTx 的流程：func (b *EthAPIBackend) SendTx(ctx context.Context, signedTx *types.Transaction) error { return b.eth.txPool.AddLocal(signedTx)}// AddLocal enqueues a single transaction into the pool if it is valid, marking// the sender as a local one in the mean time, ensuring it goes around the local// pricing constraints.func (pool *TxPool) AddLocal(tx *types.Transaction) error { return pool.addTx(tx, !pool.config.NoLocals)}// addTx enqueues a single transaction into the pool if it is valid.func (pool *TxPool) addTx(tx *types.Transaction, local bool) error { pool.mu.Lock() defer pool.mu.Unlock() // Try to inject the transaction and update any state replace, err := pool.add(tx, local) if err != nil { return err } // If we added a new transaction, run promotion checks and return if !replace { from, _ := types.Sender(pool.signer, tx) // already validated pool.promoteExecutables([]common.Address{from}) } return nil}主要的验证流程在 add 内：// add validates a transaction and inserts it into the non-executable queue for// later pending promotion and execution. If the transaction is a replacement for// an already pending or queued one, it overwrites the previous and returns this// so outer code doesn&#39;t uselessly call promote.//// If a newly added transaction is marked as local, its sending account will be// whitelisted, preventing any associated transaction from being dropped out of// the pool due to pricing constraints.func (pool *TxPool) add(tx *types.Transaction, local bool) (bool, error) { // If the transaction is already known, discard it hash := tx.Hash() if pool.all.Get(hash) != nil { log.Trace(&quot;Discarding already known transaction&quot;, &quot;hash&quot;, hash) return false, fmt.Errorf(&quot;known transaction: %x&quot;, hash) } // If the transaction fails basic validation, discard it if err := pool.validateTx(tx, local); err != nil { log.Trace(&quot;Discarding invalid transaction&quot;, &quot;hash&quot;, hash, &quot;err&quot;, err) invalidTxCounter.Inc(1) return false, err } // If the transaction pool is full, discard underpriced transactions if uint64(pool.all.Count()) &amp;gt;= pool.config.GlobalSlots+pool.config.GlobalQueue { // If the new transaction is underpriced, don&#39;t accept it if !local &amp;amp;&amp;amp; pool.priced.Underpriced(tx, pool.locals) { log.Trace(&quot;Discarding underpriced transaction&quot;, &quot;hash&quot;, hash, &quot;price&quot;, tx.GasPrice()) underpricedTxCounter.Inc(1) return false, ErrUnderpriced } // New transaction is better than our worse ones, make room for it drop := pool.priced.Discard(pool.all.Count()-int(pool.config.GlobalSlots+pool.config.GlobalQueue-1), pool.locals) for _, tx := range drop { log.Trace(&quot;Discarding freshly underpriced transaction&quot;, &quot;hash&quot;, tx.Hash(), &quot;price&quot;, tx.GasPrice()) underpricedTxCounter.Inc(1) pool.removeTx(tx.Hash(), false) } } // If the transaction is replacing an already pending one, do directly from, _ := types.Sender(pool.signer, tx) // already validated if list := pool.pending[from]; list != nil &amp;amp;&amp;amp; list.Overlaps(tx) { // Nonce already pending, check if required price bump is met inserted, old := list.Add(tx, pool.config.PriceBump) if !inserted { pendingDiscardCounter.Inc(1) return false, ErrReplaceUnderpriced } // New transaction is better, replace old one if old != nil { pool.all.Remove(old.Hash()) pool.priced.Removed() pendingReplaceCounter.Inc(1) } pool.all.Add(tx) pool.priced.Put(tx) pool.journalTx(from, tx) log.Trace(&quot;Pooled new executable transaction&quot;, &quot;hash&quot;, hash, &quot;from&quot;, from, &quot;to&quot;, tx.To()) // We&#39;ve directly injected a replacement transaction, notify subsystems go pool.txFeed.Send(NewTxsEvent{types.Transactions{tx}}) return old != nil, nil } // New transaction isn&#39;t replacing a pending one, push into queue replace, err := pool.enqueueTx(hash, tx) if err != nil { return false, err } // Mark local addresses and journal local transactions if local { if !pool.locals.contains(from) { log.Info(&quot;Setting new local account&quot;, &quot;address&quot;, from) pool.locals.add(from) } } pool.journalTx(from, tx) log.Trace(&quot;Pooled new future transaction&quot;, &quot;hash&quot;, hash, &quot;from&quot;, from, &quot;to&quot;, tx.To()) return replace, nil} 如果 hash 在池内已经存在，那么忽略 验证 tx 的有效性 如果池子已经满了，tx 不是 local，而且tx的价格最低，那么忽略，否则的话，清除目前最低价格的腾出一个位置。 如果 from 和 nonce 都相同，那么就替换掉之前的交易，否则就插入队列。 更新下 pool 的 locals 列表。// validateTx checks whether a transaction is valid according to the consensus// rules and adheres to some heuristic limits of the local node (price and size).func (pool *TxPool) validateTx(tx *types.Transaction, local bool) error { // Heuristic limit, reject transactions over 32KB to prevent DOS attacks if tx.Size() &amp;gt; 32*1024 { return ErrOversizedData } // Transactions can&#39;t be negative. This may never happen using RLP decoded // transactions but may occur if you create a transaction using the RPC. if tx.Value().Sign() &amp;lt; 0 { return ErrNegativeValue } // Ensure the transaction doesn&#39;t exceed the current block limit gas. if pool.currentMaxGas &amp;lt; tx.Gas() { return ErrGasLimit } // Make sure the transaction is signed properly from, err := types.Sender(pool.signer, tx) if err != nil { return ErrInvalidSender } // Drop non-local transactions under our own minimal accepted gas price local = local || pool.locals.contains(from) // account may be local even if the transaction arrived from the network if !local &amp;amp;&amp;amp; pool.gasPrice.Cmp(tx.GasPrice()) &amp;gt; 0 { return ErrUnderpriced } // Ensure the transaction adheres to nonce ordering if pool.currentState.GetNonce(from) &amp;gt; tx.Nonce() { return ErrNonceTooLow } // Transactor should have enough funds to cover the costs // cost == V + GP * GL if pool.currentState.GetBalance(from).Cmp(tx.Cost()) &amp;lt; 0 { return ErrInsufficientFunds } intrGas, err := IntrinsicGas(tx.Data(), tx.To() == nil, pool.homestead) if err != nil { return err } if tx.Gas() &amp;lt; intrGas { return ErrIntrinsicGas } return nil} 交易的大小不能超过 32K 签名不能小于0 交易gas limit 不能小于现在 pool 的 limits 什么是 signer? signer 可以用来从 tx 得到 from 如果不是 local，那么 price 不能小于现在池子的 price nonce 不能小于 池子的 nonce 余额要足够 计算实际的 gas 消耗，不能大于交易的 gas limit 都没问题的话，就返回成功" }, { "title": "区块链周报-2018年51周", "url": "/posts/%E5%8C%BA%E5%9D%97%E9%93%BE%E5%91%A8%E6%8A%A5-2018%E5%B9%B451%E5%91%A8/", "categories": "report", "tags": "", "date": "2018-12-24 00:00:00 +0800", "snippet": "2018-12-18 港交所对批准矿机巨头的IPO申请表示“犹豫” 香港证券交易所对于批准这些比特币挖矿公司上市非常犹豫，因为他们认为这个行业波动性太大、风险极高，甚至可能在一两年时间里彻底消失。香港证券交易所不愿成为第一个吃螃蟹的人，也不希望成为世界上第一个批准加密货币挖矿公司上市的证券交易所。 2018-12-20 EOS 游戏被黑客攻击，超500万元损失 大部分是竞猜类游戏，这次攻击方式还是传统的双花，利用合约漏洞。 2018-12-21 比特币重回4000 USDT上方，刷新近半月新高 一种说法是受到比特币高点（2017年12月）一周年的影响，中午 12:40 是 $4081 委内瑞拉成为全球最大的加密货币手机市场 Dash记录了委内瑞拉特殊环境推动的又一个采用里程碑，销售超过66000台KRIP手机，这些手机设计有针对Dash生态系统优化的特殊加密货币增强功能。 信通院区块链主管：我国超80%的区块链技术平台使用国外开源技术 卿博士的文章，中国区块链企业仅次于美国，专利也很多，但是源码贡献度不高，自主技术平台也不多，大部分是采用国外开源的超级账本、以太坊等。 全球首个受监管的区块链房地产众筹平台将在瑞士上线 2019 年初上线 " }, { "title": "区块链周报-2018年50周", "url": "/posts/%E5%8C%BA%E5%9D%97%E9%93%BE%E5%91%A8%E6%8A%A5-2018%E5%B9%B450%E5%91%A8/", "categories": "report", "tags": "", "date": "2018-12-17 00:00:00 +0800", "snippet": "2018-12-10 香港金管局：已拒绝部分虚拟银行申请者，争取明年首季开始发牌 香港区块链协会秘书长诸葛畅：香港经管局与七大银行正在研究分布式账本 香港区块链协会秘书长诸葛畅表示，香港经管局牵头正与七大银行一起做分布式账本研究，涉及数据分析、按揭业务、贸易融资等。目前七大银行已在按揭业务资产确权与估值方面应用了这一技术，显著提高了这一过程的时间，降低了人为失误。 日本将采取措施制止加密货币交易逃税行为 韩国新财长：拟向数字货币和初始数字货币发行征税 美国众议院议员计划提出两项加密货币法案，以保护消费者并保持美国的竞争力 V神发布CBC Casper教程并称要解释它 这里有全面介绍，https://github.com/ethereum/cbc-casper/wiki 2019年起，委内瑞拉只用石油币出售石油产品 法币贬值到这种地步，会有人相信政府来买石油币么？ 2018-12-12 “微信支付区块链电子发票”覆盖深圳所有微信支付商家 12月11日，国家税务局总局深圳市税务局区打通微信支付平台，推出“微信支付区块链电子发票”功能。据此前消息，今年十月，微信支付电子区块链发票已经在深圳部分商家试点使用。而这一次正式上线，意味着深圳所有开通微信支付的商家，都能登陆自己的微信支付商户平台直接开通区块链电子发票功能。 有点厉害了 中国信通院推出《区块链的技术发展与应用》 研究报告 好像还没有公开下载 三星提交了三个区块链商标申请 三星已经提交了三个商标申请，揭示了其在手机区块链和加密货币软件方面的计划。三个申请分别为区块链密钥存储、区块链密钥箱和区块链核心。 主要是手机秘钥管理，手机区块链这地方没提。 htc 之前推出了一个只能用数字货币购买的手机，exodus1，0.15比特币，现在买断货了。。 美国SEC主席：若遵循规则，ICO是筹集资金的“有效途径” V神：区块链非金融应用比金融应用更具优势 一改之前说的金融和游戏领域 2018-12-13 “门头沟”前 CEO 或被判处 10 年刑期 起诉理由是嫌疑人涉嫌通过门头沟账户非法窃取用户 3 亿 4100 万日元的资产。 Mt.Gox 2010 年就转变为比特币交易所了（之前是游戏线上交易平台），很多漏洞当时就存在，而且很可能监守自盗 比特大陆又遭起诉，UnitedCorp称其操纵BCH硬分叉 2018-12-14 首单区块链保险秒赔现身 多数传统险企观望 日前，首单支付宝区块链理赔出现，用户从提交申请到理赔款到账，全程用时仅5秒。 中国银行首席研究员：区块链或使整个货币发生变革 支付，尤其是跨境支付 P2P 数字货币 IEEE正式立项《供应链金融中的区块链标准》 近日，IEEE正式立项《供应链金融中的区块链标准》，这是IEEE首个评审通过的金融业区块链标准。这一标准将定义基于区块链的供应链金融通用框架、角色模型、典型业务流程、技术要求、安全要求等。 " }, { "title": "区块链周报-2018年49周", "url": "/posts/%E5%8C%BA%E5%9D%97%E9%93%BE%E5%91%A8%E6%8A%A5-2018%E5%B9%B449%E5%91%A8/", "categories": "report", "tags": "", "date": "2018-12-10 00:00:00 +0800", "snippet": "2018-12-03 中本聪在白皮书发布网站上的个人状态已更新 添加了一个单词”nour” 。。。 法中委员会秘书长林碧溪：中法两国深入区块链等技术合作 央行货币金银局局长王信：法定数字货币的研发已经提上日程 法定数字货币 中国区块链技术应用反诈骗中心正式成立 重拳整治打击利用披着区块链外衣发行的传销币、分叉币、山寨币、资金盘、电信诈骗，以及自媒体写黑稿诈骗、新型网络犯罪等违法活动 杭州余杭创新园区正式开园，包含区块链孵化器 温州成立区块链研究中心 力争到2022年，全市打造2至3个区块链产业基地 比特大陆推出比特大陆数字货币指数 该指数追踪市值最大且流动性佳的数字货币的表现，采用美元计价。 目前投资加密领域的基金超过900家，约70%专注投资处于非常早期的项目 Smart Dubai（智慧迪拜）计划推行政府无纸化办公，并在未来几年落地20余项“区块链+政务”应用 区块链参考架构的国家标准 正在立项和起草过程中 工信部互联网金融安全技术重点实验室主任吴震表示，已经在制定区块链参考架构的国家标准，现在正在立项和起草过程中，会对区块链的术语、参考架构、角色、功能模块进行定义。 2018-12-04 intel 节能技术可以减少比特币挖矿 15% 能量消耗 继 SGX 之后的又一动作 重庆市政府将利用区块链等技术建立涉企数据安全保密责任制度 主要是记录企业数据，建立评价体系 美国SEC主席：加密、比特币和区块链应该符合我们的规则 国证券交易委员会（SEC）主席 Jay Clayton 表示，SEC 有健全的规则，经受住了时间的考验，不应该为了适应技术而改变这些规则，“技术应该能够符合我们的规则。我认为这项技术对提高我们市场的效率有着不可思议的前景，但我不会仅仅因为新技术就改变那些保护投资者方面的发行规则或交易规则。” 美国怀俄明州立法委员会投票通过区块链银行法案 该法案允许银行提供基于区块链资产的金融服务 2018-12-05 基于区块链技术的征信数据共享平台麻袋财富，想解决数据获取、变现的痛点 许可制机制，入网成员提供成员私有数据查询业务，查询记录记录在区块链进行费用计算。 北京市互金协会发布《关于防范以STO名义实施违法犯罪活动的风险提示》 有人利用 STO 名义继续从事宣传培训、项目推广、融资。 2018-12-07 以太坊创世地址半个月清空43.6万枚ETH 以太坊是ICO融资的，这个地址是融资人中的一个 丹麦超1500家餐厅接受比特币订餐 江卓尔发文分析屯币61.6万BTC+75万BCH的神秘处女座 这波熊市之后，币就更集中了，比特币是PoW的，币集中可以说不是什么问题，也可以说有些问题，毕竟不是法币。 美国政府出资80万美元寻求防伪区块链解决方案 美国华裔物理学家、丹华资本创始人张首晟教授辞世 张首晟教授生前高度评价区块链技术，并对区块链行业有深入研究和重要贡献。 " }, { "title": "区块链周报-2018年48周", "url": "/posts/%E5%8C%BA%E5%9D%97%E9%93%BE%E5%91%A8%E6%8A%A5-2018%E5%B9%B448%E5%91%A8/", "categories": "report", "tags": "", "date": "2018-12-03 00:00:00 +0800", "snippet": "2018-11-26 比特币继续下跌,跌破 3500 点2018-11-27 证券型 TOKEN 创企 Securitize 完成 1270 万美元 A 轮融资 将证卷产品转移到去中心化账本 https://36kr.com/p/5164051.html 比特币价格闪崩引起矿圈大关机 目前的算力情况看，虽然散户关机无数，但像比特币这种主力币的算力整体没掉太多。“多一个离场者就少一个算力竞争，能在这个空档坚持下来的矿工，就能提升自身算力在全网的比重，即出块概率。大庄都在拼命堆机器。” 目前比特币价格 3700 左右 https://36kr.com/p/5163992.html 巴拉圭支持建立“世界上最大的”比特币矿场 韩国的区块链技术基金会与政府达成了至少五年的交易 https://36kr.com/p/5163964.html 韩国 TTC Protocol 去中心化社交网络 该协议采用了多层 BFT-DPoS 共识机制, 单链处理速度为每秒上千笔交易,用户的登录浏览、发布、评论、点赞、转发行为，可以获得数量不等的 TTC 代币激励 https://36kr.com/p/5164056.html 俄罗斯移动运营商与联邦储蓄银行完成区块链商业债券交易 据报道，俄罗斯中央证券存管机构——国家结算存托所(NSD)宣布，俄罗斯移动网络运营商MTS与俄罗斯国有银行俄罗斯联邦储蓄银行之间完成了一笔基于区块链的商业债券交易。 俄罗斯联邦储蓄银行全球市场部门负责人副总裁Andrei Shemetov表示，区块链对银行业来说非常重要，因为它能降低成本、大幅增加交易的速度和安全性，并提供空前的信任和交易透明度。 http://www.bitcoin86.com/block/32198.html Overstock宣布放弃其零售业务转投区块链，股价瞬间飙升23% 近期，美国互联网零售商Overstock股价飙升。此前该公司首席执行官Patrick Byrne宣布，计划放弃其零售业务，并专注于此前收购的区块链初创公司。 overstock 成立于1997年, overstock 在2015年7月开始接收比特币交易。这是一家放弃了主营业务转投区块链的公司。 http://www.bitcoin86.com/block/32165.html 关于 STO https://36kr.com/p/5163981.html " }, { "title": "加密数字货币的“悖论”", "url": "/posts/%E5%8A%A0%E5%AF%86%E6%95%B0%E5%AD%97%E8%B4%A7%E5%B8%81%E7%9A%84-%E6%82%96%E8%AE%BA/", "categories": "study", "tags": "", "date": "2018-12-03 00:00:00 +0800", "snippet": "今天看到一篇文章《加密数字货币的 7 个致命悖论》，挺有意思，这里逐条说下自己的看法。悖论是说命题中有两个都能自圆其说的对立的结论。拥堵悖论对传统交换媒介来说，使用的人越多越好。就像电信或者社交网络里面，网络外部性意味着用户越多，对其他人注册的吸引力就越大。此外，大多数传统平台均从规模经济中受益：因为它们的成本基本上是固定的，将其分散在更多的交易中会降低平均成本。但是加密数字货币平台却不一样。它们的成本基本上是可变的，它们的容量却基本上是固定的。就像高峰时间的伦敦地铁一样，加密数字货币平台容易受到拥堵的影响：客户越多会让它们吸引力下降。其中一些（并非全部）容量非常受限：据估计比特币每秒的交易量为7，visa的是24000。更多的交易为了获得处理而竞争会导致堵塞和延迟。交易费被迫提高以消除过多的需求。所以比特币的高交易成本随着交易需求的增加变得愈发恶化而不是改善。 这里举的例子不是很好，用比特币10分钟出一个块来举例要好很多。任何交易系统肯定是TPS越高越好，但是太高的TPS对比特币的共识算法并不是好事，PoW算法中出块过快，会导致分叉太多，对比特币的稳定性有很大的影响。 但是基于比如PoS/DPoS的共识算法没这个问题，TPS可以做得很高，所以拥堵悖论并不是在所有数字货币上都存在的。存储悖论讽刺的是，虚拟加密数字货币要依赖于分布式账本，这个东西靠系统规模数字化存储成本分摊，容易受到规模不经济的严重影响。每一位用户都必须维护整个的交易历史的一个副本，意味着总的存储需求是N平方倍。BIS已经计算出涵括美国所有零售交易的假设性分布账本的数字，测算结果是在2年半内存储需求将超过100GB/用户。 分布式账本希望大家都存储副本，这样可以增加冗余，对抗灾难，但是这样会造成巨大的浪费。 这个应该不能算是悖论，只能说是冗余备份的一个缺点，而且并非数字货币独有，比如传统的数据库备份，都有这样的问题。那么这个缺点现在能怎么解决呢？数据归档、checkpoint、分片或许可以解决一部分问题。挖矿悖论用新的货币单位奖赏“矿工”处理交易导致了用户与矿工之间的紧张关系。这一点在比特币就一个区块能处理多少交易的冲突中已经表现得很清楚了。矿工希望这个保持小规模：保持货币的不流动性，造成更多的拥堵并且提高交易费用——从而增加矿工的奖赏，因为后者面对的能源密集型交易验证规模一直在涨。但用户想要的恰恰相反：更高的容量，更低的交易成本，以及更好的流动性，所以更偏爱更大的区块大小。 挖矿奖励确实引发了很多问题，使矿工之间不合作，主要的原因是利益竞争，这个是机制的问题。 但是挖矿奖励不是必须的，在peercoin的PoS算法中，矿工费是直接消耗掉的，矿工间就没有了这种直接的冲突，也就不会为了矿工费来互相恶意竞争，算是解决了这个问题（但是有其他的问题）。集中悖论尽管支持者屡屡提及去中心化、非居间化以及大众化，但大多数加密数字货币却展现出了极高的所有权集中度——通常是在矿工以及/或者“Hodlers”手中。据估计97%的比特币被仅4%的地址所持有，每一个区块都存在不平等。集中的投资者的欲望以及极端化的情绪使得大玩家很难变现，因为卖出的行动会导致价格直线下跌……资产是靠易手时的市场价来估值的。任何时候都只有一小部分交易的发生。所以价格反映的其实是边际市场参与者的看法。你可以靠买入更多来提高所持有的资产价格，因为你的购买会推高市场价格。但意识到受益需要卖出——这得有别人成为边际买家，从而又会让市场价格下挫。对于很多资产来说这些流动性的影响是很小的。但对于加密数字货币来说其影响却要大得多因为1）交易是不流动的，2）一些玩家相对于市场体量庞大3）买家和卖家之间没有天然平衡4）观点更加易变和极化。高价格反映的是囤积居奇，而不是轻易卖给一批有购买意愿的买家的能力。对于一些所有权集中度高的资产来说，投资者有时候会害怕占主导地位的玩家卖出。相对于中国持有的美国国债，或者央行持有的黄金量，加密数字货币的所有权集中度——以及赎回诱发的崩溃风险都要高得多。 币集中的问题，我觉得是两个方面造成的： 一是这个游戏越来越成为少数人的游戏，因为ASIC，普通人参与不进来；二是币的发行是递减的，大家奇货可居。但是如果只是说货币集中，那么不是什么大问题，现在的所有法币不都是集中在少数人手里么，至少不会比现在差。 这里说的悖论是，作为持有者，我希望价格高，那么就不能卖手里的币（大家都卖了必然跌），但是因为这个不是法币，如果想变现又必须卖，所以这里是有悖论的。其实囤积所有商品都有这个悖论，但是可能是因为币量有限和币集中，对数字货币来说就大很多。另外如果数字货币是法币，就没有了商品这层属性，这种悖论本身就不存在了。 主要麻烦的是算力集中，这个是与区块链技术本身矛盾，这个是机制设计的问题。估值悖论经济理论的谜题是为什么私有的加密数字货币会有价值可言。资产定价的贴现现金流模型说价值来自于（风险调整后的折现净现值）未来收入。对于政府债券来说就是利息+偿还本金，对于股票来说就是股息，对于房地产来说就是房租。这些收益流的定价计算会hen复杂，但对于没有收益的加密数字货币来说计算就很简单：零收入意味着零价值。第二个来源是“固有价值”。黄金没有股息但作为制作珠宝的商品或者个人使用来说是有价值的。香烟在军营的囚犯中是作为商品货币流通的，因为它们具有消费价值。但加密数字货币没有内在价值。一些人认为挖矿保本的电力消耗为加密数字货币的价格提供了一个基础。但是，用Jon Danielsson的话来说：“挖矿的成本是沉没成本，而不是未来收入的保障。”如果我浪费了150英镑雇人去寻找和挖掘埋葬在父母花园的我童年时的宠物龟的残骸，那些成本并不能让那些骨头在投资者眼里值150英镑。还有其他的价值来源吗？也许就只有未来加密数字货币的价值会比今天更高从而抛售获得利润的期望？就像保罗·克鲁格曼认为那样，问题在于，如果它们的“价值完全依赖于自我实现的期望”的话，这就是教科书对泡沫的定义。 很有意思的是这句话，“挖矿的成本是沉没成本，而不是未来收入的保障。”如果我浪费了150英镑雇人去寻找和挖掘埋葬在父母花园的我童年时的宠物龟的残骸，那些成本并不能让那些骨头在投资者眼里值150英镑。” 这个谈不上是悖论，就是说数字货币本身没有价值支撑而已，这个本来就是这样，纸币本身有价值么？是没有的，有的是大家都认可的是它的交换价值，而交换价值，我觉得主要还是看币的稳定性。匿名悖论加密数字货币提供的（更大）的匿名性其实是劣势而不是优势。诚然，这给洗钱、避税以及违法商品提供者提供了核心的交易需求，因为它们使得资金和交易者很难被跟踪。但对于（范围大得多）的合法金融交易来说，这却是缺点。这使得检测不法行为更加困难，并限制了补救行动/执法行动的实施。尽管区块链能够验证支付是否被接收并防止重复付款（虽然不够完美），但很多问题仍未解决。首先，当交易和持有无法回溯制定方时，市场操纵或者公开欺诈的风险在提高 匿名性使比特币有了特有的应用场景，比如暗网，匿名性也限制了比特币的应用场景。 不同应用场景，使用不同的数字货币，匿名或者不匿名或者对于监管不匿名，这样应该会不错。创新悖论也许最大的讽刺在于，你对未来加密数字货币越乐观，你就得对其今天的价值越悲观。当被消费时，商品就实现了自身价值，货币则是从相信自身可被用于支付，及/或在未来保持价值中获得价值。预期将来它会变得毫无价值的话，现在它就会变得毫无价值。如果新的加密数字货币出现来解决当前这批加密数字货币的问题的话，则今天的加密数字货币就会被取代，从而变得毫无价值。当然，除非现有的加密数字货币能采用新出现的加密数字货币的任何新功能。 这里说的是，加密数字货币领域肯定是需要不断创新的，但是如果创新，那么新货币的出现会使旧货币失去价值，使价值归零。 现在数字货币大概有几千个，新的货币的出现并没有使旧货币归零，同一个货币的技术更新也一直在进行，比如比特币版本一直在更迭，但是没有影响价值。所以我觉得更可能是升级，不是取代。总归来看，这里的所谓的“悖论”都是已知的一些问题，但是很多算不上是悖论，有一些现在就有解决方案，其他的以后技术发展了，也是有可能解决掉的。" }, { "title": "区块链周报-2018年47周", "url": "/posts/%E5%8C%BA%E5%9D%97%E9%93%BE%E5%91%A8%E6%8A%A5-2018%E5%B9%B447%E5%91%A8/", "categories": "report", "tags": "", "date": "2018-11-23 00:00:00 +0800", "snippet": "2018-11-20 今日国家网信办官网刊发“区块链组稿”，正面肯定Token价值 这八篇文章分别是《区块链发展是一次长跑》、《区块链技术处于早期阶段》、《密码学是区块链的信任之源》、《“区块链+金融”擦出智慧火花》、《区块链在实体经济中的落地》、《区块链的未来应用方向》、《加快研制区块链相关标准》、《防范安全漏洞 促进实际应用》 国家一直是“限制币鼓励链”，各地区块链产业园的相继建立，北京香港先后出台政策支持区块链发展，区块链本身的政策前景是好的。 http://www.bitcoin86.com/block/31818.html 于佳宁：区块链可帮助实体经济实现降成本 中国计算机学会区块链专业委员会委员于佳宁发文表示，区块链已经成为世界级的技术创新新高地，也是国际竞争新赛道，区块链技术日益成为国家产业竞争力的重要基石。 http://www.bitcoin86.com/block/31817.html 井贤栋：区块链不是为创新而存在 是为解决真正问题 蚂蚁金服支付宝第一天开始我们极其关注信用体系的建设，支付宝用一个服务的机制解决了信用的问题，创造担保交易。芝麻信用第一次用数据技术解决了所谓信用分这样一个信用的问题，而我们今天相信区块链是会用共识的算法解决信用问题。 解决信用问题一直是蚂蚁金服的核心。 http://www.bitcoin86.com/block/31793.html 俄罗斯联邦储蓄银行：区块链的大规模应用需要1-2年时间 “围绕区块链技术的炒作目前已经结束，该技术正进入产业化发展阶段，需要一两年的时间才能实现规模化的应用。” 格里夫认为，鉴于区块链“技术尚不成熟”，全球市场“尚未做好” 区块链大规模商业应用的准备。 泡沫过去是好事，真正的核心技术可以发展。 http://www.bitcoin86.com/block/31774.html 数字货币整体大跌，比特币接连跌破5200-4700美元六道整数关口。 比特币目前是 4800 左右。 比特币的前景不是很乐观，应用场景单一是一方面，另一方面是技术不太成熟。 游戏公司 Mythical Games 完成 1600 万美元 A 轮融资，拟在 EOS 上发布新产品 https://36kr.com/p/5162743.html 2018-11-21 工商银行：采用区块链技术成功发放首笔数字信用凭据融资 利用区块链技术，将核心企业与各层级供应商间的采购资金流与贸易流集成到区块联盟平台上，供应链上任一持有应收款数字凭据的供应商均可在线向工商银行提出融资申请，申请指令经工商银行智慧信贷平台批准后，贷款可瞬间到达企业账户，有效缓解了处于产业链末端、实力较弱企业的资金压力。 供应链金融的应用 http://www.bitcoin86.com/block/31885.html 杭州金融办官员：区块链将助推杭州打造数字经济第一城 http://www.bitcoin86.com/block/31875.html 2018-11-22 比特币暴跌16%至4100美元以下，创下今年新低 目前 4500 左右 https://36kr.com/p/5163077.html 美国执法机构调查去年的比特币价格是否被操控 https://36kr.com/p/5163220.html 比特币价格大幅波动，矿工群体关机止损：有矿主已亏损上亿 二手转卖，基本是卖废铁。比特大陆范晓俊：目前机器难卖，买矿机相当于买币 https://finance.qq.com/a/20181121/009071.htm 中国人民银行研究局邹平座：比特币是数字资产，而非数字货币 近日，在2018北京亚太经济合作年会“数字时代的全球共识与共享”上，中国人民银行研究局研究员邹平座表示，我们正在利用区块链技术来实现人民币的数字化。比特币是数字资产，而不是数字货币，它们不具备成为“货币”的条件。 https://www.odaily.com/newsflash/128838 中国银行业协会潘光伟：正在推动筹建区块链跨行交易平台 银行机构应顺应数字化发展大势，主动将数字化转型上升为全行战略，作为转型发展的重要突破口，确立新的考核维度和考核指标体系，全面推进数字化建设。 http://difang.gmw.cn/sz/2018-11/21/content_32020843.htm 美国顶级矿场Giga Watt已申请破产 https://36kr.com/p/5163251.html 深圳文化产权交易所推出基于区块链技术的“文版通系统” http://collection.sina.com.cn/wjs/2018-11-20/doc-ihmutuec2057199.shtml 食品电商跨界区块链，「奥丁丁」落地食物溯源、旅宿管理场景 基于以太坊，视频溯源和旅宿管理 https://36kr.com/p/5161901.html 2018-11-23 韩国对ICO禁令的裁决推迟至12月 猜测的是，一部分来源于韩国企业区块链业务的出走。 https://36kr.com/p/5163513.html EOS项目方疑高位套现30亿，大部分去向不明 CEO : 如果太透明，会损失竞争优势 https://36kr.com/p/5163450.html 法国政府同意2.4万当地烟草商店明年开卖比特币 在法国，所谓的”烟草商店”更像便利店，向当地居民出售各种各样的商品。这个允许在“烟草商店”出售比特币的举措几乎是作为一项和平协议来推出的，以补偿政府在未来几年里提高烟草价格的计划（这一进展旨在将一盒香烟的价格推高至10欧元）。 法国一直积极拥抱比特币 https://36kr.com/p/5163385.html " }, { "title": "以太坊幽灵协议", "url": "/posts/%E4%BB%A5%E5%A4%AA%E5%9D%8A%E5%B9%BD%E7%81%B5%E5%8D%8F%E8%AE%AE/", "categories": "study", "tags": "", "date": "2018-11-19 00:00:00 +0800", "snippet": "这篇文章要再写写以太坊的幽灵协议（ghost），这个在之前的以太坊白皮书里简单介绍过，不是很详细，最近又看到了一些文章，发现自己之前想的不是很全面，所以这篇再总结下。1. 幽灵协议提出的背景“幽灵“协议（”Greedy Heaviest Observed Subtree” (GHOST) protocol）是由Yonatan Sompolinsky 和 Aviv Zohar在2013年12月引入的创新。幽灵协议提出的动机是当前快速确认的块链因为区块的高作废率而受到低安全性困扰 幽灵协议提出的动机是当前快速确认的块链因为区块的高作废率而受到低安全性困扰因为区块需要花一定时间（设为t）扩散至全网，如果矿工A挖出了一个区块然后矿工B碰巧在A的区块扩散至B之前挖出了另外一个区块，矿工B的区块就会作废并且没有对网络安全作出贡献。 这是第一个危害，浪费算力，也就是算力没有对网络安全做出贡献。此外，这里还有中心化问题：如果A是一个拥有全网30%算力的矿池而B拥有10%的算力，A将面临70%的时间都在产生作废区块的风险而B在90%的时间里都在产生作废区块。因此，如果作废率高，A将简单地因为更高的算力份额而更有效率，综合这两个因素，区块产生速度快的块链很可能导致一个矿池拥有实际上能够控制挖矿过程的算力份额。 这是另一个危害，高作废率导致了算力低的节点一直在同步别人的区块，永远的落后别人一步，高算力的节点实际上就获得了更高的效率，从而更多得控制整个网络。2. 幽灵协议内容幽灵协议引入了“叔区块”的概念。叔块，其定义是直系父区块的兄弟区块，这种区块在之前是直接作废的。协议的内容简单来说就是，挖叔区块的人可以获得一定量的奖励，挖父区块的人如果包含别人的叔区块，也可以额外获得奖励，另外在计算最长链的时候把叔区块也考虑在内，综合最长的才是最长链。看下下面这张图：如果单纯的计算最长链原则，主链应该是 0&amp;lt;-1B&amp;lt;-2D&amp;lt;-3F&amp;lt;-3F&amp;lt;-4C&amp;lt;-5B.如果采用了 GHOST 协议, 以前的”废块”也会被考虑到主链的计算量中。每一个节点下含一个子树, 兄弟节点之间子树节点最多的被选为主链。这样一来，0&amp;lt;-1B&amp;lt;-2C&amp;lt;-3D&amp;lt;-4B 成为主链。如果采用 GHOST 协议， 一个攻击者仅仅提供一个 1A 到 6A 的长链并不能被认为是主链。对于 4B 来说，它的直系父节点有 3D, 2C, 1B 等等，这些父节点的兄弟节点 3C, 3E, 2B, 2D, 1A 都是 4B 的叔节点。奖励是怎么计算的呢？ 挖叔区块的节点会获得 ((叔块高度+8-当前块的高度)/8 ) 的基本奖励。 假设 3D 是新产生的区块，对于其叔块 2B 而言，高度相差1，所以它的奖励是（-1+8）/8 基本奖励，即 7/8 基本奖励。而如果是 1A 能获得奖励，则它获得 6/8*基本奖励，依次类推。 挖主链区块的节点会获得基本奖励和矿工费，另外因为包含了叔区块最多可以额外获得 1/32 的基本奖励。 另外，一个节点最多包含2个叔块。一个叔块只能有一个后继。3. 这样做会有什么效果呢？首先挖叔区块的人会获得奖励，这样就会激励他们把挖出来的区块广播出去，挖主链的人呢，也因为有激励，会把叔区块包含进来，而最长链是按照主块和叔区块整体来算的，这样之前的废区块就为链的安全尽了一份力。而且因为有奖励，挖叔区块的人，更倾向于选择挖主链，而不是选择忽略主链而只维持自己的链（因为挖主链会损失所有自己已挖块的奖励）。这样综合来看，就是幽灵协议使主分支团结了其他的分支，让大家更愿意一起合作，链就不是只由算力高的节点控制，链的安全性就更高。4. 参考https://github.com/ethereum/wiki/wiki/%5B%E4%B8%AD%E6%96%87%5D-%E4%BB%A5%E5%A4%AA%E5%9D%8A%E7%99%BD%E7%9A%AE%E4%B9%A6#%E5%BA%94%E7%94%A8https://cooli7wa.com//2018/08/19/%E6%AF%94%E7%89%B9%E5%B8%81%E4%BB%A5%E5%A4%AA%E5%9D%8A%E7%99%BD%E7%9A%AE%E4%B9%A6/https://www.jianshu.com/p/a7082e03293ehttp://www.aquagemini.com/understanding-sto-security-token-offering/https://www.jianshu.com/p/69cbbdcaf2cc" }, { "title": "区块链周报-2018年46周", "url": "/posts/%E5%8C%BA%E5%9D%97%E9%93%BE%E5%91%A8%E6%8A%A5-2018%E5%B9%B446%E5%91%A8/", "categories": "report", "tags": "", "date": "2018-11-16 00:00:00 +0800", "snippet": "我想在博客加个区块链周报，记录下区块链要闻来跟紧区块链行业趋势，也是督促自己在学习技术的时候，也不要忘了抬头看看环境。这是第一期周报，这期没有链接，我觉得还是得加入链接，方便大家看原文，下期开始加入吧，应该也会多写些自己的看法。2018-11-13 网易星球上线不到两月，日活百万 区块链公司 EVAIO 计划向 FF 投资 9 亿美元 贵州、新疆矿场被要求停电整改，接收税务和实名制检查2018-11-14 全国首张区块链不动产电子凭证在娄底诞生。娄底率先在湖南省成立了首个区块链产业园区 韩国Naver计划用区块链技术进军日本搜索市场保护搜索所需数据 赵长鹏：币安非常看好STO，市场非常大STO，security token offer，证劵型通证发行2018-11-15 北京时间 11 月 16 日凌晨 0 点 40 分， BCH 将执行硬分叉，这场在吴忌寒和 CSW 之间的 BCH 分叉之争终将迎来一个结果。2018-11-16 16 日 01:52，SVPool 矿池挖出最后一个公共区块后，BCH 链劈成两半，再往下挖便分为由比特大陆支持的 BCH ABC 链和 CSW（澳本聪）带领的 BSV 链。目前ABC 链已经小幅甩开了 SV 链（超过 16 个区块），但并没有人宣布获胜或退出，烧钱游戏还在继续。 比特币暴跌，部分稳定币出现大幅溢价比特币跌破5600美元关口 京东金融推出“京东互保” 引入区块链技术“京东互保”类似支付宝“互相保”，两者保障的疾病种类都是100种，等待90天，手续费10% 。互相保需要芝麻分650以上，京东互保需要提前缴费。 网易公布第三季度财报，将区块链技术用于游戏研发中 李鸣，区块链早期还是要回归技术的本质，脱离开技术的研究和应用的实践，谈论经济、社会都是空洞的。 李开复：对区块链仍持观望态度 1.POW问题如何解决。目前尚未出现解决方案； 2. 如何解决币、链捆绑过紧的问题。李开复表示，相信会出现与加密货币完全脱钩的区块链应用，但目前还没有看到； 3. ICO是否会得到明确的监管。在监管方面，美国和欧盟已经有所行动，中国也可能会跟随他们的步伐。但直到监管明确之前，自己都会继续持观望态度。 " }, { "title": "ZooKeeper介绍", "url": "/posts/ZooKeeper%E4%BB%8B%E7%BB%8D/", "categories": "study", "tags": "", "date": "2018-11-08 00:00:00 +0800", "snippet": "这篇来学习下 ZooKeeper，全部内容来源于网上两篇文章（在参考内），将两篇内容融合在一起，先有个大体的了解，算是入门吧。1. 基础概念ZooKeeper 是一个开源的分布式协调服务，分布式应用程序可以基于 ZooKeeper 实现诸如数据发布/订阅、负载均衡、命名服务、分布式协调/通知、集群管理、Master 选举、分布式锁和分布式队列等功能。ZooKeeper 的设计目标是将那些复杂且容易出错的分布式一致性服务封装起来，构成一个高效可靠的原语集，并以一系列简单易用的接口提供给用户使用。2. 一些特性 ZooKeeper 本身就是一个分布式程序，为了保证高可用，最好是以集群形态来部署，这样只要集群中大部分机器是可用的（能够容忍一定的机器故障），那么 ZooKeeper 本身仍然是可用的。 ZooKeeper 将数据保存在内存中，这也就保证了高吞吐量和低延迟（但是内存限制了能够存储的容量不太大，此限制也是保持 ZNode 中存储的数据量较小的进一步原因）。 ZooKeeper 是高性能的。 在“读”多于“写”的应用程序中尤其地高性能，因为“写”会导致所有的服务器间同步状态。（“读”多于“写”是协调服务的典型场景。） ZooKeeper 有临时节点的概念。 当创建临时节点的客户端会话一直保持活动，瞬时节点就一直存在。而当会话终结时，瞬时节点被删除。持久节点是指一旦这个 ZNode 被创建了，除非主动进行 ZNode 的移除操作，否则这个 ZNode 将一直保存在 Zookeeper 上。 ZooKeeper 底层其实只提供了两个功能：①管理（存储、读取）用户程序提交的数据；②为用户程序提交数据节点监听服务。 ZooKeeper 最好使用奇数台服务器构成集群，因为使用的是 ZAB 算法，而 ZAB 是多数算法，所以在保证可靠性一样的情况下，奇数可以减少一台服务器的使用。3. 集群角色ZooKeeper 中没有选择传统的 Master/Slave 概念，而是引入了Leader、Follower 和 Observer 三种角色。ZooKeeper 集群中的所有机器通过一个 Leader 选举过程来选定一台称为 “Leader” 的机器，Leader 既可以为客户端提供写服务又能提供读服务。除了 Leader 外，Follower 和 Observer 都只能提供读服务。Follower 和 Observer 唯一的区别在于 Observer 机器不参与 Leader 的选举过程，也不参与写操作的“过半写成功”策略，因此 Observer 机器可以在不影响写性能的情况下提升集群的读性能。4. SessionSession 指的是 ZooKeeper 服务器与客户端会话。在 ZooKeeper 中，一个客户端连接是指客户端和服务器之间的一个 TCP 长连接。客户端启动的时候，首先会与服务器建立一个 TCP 连接，从第一次连接建立开始，客户端会话的生命周期也开始了。通过这个连接，客户端能够通过心跳检测与服务器保持有效的会话，也能够向 Zookeeper 服务器发送请求并接受响应，同时还能够通过该连接接收来自服务器的 Watch 事件通知。 Session的sessionTimeout 值用来设置一个客户端会话的超时时间。当由于服务器压力太大、网络故障或是客户端主动断开连接等各种原因导致客户端连接断开时，只要在 sessionTimeout 规定的时间内能够重新连接上集群中任意一台服务器，那么之前创建的会话仍然有效。在为客户端创建会话之前，服务端首先会为每个客户端都分配一个 sessionID。由于 sessionID 是 Zookeeper 会话的一个重要标识，许多与会话相关的运行机制都是基于这个 sessionID 的，因此，无论是哪台服务器为客户端分配的 sessionID，都务必保证全局唯一。5. Znode在谈到分布式的时候，一般节点指的是组成集群的每一台机器。而 ZooKeeper 中的数据节点是指数据模型中的数据单元，称为 ZNode。ZooKeeper 将所有数据存储在内存中，数据模型是一棵树（ZNode Tree），由斜杠（/）进行分割的路径，就是一个 ZNode，如 /hbase/master,其中 hbase 和 master 都是 ZNode。每个 ZNode上都会保存自己的数据内容，同时会保存一系列属性信息。在 ZooKeeper 中，ZNode 可以分为持久节点和临时节点两类。持久节点所谓持久节点是指一旦这个 ZNode 被创建了，除非主动进行 ZNode 的移除操作，否则这个 ZNode 将一直保存在 ZooKeeper 上。临时节点临时节点的生命周期跟客户端会话绑定，一旦客户端会话失效，那么这个客户端创建的所有临时节点都会被移除。另外，ZooKeeper 还允许用户为每个节点添加一个特殊的属性：SEQUENTIAL。一旦节点被标记上这个属性，那么在这个节点被创建的时候，ZooKeeper 就会自动在其节点后面追加上一个整型数字，这个整型数字是一个由父节点维护的自增数字。6. 版本ZooKeeper 的每个 ZNode 上都会存储数据，对应于每个 ZNode，ZooKeeper 都会为其维护一个叫作 Stat 的数据结构，Stat 中记录了这个 ZNode 的三个数据版本，分别是 version（当前ZNode的版本）、cversion（当前 ZNode子节点的版本）和 aversion（当前 ZNode 的 ACL 版本）。7. 事务操作在 ZooKeeper 中，能改变 ZooKeeper 服务器状态的操作称为事务操作。一般包括数据节点创建与删除、数据内容更新和客户端会话创建与失效等操作。对应每一个事务请求，ZooKeeper 都会为其分配一个全局唯一的事务ID，用 ZXID 表示，通常是一个64位的数字。每一个ZXID对应一次更新操作，从这些 ZXID 中可以间接地识别出 ZooKeeper 处理这些事务操作请求的全局顺序。8. WatcherWatcher（事件监听器），是 ZooKeeper 中一个很重要的特性。ZooKeeper 允许用户在指定节点上注册一些Watcher，并且在一些特定事件触发的时候，ZooKeeper 服务端会将事件通知到感兴趣的客户端上去。该机制是ZooKeeper 实现分布式协调服务的重要特性。9. ACLZooKeeper 采用 ACL（Access Control Lists）策略来进行权限控制。ZooKeeper定义了如下5种权限。 CREATE: 创建子节点的权限。 READ: 获取节点数据和子节点列表的权限。 WRITE：更新节点数据的权限。 DELETE: 删除子节点的权限。 ADMIN: 设置节点ACL的权限。注意：CREATE 和 DELETE 都是针对子节点的权限控制。10. 参考https://zhuanlan.zhihu.com/p/44731983http://blog.jobbole.com/110388/" }, { "title": "Kafka介绍", "url": "/posts/Kafka%E4%BB%8B%E7%BB%8D/", "categories": "study", "tags": "", "date": "2018-11-07 00:00:00 +0800", "snippet": "最近一直在看 fabric， fabric 目前使用的共识算法是 kafka + zookeeper，最近查了些资料，先大概了解下这个算法。这是第一篇，主要介绍 kafka。来源及设计目标Kafka 是一种分布式的，基于发布/订阅的消息系统，原本开发自 LinkedIn，它以可水平扩展和高吞吐率等特点被广泛使用。Kafka 主要设计目标如下： 以时间负责度为 O(1) 的方式提供消息持久化能力，即使对 TB 级以上数据也能保证常数时间负责度的访问性能。 高吞吐率。即使在非常廉价的商用机器上也能做到单机支持每秒 100K 条以上消息的传输。 支持 Kafka Server 间的消息分区，及分布式消费，同时保证每个 Partition 内的消息顺序传输。 同时支持离线数据处理和实时数据处理。 支持在线水平扩展，broker 数量越多，集群吞吐率越高架构 BrokerKafka 集群包含一个或多个服务器，这种服务器被称为 broker Topic每条发布到 Kafka 集群的消息都有一个类别，这个类别被称为 Topic。（物理上不同 Topic 的消息分开存储，逻辑上一个 Topic 的消息虽然保存于一个或多个 broker 上但用户只需指定消息的 Topic 即可生产或消费数据而不必关心数据存于何处） PartitionParition 是物理上的概念，每个 Topic 包含一个或多个 Partition。producer 可以决定将消息发送到哪个 partition（依靠指定 partition 或者 key），这种设计提高了 kafka 的吞吐率。 Producer负责发布消息到 Kafka broker Consumer消息消费者，向 Kafka broker 读取消息的客户端。 Consumer Group每个 Consumer 属于一个特定的 Consumer Group（可为每个 Consumer 指定 group name，若不指定 group name 则属于默认的 group）Topic 和 Partition每个 topic 类似一个 queue，不过 topic 可以分多个 partition，每个 partition 对应一个目录，目录下是消息文件。partition 的设计，使消息被分开存储，在每个 partition 内属于顺序写磁盘，因此效率很高，提高了吞吐率。另外 kafka 集群会保留所有的消息，无论是否被消费，但是因为磁盘有限制，kafka 提供了两种策略删除旧数据，一种是基于时间，一种是基于 partition 文件大小。Consumer Groupkafka 的消费组是一个很有意思的概念，可以看下面这个图：同一个 Topic 的一条消息只能被同一个 group 内的一个 consumer 消费，但是多个 consumer group 可同时消费这条消息。这样的设计可以实现下面这些功能： 消息的广播（发送给所有 consumer）。只要每个 consumer 有一个独立的 group 就可以。 消息的单播（只有一个 consumer 会收到消息）。只要将所有 consumer 放到一个 group 内即可。 同时提供离线和实时处理。比如用 Storm 对消息进行实时在线处理，用 Hadoop 对消息离线处理，同时将消息备份到另一个数据中心，这种情况只需要将三种不同的 consumer 放到不同的 group 内即可。消息投递语义 At most once 消息可能会丢，但绝不会重复传输 At least one 消息绝不会丢，但可能会重复传输 Exactly once 每条消息肯定会被传输一次且仅传输一次分区选择策略Producer 可以通过分区选择策略来选择将消息提交到 topic 的哪个 partition 内，通过指定 partiton 参数或者 key。分区选择策略分为两种： 消息的key为 null如果 key 为 null，则先根据 topic 名获取上次计算分区时使用的一个整数并加一。然后判断 topic 的可用分区数是否大于 0，如果大于 0 则使用获取的nextValue的值和可用分区数进行取模操作。 如果 topic 的可用分区数小于等于 0，则用获取的nextValue的值和总分区数进行取模操作（其实就是随机选择了一个不可用分区）。 消息的key不为 null不为 null 的选择策略很简单，就是根据 hash 算法murmur2就算出 key 的 hash 值，然后和分区数进行取模运算。所以： 如果不手动指定分区选择策略类，则会使用默认的分区策略类。 如果不指定消息的 key，则消息发送到的分区是随着时间不停变换的。 如果指定了消息的 key，则会根据消息的 hash 值和 topic 的分区数取模来获取分区的。 如果应用有消息顺序性的需要，则可以通过指定消息的 key 和自定义分区类来将符合某种规则的消息发送到同一个分区。同一个分区消息是有序的，同一个分区只有一个消费者就可以保证消息的顺序性消费。kafka 与 zookeeperzookeeper 管理 kafka 集群配置，选举 kafka leader，以及 rebalance 等操作。在旧版里，zookeekper 还负责消费的 offset 的管理，不过这增加了 zookeeper 的负担，所以在新版里，offset 的管理被移动到了 kafka 集群的一个叫 __consumer_offsets 的 topic 内。这里单独说下，leader。每个 topic 的每个 partition 都有一个 leader，producer 发布的消息最开始只存到 leader 内，leader 再跟其他 follower 同步消息，所以这简化了 producer 的操作。而如果 leader 出了问题，zookeeper 会根据维护的 ISR (in-sync replica) 来选择一个合适的副本作为新的 leader，注意哦，这里不是传统的少数服从多数的算法，是 zookeeper leader 直接选取的（其实 zookeeper 不是单独一个节点，也是一个集群，他们之间的同步和选举是使用的 ZAB (Zookeeper Atomic Broadcast) 算法，这个以后的 zookeeper 文章中再说）。扩展 - 为何使用消息系统 解耦 在项目启动之初来预测将来项目会碰到什么需求，是极其困难的。消息系统在处理过程中间插入了一个隐含的、基于数据的接口层，两边的处理过程都要实现这一接口。这允许你独立的扩展或修改两边的处理过程，只要确保它们遵守同样的接口约束。 冗余 有些情况下，处理数据的过程会失败。除非数据被持久化，否则就造成丢失。消息队列把数据进行持久化直到它们已经被完全处理，通过这一方式规避了数据丢失风险。许多消息队列采用的“插入-获取-删除”范式中，在把一个消息从队列中删除之前，需要你的处理系统明确的指出该消息已经被处理完毕，从而确保你的数据被安全的保存直到你使用完毕。 扩展性 因为消息队列解耦了你的处理过程，所以增大消息入队和处理的频率是很容易的，只要另外增加处理过程即可。不需要改变代码、不需要调节参数。扩展就像调大电力按钮一样简单。 灵活性 &amp;amp; 峰值处理能力 在访问量剧增的情况下，应用仍然需要继续发挥作用，但是这样的突发流量并不常见；如果为以能处理这类峰值访问为标准来投入资源随时待命无疑是巨大的浪费。使用消息队列能够使关键组件顶住突发的访问压力，而不会因为突发的超负荷的请求而完全崩溃。 可恢复性 系统的一部分组件失效时，不会影响到整个系统。消息队列降低了进程间的耦合度，所以即使一个处理消息的进程挂掉，加入队列中的消息仍然可以在系统恢复后被处理。 顺序保证 在大多使用场景下，数据处理的顺序都很重要。大部分消息队列本来就是排序的，并且能保证数据会按照特定的顺序来处理。Kafka 保证一个 Partition 内的消息的有序性。 缓冲 在任何重要的系统中，都会有需要不同的处理时间的元素。例如，加载一张图片比应用过滤器花费更少的时间。消息队列通过一个缓冲层来帮助任务最高效率的执行——写入队列的处理会尽可能的快速。该缓冲有助于控制和优化数据流经过系统的速度。 异步通信 很多时候，用户不想也不需要立即处理消息。消息队列提供了异步处理机制，允许用户把一个消息放入队列，但并不立即处理它。想向队列中放入多少消息就放多少，然后在需要的时候再去处理它们。 扩展 - 消息系统的 Push 和 PullPush 和 Pull 各有优缺点，这里直接看一张图：还可以将两者结合起来：PUSH和PULL两种模式结合将信息推送与拉取两种模式结合能做到取长补短，使二者优势互补。根据推、拉结合顺序及结合方式的差异，又分以下四种不同推拉模式： 先推后拉——先由信源及时推送公共信息，再由用户有针对性地拉取个性化信息； 先拉后推——根据用户拉取的信息，信源进一步主动提供（推送）与之相关的信息； 推中有拉——在信息推送过程中，允许用户随时中断并定格在感兴趣的网页上，以拉取更有针对性的信息； 拉中有推——根据用户搜索（即拉取）过程中所用的关键字，信源主动推送相关的最新信息。参考http://www.infoq.com/cn/articles/kafka-analysis-part-1https://www.jianshu.com/p/d3e963ff8b70https://leokongwq.github.io/2017/02/27/mq-kafka-producer-partitioner.htmlhttps://blog.csdn.net/u013256816/article/details/80300225http://forlan.iteye.com/blog/2372496https://www.cnblogs.com/smartloli/p/6266453.htmlhttps://www.biaodianfu.com/push-pull.htmlhttps://www.jianshu.com/p/5b15cbb88b51" }, { "title": "PBFT和RAFT相关", "url": "/posts/PBFT%E5%92%8CRAFT%E7%9B%B8%E5%85%B3/", "categories": "study", "tags": "", "date": "2018-11-05 00:00:00 +0800", "snippet": "区块链也是一种分布式系统，很多传统分布式系统上的共识机制在区块链上也有使用，比如 paxos、raft、pbft 等，一般应用在联盟链和私链上。而公链的算法主要是 PoW、PoS、DPoS、RPCA 等。这里有张图，不是很全，有个直观的印象：这篇文章主要是想总结下 PBFT、RAFT 相关的知识。其实这几个算法之前就看过了，但是一直没有来总结，主要是觉得涉及的东西很多，自己并不算充分了解，只是如果迟迟不总结，又觉得知识很松散，不便于记忆和补漏，所以还是先写一篇，总结下目前自己对这些算法的认识，以后再不断更新。所以如果哪里有问题，欢迎指出。预计总结下面这些内容： CFT (Crash Fault Tolerance) 与 BFT (Byzantine Fault Tolerance) RAFT (Replicated And Fault Tolerant)，属于 CFT 两将军问题 BFT (Byzantine Fault Tolerance) ，属于 BFT PBFT (Practical Byzantine Fault Tolerance)，属于 BFTCFT (Crash Fault Tolerance) 与 BFT (Byzantine Fault Tolerance)先说这个，是因为可能很多人并不熟悉之间的区别（我以前也是），知道这个有助于理解算法的使用范围。根据要解决的问题是普通错误还是拜占庭将军问题，共识算法可分为 CFT 和 BFT。普通错误是指，节点不会作恶，错误一般是故障，比如掉线、死机等引起；拜占庭问题是指，节点可能因为贪图利益而故意伪造信息、拦截信息等。CFT 的一些经典算法有 Paxos、RAFT 等；而 BFT 的算法分为两类，确定性和概率性，确定性算法比如 PBFT，而概率性算法有 PoW、PoS等，确定性算法一般用在联盟链，概率性的一般用在公链上。RAFT (Replicated And Fault Tolerant)RAFT 是 CFT 的一种，即节点只会故障不会作恶。RAFT 中的每个节点有三种角色，跟随者（follower），候选人（candidate），领导者（leader），集群中的每个节点在某一时刻只能是这三种角色的其中一种，角色会随着时间和条件的变化而互相转换。RAFT 算法主要有两个过程：一个是领导者选举，另一个是日志复制。领导者选举发生在无法与旧的领导者通信的情况下，多个跟随者会将自己转换为候选人，然后进行选举，只有一个候选人可以选举成功，一旦成功，自己就成为了新的领导者；而日志复制过程分为记录日志和提交数据两个阶段。RAFT 算法支持最大的容错故障节点数是 \\((N-1)/2\\)，其中 N 为集群中的总的节点数量。这里有一个很不错的 RAFT 动画，介绍了选举、日志复制、网络分区（脑裂）等情况，可以看看。两将军问题两军问题是计算机领域的一个思想实验，用来阐述在一个不可靠的通信链路上试图通过通信以达成一致是存在缺陷的和困难的，这个问题和更有名的“拜占庭将军问题”有关。两军问题是在计算机通信领域首个被证明无解的问题，由此也可以推论出，在随机通信失败的条件下“拜占庭将军问题”也同样无解。这里的随机通信失败（不可靠通信链路），是指通信可能丢失或者无限期延迟，但一般假设消息本身不会被篡改。下面是一个简化的例子，将军甲和将军乙打算一起进攻敌军，但是甲乙间的通信线路必须经过敌军阵营，所以就可能信使被杀导致通信失败。可以看出来，由于通信的不可靠，无论哪方将军发送一条消息之后都渴望得到反馈来确认消息被接收了，同时又想回复一条消息给对方，告诉对方他的消息已经被收到了，这就导致了几乎不可能达成共识。设想如果通信可靠，一方发送完消息后，知道消息一定会被对方在有限时间内收到，就不用渴望反馈了。是不是觉得这个问题基本无解了？其实也不是，对于工程来说，虽然通信无法做到 100% 可靠，但是可以通过多次发送同一个消息，来保证这些消息有很大的概率被至少接收到一次，从而降低了不可靠的程度，而一方只要接收到至少一次消息，就按照消息执行，那么共识也是可以达成的。BFT (Byzantine Fault Tolerance)这里该介绍 BFT 了，也就是拜占庭容错。前面的两将军问题不算是 BFT，因为并没有什么作恶节点，主要是不可靠通信导致的问题。那么为什么要先说两将军问题呢？主要是两将军问题引出了一个很重要的常识：在通信不可靠的情况下，试图通过通信达成一致是不可能的或者是十分困难的，这同样适用于 BFT，所以这里讨论的 BFT 的前提是：信道可靠，也就是在有限的时间内，消息一定可以到达，而且消息本身无法被更改。BFT 算法支持最大的容错故障节点数是 \\((N-1)/3\\)，其中 N 为集群中的总的节点数量。BFT 算法复杂度是 \\(O(n^{f+1})\\)。至于容错故障节点的数量和复杂度是怎么得出的，这里先截取一段来源于网上的解释：例如，N = 3，F = 1 时。若提案人不是叛变者，提案人发送一个提案出来，收到的叛变者可以宣称收到的是相反的命令。则对于第三个人（忠诚者）会收到两个相反的消息，无法判断谁是叛变者，则系统无法达到一致。若提案人是叛变者，发送两个相反的提案分别给另外两人，另外两人都收到两个相反的消息，无法判断究竟谁是叛变者，则系统无法达到一致。更一般的，当提案人不是叛变者，提案人提出提案信息 1，则对于合作者来看，系统中会有 N - F 份确定的信息 1，和 F 份不确定的信息（可能为 0 或 1，假设叛变者会尽量干扰一致的达成），N − F &amp;gt; F，即 N &amp;gt; 2F情况下才能达成一致。当提案人是叛变者，会尽量发送相反的提案给 N - F 个合作者，从收到 1 的合作者看来，系统中会存在(N - F)/2 个信息 1，以及 (N - F)/2 个信息 0；从收到 0 的合作者看来，系统中会存在 (N - F)/2 个信息 0，以及 (N - F)/2 个信息 1；另外存在 F − 1 个不确定的信息。合作者要想达成一致，必须进一步的对所获得的消息进行判定，询问其他人某个被怀疑对象的消息值，并通过取多数来作为被怀疑者的信息值。这个过程可以进一步递归下去。Leslie Lamport 等人在论文《Reaching agreement in the presence of faults》中证明，当叛变者不超过 1/3 时，存在有效的拜占庭容错算法（最坏需要 F+1 轮交互）。反之，如果叛变者过多，超过 1/3，则无法保证一定能达到一致结果。下面是几张图，解释下上面的文字。位于上方的提案人，下方的是接收者。第一种情况，提案人是叛变者，会给不同的接收者发送不同的消息：第二种情况，接受者是叛变者，会给其他的接收者发送不同的消息：这是 3 个和 4 个节点的情况，我们知道了再有 1 个节点作恶的情况下，必须要有 3 个正常节点才能达成共识。那么如果节点数很多的情况怎么办呢？比如 9 个节点，其中 3 个作恶节点，能不能达成共识呢？是不能的，可以这么理解，将 9 个节点 3 个分为一组，那么可以得到 3 组，一组都是作恶节点，两组是正常节点，那么这种情况就和 3 个节点的情况一致了，是无法达成共识的，所以最大的容错故障节点数是 \\((N-1)/3\\)。这里有一个视频，讲了两将军问题和BFT（需要翻墙），很不错，可以看看。PBFT (Practical Byzantine Fault Tolerance)终于到 PBFT 了，PBFT 主要是将 BFT 的算法负责度从指数级降到了多项式级，即 \\(O(n^2)\\)。PBFT 算法支持最大的容错故障节点数也是 \\((N-1)/3\\)，这个跟 BFT 一致。PBFT 算法的理解，其实也可以参考上面的图，将提案人换成主节点即可。这里主要说下算法的基本流程，主要有四步： 客户端发送请求给主节点  主节点广播请求给其它节点，节点执行 pbft 算法的三阶段共识流程。 节点处理完三阶段流程后，返回消息给客户端。 客户端收到来自 f+1 个节点的相同消息后，代表共识已经正确完成。其中三阶段的流程如下图：下面是这几个流程的解释，来自 知乎美图，写得挺好的，我这里就粘过来了，不自己写了。算法的核心三个阶段分别是 pre-prepare 阶段（预准备阶段），prepare 阶段（准备阶段）， commit 阶段（提交阶段）。图中的C代表客户端，0，1，2，3 代表节点的编号，打叉的3代表可能是故障节点或者是问题节点，这里表现的行为就是对其它节点的请求无响应。0 是主节点。整个过程大致是如下：首先，客户端向主节点发起请求，主节点 0 收到客户端请求，会向其它节点发送 pre-prepare 消息，其它节点就收到了pre-prepare 消息，就开始了这个核心三阶段共识过程了。 Pre-prepare 阶段：节点收到 pre-prepare 消息后，会有两种选择，一种是接受，一种是不接受。什么时候才不接受主节点发来的 pre-prepare 消息呢？一种典型的情况就是如果一个节点接受到了一条 pre-pre 消息，消息里的 v 和 n 在之前收到里的消息是曾经出现过的，但是 d 和 m 却和之前的消息不一致，或者请求编号不在高低水位之间，这时候就会拒绝请求。拒绝的逻辑就是主节点不会发送两条具有相同的 v 和 n ，但 d 和 m 却不同的消息。 Prepare 阶段：节点同意请求后会向其它节点发送 prepare 消息。这里要注意一点，同一时刻不是只有一个节点在进行这个过程，可能有 n 个节点也在进行这个过程。因此节点是有可能收到其它节点发送的 prepare 消息的。在一定时间范围内，如果收到超过 2f 个不同节点的 prepare 消息，就代表 prepare 阶段已经完成。 Commit 阶段：于是进入 commit 阶段。向其它节点广播 commit 消息，同理，这个过程可能是有 n 个节点也在进行的。因此可能会收到其它节点发过来的 commit 消息，当收到 2f+1 个 commit 消息后（包括自己），代表大多数节点已经进入 commit 阶段，这一阶段已经达成共识，于是节点就会执行请求，写入数据。处理完毕后，节点会返回消息给客户端，这就是 pbft 算法的全部流程。注解：V：当前视图的编号。视图的编号是什么意思呢？比如当前主节点为 A，视图编号为 1，如果主节点换成 B，那么视图编号就为 2，这个概念和 raft 的 term 任期是很类似的。N：当前请求的编号。主节点收到客户端的每个请求都以一个编号来标记。M：消息的内容d或D（m）：消息内容的摘要i： 节点的编号对于 pre-prepare 阶段，主节点广播 pre-prepare 消息给其它节点即可，因此通信次数为 n-1 ；对于 prepare 阶段，每个节点如果同意请求后，都需要向其它节点再 广播 parepare 消息，所以总的通信次数为 n*（n-1），即 n^2-n ；对于 commit 阶段，每个节点如果达到 prepared 状态后，都需要向其它节点广播 commit 消息，所以总的通信次数也为 n*（n-1） ，即 n^2-n 。所以总通信次数为 （n-1）+（n^2-n）+（n^2-n） ，即 2n^2-n-1 ，因此pbft算法复杂度为 O（n^2） 。OK，到这里，想说的就差不多了，至于 PBFT 的高低水位和视图更改等，大家如果感兴趣，可以看看 知乎美图 的这篇文章，介绍挺详细。这篇文章主要是总结下自己对 CFT、BFT 的一些算法的理解，希望可以帮到大家，就到这里吧。参考https://www.youtube.com/watch?v=_e4wNoTV3Gwhttp://thesecretlivesofdata.com/raft/https://yeasy.gitbooks.io/blockchain_guide/content/distribute_system/bft.htmlhttps://yeasy.gitbooks.io/blockchain_guide/content/distribute_system/paxos.htmlhttps://zhuanlan.zhihu.com/p/35847127http://blog.liqilei.com/bai-zhan-ting-gong-shi-suan-fa-zhi-pbftjie-xi/https://www.jianshu.com/p/78e2b3d3af62https://blog.csdn.net/kojhliang/article/details/71515199https://hk.saowen.com/a/9341619d25e086ac42772e8b429078bbe8227615792b3508cb54a431d430df1c" }, { "title": "Fabric_tutorials_chaincode", "url": "/posts/Fabric_tutorials_chaincode/", "categories": "study", "tags": "", "date": "2018-10-29 00:00:00 +0800", "snippet": "这篇学习下链码，原文在这里，写的很好，这里就不做详细描述了，但会做一些扩展，并会进行下总结。基础概念Fabric 的智能合约称为链码（chaincode），分为系统链码和用户链码： 系统链码用来实现系统层面的功能，包括系统的配置，用户链码的部署、升级，用户交易的签名和验证策略等。 用户链码用于实现用户的应用功能。开发者编写链码应用程序并将其部署到网络上。终端用户通过与网络节点交互的客户端应用程序调用链码。链码被编译成一个独立的应用程序，运行于隔离的Docker容器中，在链码部署的时候会自动生成合约的Docker镜像。看下之前的 byfn 的教程中创建的所有的 container，标记的这几个就是独立的链码容器:相较于以太坊，Fabric 链码和底层账本是分开的，升级链码时并不需要迁移账本数据到新链码当中，真正实现了逻辑与数据的分离。链码可以调用另一个链码，不一定非要在同一个 channel 内。当调用另一个 channel 内的链码的时候，只能使用 query 接口。有一种链码 API 还能够提供额外的控制，每个资产的 key/value 中可以包含客户的身份识别，这样只允许这个客户才能够更新这个资产。chaincode lifecycle API package package 包含三个部分： chaincode，用 CDS (Signed ChaincodeDeploymentSpec) 定义，包含代码、名称、版本等。 可选的实例化规则，语法与背书规则类似 拥有 chaincode 的组织的签名 用来确定 chaincode 的所有权 用来验证 package 的内容 用来检测 package 贿赂 这个命令主要是用来生成 SignedCDS，用于多签名的情况。 如果只是单签名，直接使用 install 就好。 下面这条命令是，生成 chaincode package，并签名 # 生成并签第一个名peer chaincode package -n mycc -p github.com/hyperledger/fabric/examples/chaincode/go/example02/cmd -v 0 -s -S -i &quot;AND(&#39;OrgA.admin&#39;)&quot; ccpack.out# 其他人签名peer chaincode signpackage ccpack.out signedccpack.out install 这条命令发送 SignedProposal 给 LSCC (lifecycle system chaincode，在 System Chaincode)，会创建 SignedCDS，并在节点上安装。 peer chaincode install -n asset_mgmt -v 1.0 -p sacc instantiate instantiate 会调用 LSCC 在 channel 上创建和初始化 chaincode，chaincode 可能会在多个 channel 上安装和初始化，但是相互之间是独立的。 channel MSP 的管理者可以调用 instantiate 来实例化一个 chaincode，当这个交易请求到达背书节点的时候，需要验证创建者的签名，在节点提交到账本的时候，还需要再验证一遍。 在 instantiate 的时候，可以提供初始值，也需要提供背书规则： peer chaincode instantiate -n sacc -v 1.0 -c &#39;{&quot;Args&quot;:[&quot;john&quot;,&quot;0&quot;]}&#39; -P &quot;AND (&#39;Org1.member&#39;,&#39;Org2.member&#39;)&quot; upgrade upgrade 可以升级 chaincode 代码和版本，但是名字必须保持不变，另外实例化规则使用的是现有的规则，这个保证了升级的合法性。 另一点是，upgrade 是针对单个节点的，这个节点更新了 chaincode，但是其他的节点可能还是使用的旧版本，这个需要注意。 最后一点是，upgrade 会调用合约的 Init 函数，在这里可以进行升级和重初始化数据的工作，因为会操作数据，所以也需要注意不要弄丢数据。 peer chaincode 命令用法Usage: peer chaincode [command]Available Commands: install Package the specified chaincode into a deployment spec and save it on the peer&#39;s path. instantiate Deploy the specified chaincode to the network. invoke Invoke the specified chaincode. list Get the instantiated chaincodes on a channel or installed chaincodes on a peer. package Package the specified chaincode into a deployment spec. query Query using the specified chaincode. signpackage Sign the specified chaincode package upgrade Upgrade chaincode.Flags: --cafile string Path to file containing PEM-encoded trusted certificate(s) for the ordering endpoint -h, --help help for chaincode -o, --orderer string Ordering service endpoint --tls Use TLS when communicating with the orderer endpoint --transient string Transient map of arguments in JSON encodingGlobal Flags: --logging-level string Default logging level and overrides, see core.yaml for full syntax --test.coverprofile string Done (default &quot;coverage.cov&quot;) -v, --versionUse &quot;peer chaincode [command] --help&quot; for more information about a command.几个 chaincode API 和 函数API： Init，在链码 instantiate 或 upgrade 的时候会调用，做一些必要的初始化工作，必须实现。 Invoke，链码调用的主要入口函数，在这里根据提供的函数名，调用不同的自定义的函数，必须实现。函数： args := stub.GetStringArgs() # 初始化时，从交易中获取参数 err := stub.PutState(args[0], []byte(args[1])) # 写入状态 fn, args := stub.GetFunctionAndParameters() # 从交易中获取函数名和参数 value, err := stub.GetState(args[0]) # 读取状态 System chaincode文章简单介绍了下 system chaincode： LSCC Lifecycle system chaincode handles lifecycle requests described above. 这个与 chaincode 生命周期相关的，上面介绍那些都是这个范围。 CSCC Configuration system chaincode handles channel configuration on the peer side. 这个与 channel 的配置有关，我们知道虽然 channel 配置是各个 channel 自己定义的，但是系统会管理所有的配置。 QSCC Query system chaincode provides ledger query APIs such as getting blocks and transactions. 这个我想与之前说过的，新加入节点需要从 order 节点获取区块有关。 System Chaincode Pluginssystem chaincode 是以插件的形式存在的，在节点启动的时候注册并部署。编译 system chaincode plugins，用下面的命令，编译的结果是 .so：go build -buildmode=pluginfabric 的 core.yaml 中也需要进行相关的配置：...chaincode: systemPlugins: - enabled: true name: mysyscc path: /opt/lib/syscc.so invokableExternal: true invokableCC2CC: true...chaincode: system: mysyscc: enable其他注意点 下面这条命令在国内用，很可能长时间没有反应 go get -u github.com/hyperledger/fabric/core/chaincode/shim 可以直接 clone fabric 的源码，然后将源码放入到 go/src/github.com/hyperledger 下 cd ~/go/src/github.commkdir hyperledger &amp;amp;&amp;amp; cd hyperledgergit clone git@github.com:hyperledger/fabric.git 创建 chaincode 容器，有两种方式。一种是直接创建 chaincode 的容器，在容器内跑起来 chaincode，然后才在背书节点进行 chaincode 的 install 和 instantiate，另一种是在 byfn 中使用的，在背书节点进行 instantiate 的时候，由系统自动创建一个 chaincode 的容器，而且需要提供 order 节点的证书。 " }, { "title": "Fabric_tutorials_private_data", "url": "/posts/Fabric_tutorials_private_data/", "categories": "study", "tags": "", "date": "2018-10-25 00:00:00 +0800", "snippet": "这篇文章，学习操作下 private data，教程的示例很清晰，没什么需要多说的，所以这篇会很短。private data 用 collection 表示的，collection definition 文件描述了有权限使用、保存和传播私有数据的节点，以及私有数据应该被保存多久。collection definition 只有 5 个参数： name，名字 policy，定义了哪些节点有权限保存私有数据 requiredPeerCount，背书节点传播私有数据到其他节点，这个参数定义的是传播的最小数量 maxPeerCount，这个参数定义的是传播的最大数量 blockToLive，定义了私有数据应该被保存的时间，以区块数量为单位，超期之后会被删除，如果想永久保存，那么就设为 0collection definition 被保存为 json 文件，在合约实例化的时候，通过 –collections-config 参数传入，官网的例子如下：// collections_config.json[ { &quot;name&quot;: &quot;collectionMarbles&quot;, &quot;policy&quot;: &quot;OR(&#39;Org1MSP.member&#39;, &#39;Org2MSP.member&#39;)&quot;, &quot;requiredPeerCount&quot;: 0, &quot;maxPeerCount&quot;: 3, &quot;blockToLive&quot;:1000000 }, { &quot;name&quot;: &quot;collectionMarblePrivateDetails&quot;, &quot;policy&quot;: &quot;OR(&#39;Org1MSP.member&#39;)&quot;, &quot;requiredPeerCount&quot;: 0, &quot;maxPeerCount&quot;: 3, &quot;blockToLive&quot;:3 }]需要注意的是，在合约内，也需要配套的私有数据结构体定义：// Peers in Org1 and Org2 will have this private data in a side databasetype marble struct { ObjectType string `json:&quot;docType&quot;` Name string `json:&quot;name&quot;` Color string `json:&quot;color&quot;` Size int `json:&quot;size&quot;` Owner string `json:&quot;owner&quot;`}// Only peers in Org1 will have this private data in a side databasetype marblePrivateDetails struct { ObjectType string `json:&quot;docType&quot;` Name string `json:&quot;name&quot;` Price int `json:&quot;price&quot;`}在这个例子里，org1 和 org2 可以操作名字为 collectionMarbles 的数据，org1 可以操作 collectionMarblePrivateDetails 的数据。合约实例化的命令如下，可以看到使用了上面的 json 文件：peer chaincode instantiate -o orderer.example.com:7050 --tls --cafile $ORDERER_CA -C mychannel -n marblesp -v 1.0 -c &#39;{&quot;Args&quot;:[&quot;init&quot;]}&#39; -P &quot;OR(&#39;Org1MSP.member&#39;,&#39;Org2MSP.member&#39;)&quot; --collections-config $GOPATH/src/github.com/chaincode/marbles02_private/collections_config.json" }, { "title": "Fabric_tutorials_add_org", "url": "/posts/Fabric_tutorials_add_org/", "categories": "study", "tags": "", "date": "2018-10-25 00:00:00 +0800", "snippet": "这篇学习下如何在 fabric 内添加一个组织和对应的节点，官方文档在 这里，总结下整个流程和注意事项。自动操作与之前一样，官方提供了自动操作的脚本./eyfn.sh up手动操作 创建 org3 的证书和 json 配置文件 # 创建证书cd org3-artifacts../../bin/cryptogen generate --config=./org3-crypto.yaml# 生成配置文件，org3.jsonexport FABRIC_CFG_PATH=$PWD &amp;amp;&amp;amp; ../../bin/configtxgen -printOrg Org3MSP &amp;gt; ../channel-artifacts/org3.json# 将网络的 order 证书复制到 org3 目录，方便后续的 org3 节点操作cd ../ &amp;amp;&amp;amp; cp -r crypto-config/ordererOrganizations org3-artifacts/crypto-config/ 进入 cli 获取生成新的 channel config # 进入 cli 并准备下环境变量docker exec -it cli bashexport ORDERER_CA=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/ordererOrganizations/example.com/orderers/orderer.example.com/msp/tlscacerts/tlsca.example.com-cert.pem &amp;amp;&amp;amp; export CHANNEL_NAME=mychannel# 获取当前的 channel configpeer channel fetch config config_block.pb -o orderer.example.com:7050 -c $CHANNEL_NAME --tls --cafile $ORDERER_CA# 从 pb 转换为 json 格式configtxlator proto_decode --input config_block.pb --type common.Block | jq .data.data[0].payload.data.config &amp;gt; config.json# 将 org3 的配置加到 json 内，这里使用到了之前制作的 org3 的配置文件 org3.jsonjq -s &#39;.[0] * {&quot;channel_group&quot;:{&quot;groups&quot;:{&quot;Application&quot;:{&quot;groups&quot;: {&quot;Org3MSP&quot;:.[1]}}}}}&#39; config.json ./channel-artifacts/org3.json &amp;gt; modified_config.json# 将原始的 json 转换为 pb configtxlator proto_encode --input config.json --type common.Config --output config.pb# 将加入 org3 之后的 json 转换为 pbconfigtxlator proto_encode --input modified_config.json --type common.Config --output modified_config.pb# 对比 pb，计算出 update 部分configtxlator compute_update --channel_id $CHANNEL_NAME --original config.pb --updated modified_config.pb --output org3_update.pb# 将 update.pb 转换为 jsonconfigtxlator proto_decode --input org3_update.pb --type common.ConfigUpdate | jq . &amp;gt; org3_update.json# 包装下 jsonecho &#39;{&quot;payload&quot;:{&quot;header&quot;:{&quot;channel_header&quot;:{&quot;channel_id&quot;:&quot;mychannel&quot;, &quot;type&quot;:2}},&quot;data&quot;:{&quot;config_update&quot;:&#39;$(cat org3_update.json)&#39;}}}&#39; | jq . &amp;gt; org3_update_in_envelope.json# 将包装后的 json 转换为 pbconfigtxlator proto_encode --input org3_update_in_envelope.json --type common.Envelope --output org3_update_in_envelope.pb 这里的 pb 和 json 的转换流程很繁琐，我觉得是因为 configtxlator 这个工具有些限制，不过只需要知道一点就好，这步是为了得到一个只包含升级部分的 pb 文件 签名和提交 config 的 update 这里需要 org1 和 org2 的签名 # peer0/org1 签名peer channel signconfigtx -f org3_update_in_envelope.pb# peer0/org2 签名和提交 updateexport CORE_PEER_LOCALMSPID=&quot;Org2MSP&quot;export CORE_PEER_TLS_ROOTCERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org2.example.com/peers/peer0.org2.example.com/tls/ca.crtexport CORE_PEER_MSPCONFIGPATH=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org2.example.com/users/Admin@org2.example.com/mspexport CORE_PEER_ADDRESS=peer0.org2.example.com:7051peer channel update -f org3_update_in_envelope.pb -c $CHANNEL_NAME -o orderer.example.com:7050 --tls --cafile $ORDERER_CA 到这步完事，config 就处理好了，org3 就可以正式加入到 channel 了 org3 加入 channel # 这里启动三个新的 container，两个是 org3 的 peer，一个是 org3 专属的 clidocker-compose -f docker-compose-org3.yaml up -d# 连接到 org3 的 cli containerdocker exec -it Org3cli bash# 配置环境变量export ORDERER_CA=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/ordererOrganizations/example.com/orderers/orderer.example.com/msp/tlscacerts/tlsca.example.com-cert.pem &amp;amp;&amp;amp; export CHANNEL_NAME=mychannel# 从 order 节点获得创世块，这里为什么不使用 gossip 的方式，最后再说peer channel fetch 0 mychannel.block -o orderer.example.com:7050 -c $CHANNEL_NAME --tls --cafile $ORDERER_CA# 加入 channelpeer channel join -b mychannel.block 升级和调用合约 # 这里还是安装之前那个合约（完全相同），只不过这次因为多了一个 org，这里版本号取为 2.0peer chaincode install -n mycc -v 2.0 -p github.com/chaincode/chaincode_example02/go/# 切换会之前的整个网络的 cli，然后在 org1 和 org2 上分别安装 version 2.0 的这个合约peer chaincode install -n mycc -v 2.0 -p github.com/chaincode/chaincode_example02/go/export CORE_PEER_LOCALMSPID=&quot;Org1MSP&quot;export CORE_PEER_TLS_ROOTCERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls/ca.crtexport CORE_PEER_MSPCONFIGPATH=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.example.com/users/Admin@org1.example.com/mspexport CORE_PEER_ADDRESS=peer0.org1.example.com:7051peer chaincode install -n mycc -v 2.0 -p github.com/chaincode/chaincode_example02/go/# 接下来是实例化合约，这里使用的不是 instantiate，而是 upgrade，同样需要提供背书策略，这里的策略是需要 org1-3 的背书peer chaincode upgrade -o orderer.example.com:7050 --tls $CORE_PEER_TLS_ENABLED --cafile $ORDERER_CA -C $CHANNEL_NAME -n mycc -v 2.0 -c &#39;{&quot;Args&quot;:[&quot;init&quot;,&quot;a&quot;,&quot;90&quot;,&quot;b&quot;,&quot;210&quot;]}&#39; -P &quot;OR (&#39;Org1MSP.peer&#39;,&#39;Org2MSP.peer&#39;,&#39;Org3MSP.peer&#39;)&quot;# 接下来就是在 org3 的 cli 里执行 query 和 invoke 等操作，证明 org3 已经被加入了peer chaincode query -C $CHANNEL_NAME -n mycc -c &#39;{&quot;Args&quot;:[&quot;query&quot;,&quot;a&quot;]}&#39;peer chaincode invoke -o orderer.example.com:7050 --tls $CORE_PEER_TLS_ENABLED --cafile $ORDERER_CA -C $CHANNEL_NAME -n mycc -c &#39;{&quot;Args&quot;:[&quot;invoke&quot;,&quot;a&quot;,&quot;b&quot;,&quot;10&quot;]}&#39;peer chaincode query -C $CHANNEL_NAME -n mycc -c &#39;{&quot;Args&quot;:[&quot;query&quot;,&quot;a&quot;]}&#39; 一些注意地方 在新组织的新节点加入到 channel 的时候，为什么不直接使用 gossip 方法获取区块，而是要通过 order 节点？ 这个是因为新的节点加入的时候，只有创世块，这个创世块里不包含新节点对应的组织加入的信息（组织加入的信息是在后面的块里），所以新节点无法验证从其他节点发送过来的信息（我觉得这里的其他节点也是后续加入的新节点，因为创世的节点，是可以被验证的），所以无法使用 gossip ，只能从 order 节点获取区块。因此需要配置为如下两种形式之一： # static leaderCORE_PEER_GOSSIP_USELEADERELECTION=falseCORE_PEER_GOSSIP_ORGLEADER=true # dynamic leaderCORE_PEER_GOSSIP_USELEADERELECTION=trueCORE_PEER_GOSSIP_ORGLEADER=false 一个节点可以安装同一个合约的不同版本 这里的同一个合约是指合约代码完全一样。 这里的不同版本是指 version 不同，实际上就是合约的实例化的初始数据可能不同，背书原则也可以不同。 " }, { "title": "Fabric_tutorials_write_app", "url": "/posts/Fabric_tutorials_write_app/", "categories": "study", "tags": "", "date": "2018-10-24 00:00:00 +0800", "snippet": "这篇简单介绍下 Writing Your First Application 中的一些注意点。官方这篇教程很简单，但是有一些需要注意的地方，希望可以帮助到大家。一些注意点 npm install 这个命令是安装 package.json 中指定的安装包，里面主要是这三个，fabric-ca-client、fabric-client 和 grpcfabric-ca-client，是 app 用来跟 fabric 内的 ca 服务通信的，在后面创建证书的时候，会用到。 fabric-client，是 app 用来跟节点通信的，后面的 query、invoke 都是通过这个。 另外在 ubuntu 1604 上安装 nodejs，可能安装是很旧的版本，在运行 npm install 的时候，就会报错，可以看下后面“npm install 的问题”里说的解决办法。 ./startFabric.sh 这个就是启动整个 fabric 网络，并安装好 chaincode enrollAdmin.js、registerUser.js、query.js、invoke.js 这些就是所谓的 app 了，enroll 和 register 会使用 fabric-ca-client 和 fabric-client 来通信，query 和 invoke 就只使用 fabric-client 通信，因为不需要和 ca 通信。 前两个是注册用户，后两个是获取信息和提交信息。 npm install 的问题 提示 /usr/bin/env: ‘node’: No such file or directory 这个是因为默认只有 nodejs 没有 node，解决办法是创建一个软链接。 sudo ln -s /usr/bin/nodejs /usr/bin/node npm install 失败 node-pre-gyp WARN Using request for node-pre-gyp https download node-pre-gyp ERR! UNCAUGHT EXCEPTION node-pre-gyp ERR! stack TypeError: this is not a typed array.node-pre-gyp ERR! stack at Function.from (native)node-pre-gyp ERR! stack at Object.&amp;lt;anonymous&amp;gt; (/home/cooli7wa/Documents/mywork/block_chain_wallet/fabric-samples/fabcar/node_modules/fabric-client/node_modules/grpc/node_modules/tar/lib/parse.js:33:27)node-pre-gyp ERR! stack at Module._compile (module.js:410:26)node-pre-gyp ERR! stack at Object.Module._extensions..js (module.js:417:10)node-pre-gyp ERR! stack at Module.load (module.js:344:32)node-pre-gyp ERR! stack at Function.Module._load (module.js:301:12)node-pre-gyp ERR! stack at Module.require (module.js:354:17)node-pre-gyp ERR! stack at require (internal/module.js:12:17)node-pre-gyp ERR! stack at Object.&amp;lt;anonymous&amp;gt; (/home/cooli7wa/Documents/mywork/block_chain_wallet/fabric-samples/fabcar/node_modules/fabric-client/node_modules/grpc/node_modules/tar/lib/list.js:10:16)node-pre-gyp ERR! stack at Module._compile (module.js:410:26)node-pre-gyp ERR! System Linux 4.4.0-137-genericnode-pre-gyp ERR! command &quot;/usr/bin/nodejs&quot; &quot;/home/cooli7wa/Documents/mywork/block_chain_wallet/fabric-samples/fabcar/node_modules/fabric-client/node_modules/grpc/node_modules/.bin/node-pre-gyp&quot; &quot;install&quot; &quot;--fallback-to-build&quot; &quot;--library=static_library&quot;node-pre-gyp ERR! cwd /home/cooli7wa/Documents/mywork/block_chain_wallet/fabric-samples/fabcar/node_modules/fabric-client/node_modules/grpcnode-pre-gyp ERR! node -v v4.2.6node-pre-gyp ERR! node-pre-gyp -v v0.10.3node-pre-gyp ERR! This is a bug in `node-pre-gyp`.node-pre-gyp ERR! Try to update node-pre-gyp and file an issue if it does not help:node-pre-gyp ERR! &amp;lt;https://github.com/mapbox/node-pre-gyp/issues&amp;gt;fabcar@1.0.0 /home/cooli7wa/Documents/mywork/block_chain_wallet/fabric-samples/fabcarnpm WARN fabcar@1.0.0 No repository field.npm ERR! Linux 4.4.0-137-genericnpm ERR! argv &quot;/usr/bin/nodejs&quot; &quot;/usr/bin/npm&quot; &quot;install&quot;npm ERR! node v4.2.6npm ERR! npm v3.5.2npm ERR! code ELIFECYCLEnpm ERR! grpc@1.14.2 install: `node-pre-gyp install --fallback-to-build --library=static_library`npm ERR! Exit status 7npm ERR! npm ERR! Failed at the grpc@1.14.2 install script &#39;node-pre-gyp install --fallback-to-build --library=static_library&#39;.npm ERR! Make sure you have the latest version of node.js and npm installed.npm ERR! If you do, this is most likely a problem with the grpc package,npm ERR! not with npm itself.npm ERR! Tell the author that this fails on your system:npm ERR! node-pre-gyp install --fallback-to-build --library=static_librarynpm ERR! You can get information on how to open an issue for this project with:npm ERR! npm bugs grpcnpm ERR! Or if that isn&#39;t available, you can get their info via:npm ERR! npm owner ls grpcnpm ERR! There is likely additional logging output above.npm ERR! Please include the following file with any support request:npm ERR! /home/cooli7wa/Documents/mywork/block_chain_wallet/fabric-samples/fabcar/npm-debug.log这个是因为 nodejs 版本过低，用下面的方法安装 8.x 版本curl -sL https://deb.nodesource.com/setup_8.x | sudo -E bash -sudo apt-get install -y nodejs" }, { "title": "Fabric_tutorials_build_network", "url": "/posts/Fabric_tutorials_build_network/", "categories": "study", "tags": "", "date": "2018-10-23 00:00:00 +0800", "snippet": "之前看过了 fabric 整体的架构和一些关键概念，对 fabric 有了整体的了解，这篇终于可以上手了 :)，这次学习下网络的构建过程。在构建之前，需要做一些准备，比如需要安装好 docker 环境，下载 fabric sample 和 docker 镜像，这些就不详述了，可以参考官方文档。这些准备好了之后，我们就可以开始构建网络。先进入 first-network 目录:cd fabric-samples/first-network这里提供了一个脚本 byfn.sh，这个脚本会执行一系列的操作，这里的所有操作在后面“手动构建网络”的时候，都会一步步用到，所以这里就是给我们一个整体的印象，执行下面的命令就可以:# 生成网络相关数据，证书、genesis block、channel config、anchor peer update# genesis block 在 ordering service 会被使用./byfn.sh generate# 启动网络，这里会比较久，等 END 出现就结束了./byfn.sh up# 关闭网络，这个比较快./byfn.sh down在看完后面的手动构建网络之后，再回头看这个流程中打印的 log，就可以发现，和手动流程是很类似的。在做手动构建网络的时候，有一点需要留意，就是下面提供的命令，在 host 机上执行的都是在 first-network 目录下执行，在 container 上执行的都是在 /opt/gopath/src/github.com/hyperledger/fabric/peer 目录下。手动构建网络 生成证书相关 # 注意这里是在 first-network 目录下执行../bin/cryptogen generate --config=./crypto-config.yaml 生成创世块 export FABRIC_CFG_PATH=$PWD &amp;amp;&amp;amp; ../bin/configtxgen -profile TwoOrgsOrdererGenesis -outputBlock ./channel-artifacts/genesis.block 创建 channel config export CHANNEL_NAME=mychannel &amp;amp;&amp;amp; ../bin/configtxgen -profile TwoOrgsChannel -outputCreateChannelTx ./channel-artifacts/channel.tx -channelID $CHANNEL_NAME 创建 anchor peer ../bin/configtxgen -profile TwoOrgsChannel -outputAnchorPeersUpdate ./channel-artifacts/Org1MSPanchors.tx -channelID $CHANNEL_NAME -asOrg Org1MSP../bin/configtxgen -profile TwoOrgsChannel -outputAnchorPeersUpdate ./channel-artifacts/Org2MSPanchors.tx -channelID $CHANNEL_NAME -asOrg Org2MSP 启动网络 docker-compose -f docker-compose-cli.yaml up -d 进入 cli 容器（默认是和 peer0/org1 连接） docker exec -it cli bash 创建和加入 channel # 这里的 --cafile 是传入 order 节点的根证书，用来校验 order 节点# 这个命令会返回一个 创世 block，用来加入 channelexport CHANNEL_NAME=mychannelpeer channel create -o orderer.example.com:7050 -c $CHANNEL_NAME -f ./channel-artifacts/channel.tx --tls --cafile /opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/ordererOrganizations/example.com/orderers/orderer.example.com/msp/tlscacerts/tlsca.example.com-cert.pem # 将 peer0/org1 加入 channelpeer channel join -b mychannel.block# 将 peer0/org2 加入 channel，因为默认是与 peer0/org1 连接，这里需要前置下环境变量CORE_PEER_MSPCONFIGPATH=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org2.example.com/users/Admin@org2.example.com/msp CORE_PEER_ADDRESS=peer0.org2.example.com:7051 CORE_PEER_LOCALMSPID=&quot;Org2MSP&quot; CORE_PEER_TLS_ROOTCERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org2.example.com/peers/peer0.org2.example.com/tls/ca.crtpeer channel join -b mychannel.block 更新 anchor peer # update peer0/org1peer channel update -o orderer.example.com:7050 -c $CHANNEL_NAME -f ./channel-artifacts/Org1MSPanchors.tx --tls --cafile /opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/ordererOrganizations/example.com/orderers/orderer.example.com/msp/tlscacerts/tlsca.example.com-cert.pem# update peer0/org2CORE_PEER_MSPCONFIGPATH=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org2.example.com/users/Admin@org2.example.com/msp CORE_PEER_ADDRESS=peer0.org2.example.com:7051 CORE_PEER_LOCALMSPID=&quot;Org2MSP&quot; CORE_PEER_TLS_ROOTCERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org2.example.com/peers/peer0.org2.example.com/tls/ca.crt peer channel update -o orderer.example.com:7050 -c $CHANNEL_NAME -f ./channel-artifacts/Org2MSPanchors.tx --tls --cafile /opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/ordererOrganizations/example.com/orderers/orderer.example.com/msp/tlscacerts/tlsca.example.com-cert.pem 安装和实例化 chaincode # 安装，这条是在 peer0/org1 安装了 chaincode，-n 是名字，-v 是版本peer chaincode install -n mycc -v 1.0 -p github.com/chaincode/chaincode_example02/go/# 在 peer0/org2 也需要安装，需要前置下环境变量CORE_PEER_MSPCONFIGPATH=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org2.example.com/users/Admin@org2.example.com/msp CORE_PEER_ADDRESS=peer0.org2.example.com:7051 CORE_PEER_LOCALMSPID=&quot;Org2MSP&quot; CORE_PEER_TLS_ROOTCERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org2.example.com/peers/peer0.org2.example.com/tls/ca.crt peer chaincode install -n mycc -v 1.0 -p github.com/chaincode/chaincode_example02/go/# 实例化，-P 是代表需要 org1 和 org2 一起背书peer chaincode instantiate -o orderer.example.com:7050 --tls --cafile /opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/ordererOrganizations/example.com/orderers/orderer.example.com/msp/tlscacerts/tlsca.example.com-cert.pem -C $CHANNEL_NAME -n mycc -v 1.0 -c &#39;{&quot;Args&quot;:[&quot;init&quot;,&quot;a&quot;, &quot;100&quot;, &quot;b&quot;,&quot;200&quot;]}&#39; -P &quot;AND (&#39;Org1MSP.peer&#39;,&#39;Org2MSP.peer&#39;)&quot; query 和 invoke # query，获取 a 的值，返回 100peer chaincode query -C $CHANNEL_NAME -n mycc -c &#39;{&quot;Args&quot;:[&quot;query&quot;,&quot;a&quot;]}&#39;# invoke，a 给 b 10，这里需要提供 order org1 org2 的证书peer chaincode invoke -o orderer.example.com:7050 --tls true --cafile /opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/ordererOrganizations/example.com/orderers/orderer.example.com/msp/tlscacerts/tlsca.example.com-cert.pem -C $CHANNEL_NAME -n mycc --peerAddresses peer0.org1.example.com:7051 --tlsRootCertFiles /opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/tls/ca.crt --peerAddresses peer0.org2.example.com:7051 --tlsRootCertFiles /opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org2.example.com/peers/peer0.org2.example.com/tls/ca.crt -c &#39;{&quot;Args&quot;:[&quot;invoke&quot;,&quot;a&quot;,&quot;b&quot;,&quot;10&quot;]}&#39; 这里 a b 的值可以是负值 " }, { "title": "Fabric_KeyConcepts_PrivateData", "url": "/posts/Fabric_KeyConcepts_PrivateData/", "categories": "study", "tags": "", "date": "2018-10-18 00:00:00 +0800", "snippet": "从之前学习的内容知道，fabric 不止提供了 channel 间的隐私保护，还提供了 channel 内的隐私保护，叫做 private data collection，可以在 channel 内创建私密通道，只有私密通道的成员才能修改和查看具体的私密数据，可以说是进一步扩展了 fabric 的使用场景。What is a private data collection?包含下面两个方面： 实际的私密数据。通过组织节点间的 gossip 协议来传输，保存在节点的私有数据库内（SideDB），ordering service 看不到私密的数据。如果想使用 gossip，那么组织之间必须建立 anchor peer（锚节点）。 私密数据的哈希。哈希数据会被背书、排序并写入到每个（注意是每个）节点的账本里，用来证明数据的有效性，便于以后的查账。看下面这张图：注意其中的 private state 和 hash。Transaction flow with private data看下涉及到私密数据的交易流程，与常规的有些不同： 应用创建交易的提议，有关的私密数据或者生成私密数据的数据放到提议的 transient 域内，并将提议传给背书节点。 背书节点模拟执行智能合约，然后将产生的私密数据存放到节点本地的 transient data store 内（临时的），然后根据 collection 规则，将私密数据通过 gossip 传递给其他被授权的节点。 背书节点将提议的结果中的公开数据和私密数据的哈希发回给应用，这里面没有私密数据。 像往常一样，应用将交易提交给 ordering service，然后 ordering service 将区块发给所有节点。 所有节点接收到区块之后，先验证下自己时候有权利获得私密数据，如果有的话，需要先检查下自己的 transient data store 中是否已经接收到背书阶段的私密数据，如果没有的话就尝试从其他节点获取数据，然后验证下数据的哈希值是否和区块中的一致，然后才将私密数据真正写入到自己的数据库内，并删除 transient data store 中的数据。Purging data在有些场景可能想清理掉节点的私密数据，fabric 将来也会提供这种功能。" }, { "title": "Fabric_KeyConcepts_Peers", "url": "/posts/Fabric_KeyConcepts_Peers/", "categories": "study", "tags": "", "date": "2018-10-18 00:00:00 +0800", "snippet": "前面学习了 MSPs，这篇来详细看下 Peers 相关的知识。原文 在这里，这篇文章的前半部分详细介绍了一些 Peer 、Ledger、Application、Chaincode、channel 之间的关系，这里前面的学习中都有所了解，在这里就不详细介绍了，这里主要看下 query 和 update 的详细流程。这张图介绍了 query 和 update 的梗概，1-3 是 query，4-5 是 update：在文章的后面，将这个过程又做了重新的划分，分为了 3 个阶段，我们一个个来看下：Phase 1: Proposal从 application（应用） 发送 propose（提议）到 endorsing peer（背书节点），然后背书节点返回背书结果给应用（这个阶段中用不到 orderers（排序节点））。应用产生提议之后，应该发送给哪些背书节点？这个是由背书规则（智能合约中定义）决定的。背书节点做些什么？每个背书节点单独使用提议的内容执行智能合约，得到 response（图中 R*），然后对 response 进行签名得到背书（图中 E*），将反馈和背书打包后返还给应用。应用在收到足够的背书之后，第一阶段就成功结束了。那么如果应用收到的反馈结果不同怎么办？首先说下什么情况会导致反馈结果不同，一种是背书节点的账本状态不对，比如比较老，那么通过智能合约计算出来的结果就可能跟别人的不一样，第二种是因为智能合约的编程语言有很多种，可能得到不确定性的结果。那么第一种情况，应用可以通过简单的发送 up-to-date 提议来让节点更新（第二种文章没介绍怎么处理），如果结果还是不同，那么节点可以选择终止后续的交易，实际上如果不终止，后续的流程中也会被拒绝。Phase 2: Packaging在这个阶段中，orderer 是核心。orderer 从很多的应用处接收交易，然后将这些交易排序打包到区块内，然后将区块发布给所有链接到 orderer 的节点上，包括之前背书的节点。需要注意的一点是，排序的顺序与接收到的顺序很可能是不一样的。另外，交易一旦被写入区块，它的位置就确定了，分叉和重写永远不会发生。这里没有具体讲排序的规则是什么样的，但是这里一定有一个规则，而且不是根据接收时间计算的。Phase 3: Validation这个阶段，orderer 节点发布区块，然后每个节点验证区块中的每笔交易，并写入账本。发布前面已经说了，这里需要注意的是，不是每个节点都需要连接到 orderer 来获取区块，也可以通过 gossip 等方式来获取区块。那么需要验证哪些东西呢？首先，交易的背书是否符合背书规则（在智能合约中定义）的要求，并且背书的反馈是否是相同的，另外还需要检查本地账本的状态是否和交易背书的时候的状态一致，因为这段时间可能发生了一些交易，改变了交易的初始状态。如果验证都通过，那么这笔交易就会被写到账本里，如果失败，那么交易不会被写入账本，但是会被保留以便查账，所以在节点内，区块的状态与 orderer 发过来的基本是一致的（除了有些有效和无效的标记），但是不是区块内的所有交易都被写到了账本里，只有验证过有效的才会被写到账本里。另外需要注意的一点是，智能合约在这个阶段中是不需要被执行的，合约只需要在第一阶段执行。最后，节点会根据验证结果发送一些事件，比如 block events、block transaction events、chaincode events，应用可以注册监听这些事件，比如交易验证失败的事件，可以做些后续的处理。一些感想看到这，联想到之前 fabric 介绍中所说的共识算法，fabric 现在针对的场景还是 CFT，也就是节点只有故障没有作恶，从上面的共识过程看，很简单，没有类似 PBFT 那种多阶段共识的流程，fabric 的计划是后续加入 BFT 的支持，现在还没有。另外 fabric 这种分阶段分节点处理不同事物的方式，使结构和流程很清晰，并且可以应对并发的情况，挺不错的。" }, { "title": "Fabric_KeyConcepts_Ledger", "url": "/posts/Fabric_KeyConcepts_Ledger/", "categories": "study", "tags": "", "date": "2018-10-18 00:00:00 +0800", "snippet": "这篇看下 Key concepts 的最后一篇文章，账本。A Blockchain Ledger在账本里包含两个部分，世界状态和区块链。世界状态记录的是键值对，键值对的值来源于区块链，区块链记录了所有的交易日志，像上节说的，这些交易包含有效和无效的，有效的交易会影响到世界状态，而无效的只是一份记录而已，为了以后的查账，区块链本身不能更改。World State每条叫做一个 state，每个 state 都有一个 version，每次 state 改变的时候，version 都会增加，所以这个也被用在节点接收区块之前的校验里，需要保证当前的 version 和交易创建的时候的相符合。世界状态可以在任何时候通过区块链重新生成。BlockchainH* 中包含本区块的所有交易的哈希和前一个区块的哈希。genesis block（创世区块）不包含任何的交易，它包含 channel 的配置的初始状态。Blocks Block Header，区块头 Block number，每个区块增加 1 Current Block Hash，当前区块的哈希 Previous Block Hash，前一个区块的哈希 Block Data，区块数据，包含所有的交易 Block Metadata，区块 meta 数据，包含创建的时间，证书，公钥，签名，也包含有效/无效的标识。Transactions Header，包含一些必要的 meta 数据，比如相关联的 chaincode 的名字和 version。 Signature，应用的签名 Proposal，应用提供的输入参数 Response，包含交易前和之后的世界状态，这个是可读写的，如果交易是有效的，那么这里的数值，最终会被记录到世界状态。 Endorsements，这里是一系列签过名的交易的 response，就是所有的背书。World State database options数据库可以使用 LevelDB 和 CouchDB，LevelDB 适合于键值对形式的数据，而 CouchDB 适合于JSON 格式的数据。从这里可以看出来 fabric 的一个重要的特性，可插拔性。" }, { "title": "Fabric_KeyConcepts_MSP", "url": "/posts/Fabric_KeyConcepts_MSP/", "categories": "study", "tags": "", "date": "2018-10-17 00:00:00 +0800", "snippet": "之前看过了 fabric 的网络架构，这篇来学习下 fabric 中很重要的 MSP (membership service provider) 相关的知识。Identity身份（Identity）决定了对区块链中资源获取和使用的权利。principal 包含了更广泛的身份信息，比如用户的组织、部门、角色等。MSP (membership service provider) 用来对身份提供可信任的鉴权，默认实现是使用 X.509 证书，采用 PKI (Public Key Infrastructure) 分层模型。Fabric 提供了一个私有的 CA 来处理证书，也可以使用公开的或者商业的 CA。MSP 机构通过 MSP 来管理他们的成员。每个机构的 MSP 不一定只有一个，对于不同的业务可能有不同的 MSP。比如下面这张图： Organizational units (OUs) 组织部门，是指每个组织内可以有不同的业务线，证书内的 OU 域，就是指这个证书可以应用于哪个业务线，使得权限控制可以细分到组织内的不同部门。 MSPs 按照作用域分为 Local MSP 和 Channel MSPs，Local MSPs 是为了clients (users) 和 nodes (peers 和 orderers)，每个 node 和 user 都必须有 local MSP，用来确定哪些请求者是组织内的成员，channel 的配置里包含不同组织的 MSP，因为 channel 的 MSPs 是对所有节点有效的，所以所有节点都另外存有 channel MSPs 的备份，并且通过共识算法同步。 MSPs 按照等级分为 Network MSP、Channel MSP、Peer MSP、Orderer MSP，前两个是 Global MSP，或两个是 Local MSP。 Network MSP，定义了网络成员列表，和谁可以可以管理任务（比如创建 channel等）。 Channel MSP，定义了 channel 成员列表，和谁可以添加成员和实例化合约等。 Peer MSP，定义了组织的成员列表，和谁可以在 peer 上安装合约等。 Orderer MSP，也是定义了组织的成员列表，不过是在 Orderer 节点上。 MSP 结构 Root CAs，根 CA Intermediate CAs，中间 CA Organiztional Units (OUs)，组织部门 Administrators，管理者 Revoked Certificates，作废的证书 Node Identity，节点身份 KeyStore (private keys)，私钥库 TLS Root CA，TLS（传输层安全协议）根 CA TLS Intermediate Ca，TLS 中间 CA " }, { "title": "Fabric_KeyConcepts_network", "url": "/posts/Fabric_KeyConcepts_network/", "categories": "study", "tags": "", "date": "2018-10-13 00:00:00 +0800", "snippet": "这篇学习 fabric 的网络架构，官方文档 在这里，写得很全面也容易懂，这里就不详细介绍了，这篇主要是总结下一些关键点。整体网络架构先看下官方的这张图：初看这张图感觉挺迷糊的，各种符号和图标很多，官方文档从头一点点介绍了这个网络的构建过程，看过之后就觉得很清晰了，不过我这里就不这么介绍了，我主要想从这张图开始，总结下图上的各个图标符号的意义和互相之间的联系，方便大家对整个架构有一个快速的了解。主要介绍下面这些： 图标及缩写的含义 其他符号的含义 各个部分的作用和联系 交易流程和节点类型图标及缩写的含义 R*，比如 R1 ~ R4，R 代表的是 organization (机构)，也就是联盟链内的各个联盟成员。 *C*，比如 NC4、CC1、CC2，C 代表的是 configuration (配置)，NC 代表的是 network config，CC 代表的是 channel config。 P*，比如 P1 ~ P3，P 代表的是 peer (节点)。 S*，比如 S5 、S6，S 代表的是 smart contract (智能合约)。 L*，比如 L1、L2，L 代表 ledger (账本)。 O*，比如 O1，O 代表 ordering service (排序服务)。 A*，比如 A1 ~ A3，A 代表 application (应用)。 CA*，比如 CA1 ~ CA4，CA 代表 Certificate Authority (证书授权中心)。 C*，比如 C1、C2，C 代表 channel (频道)。 N，代表 network (网络)。其他符号的含义C1 和 C2 的椭圆形代表 channel，与 channel 有线连接的对象，代表属于此 channel，上面的圆，标注 1 或 2，也是这个意思。各个部分的作用和联系 NC*，是整个网络的基础配置，最开始由 R4 创建，在 R1 加入之后，可以由 R1 和 R4 共同维护和更改。配置包含但是不限于访问控制、管理资源。 CC*，每个 channel 都有自己的配置 (CC)，这个是与 NC 独立的，channel 内的规则都是由 CC 唯一确定的，由 channel 内的成员来维护和更改。一旦 channel 创建了，NC 就再也无法影响 channel。 ordering service，排序服务节点，虽然图中是一个，但是实际上可以有多个。主要是收集应用的背书过的 transaction，通过共识排序生成 block，然后发送给 channel 内的节点来记录。除了这个还负责批准节点加入 channel 的请求，等等这个不是跟独立有所矛盾么？其实虽然请求是发给 ordering service，但是是根据 channel 自己的 CC 来决定是否可以加入的，所以实际上还是 channel 本身在控制。 ledger，每个 channel 有自己的账本，channel 内的 peer，都需要保存一份副本，保持同步。操作账本的规则都是 CC 规定的，所以账本物理上被节点拥有，逻辑上是 channel 拥有。 smart contract，智能合约由组织的应用开发者来创建，用来产生交易，账本的所有操作都是通过智能合约。智能合约的创建本身有两个主要的操作，安装和实例化。安装是指将智能合约部署到 channel 内的某个节点上（不一定所有节点都需要部署），被部署的节点知道合约的全部内容，而其他节点或应用，只知道合约提供的接口，通过接口可以进行账本等操作。实例化是指将某个节点已经部署过合约这件事通知 channel 内的所有节点，通知过之后，智能合约才算真正有效。另外拥有智能合约的节点可以进行背书操作，通过根据应用传送的数据和自身的智能合约，经过执行和签名产生背书后的交易。 所有的节点都可以选择接受或者拒绝交易，但是只有拥有智能合约的节点才能够进行背书。 多个 channel 之间是隔离的，但是 channel 包含的机构可以重叠，某些机构因为业务需要可以加入到不同的 channel 里面，那么他们的节点，就可以同时拥有不同 channel 的账本和智能合约。 policy 或者说 config，只要涉及到的机构达成共识就可以更改。节点类型和交易流程前面介绍了各个部分的名称和作用，现在看下每个 channel 内的节点的类型： committing peer，每个节点都是一个提交节点，记录区块到账本。 endorsing peer，拥有智能合约的节点是背书节点，可以对应用提交的交易进行验证并背书。 leader peer，每个 channel 可以拥有一个主节点，主节点接收 ordering service 发送过来的区块，并转发给其他节点。在静态模式，主节点是由配置固定的，可以有零或者多个，在动态模式，主节点是选举出的。 anchor peer，锚定节点，主要是为了不同机构之间的通信，每个机构可以有零或多个锚定节点，应用于跨机构通信的场景。再看下交易流程： 应用根据需要构造交易提案，并发送给 channel 内的背书节点。 背书节点模拟执行交易，然后将原始交易和执行结果打包并签名，发回给应用。 应用收到背书节点的回应后，打包并签名发送给 ordering service。 ordering service 对收到的交易进行共识排序，然后按照策略来生成区块，并发送给对应 channel 内的 peer 节点。 peer 节点对区块内的交易进行验证，检查交易的输入输出是否正确，然后将区块写入账本。" }, { "title": "Fabric_KeyConcepts_func_model", "url": "/posts/Fabric_KeyConcepts_func_model/", "categories": "study", "tags": "", "date": "2018-10-12 00:00:00 +0800", "snippet": "之前看了 fabric 的介绍，这篇就开始进入正题，深入学习下各种概念和特性。这里内容很多，介绍了 fabric 的方方面面，所以会陆续更新一系列文章。这篇介绍 Functionalities 和 Model，内容不多。Hyperledger Fabric Functionalities Identity management，许可网络有进入控制，这可以做额外的鉴权操作，比如有的 id 可以发布新的 chaincode，而有的 id 只能调用 chaincode Privacy and confidentiality，通过 channel 的方式来达到隐私和保密的效果，对于没有权限进入 channel 的节点，无法查看 channel 内信息。 Efficient processing， fabric 将交易执行的整个过程分离为 ordering 和 commitment，并分配给不同类型的节点来做，这样的分离使得并行计算成为可能。 Chaincode functionality，这里谈到两点，第一点是一般的 chaincode 提供的资产转移方式就有相同的规则和需求，公开透明。第二点是 system chaincode 定义了各个 channel 的参数、规则，也定义了背书和验证的一些需求和规则。 Modular design，模块化设计是 fabric 的一大特点，模块的优势一般就是容易维护和移植，这在 fabric 上体现为，各种机构和组织开发和使用的模块能够互相配合。Hyperledger Fabric Model这里主要是介绍了 fabric 的一些关键特性，主要包含下面这些方面： Assets，资产范围很广，从虚拟到实物都可以，资产的表现可以是二进制或者 JSON 格式。 Chaincode，交易的主要逻辑。 Ledger Features，每个 channel 都有自己的 ledger，ledger 主要是记录了不可变的序列的记录，而 state 是通过数据库存储的各个节点的 key-value 值。从 ordering service 发来的交易，每个节点都需要验证，验证是否符合背书规则，在写入区块之前，还需要验证状态在处理这段时间内没有被更改过。一旦写入区块链，那么交易就是无法改变的。每个 channel 的 ledger 都包含一些配置、权限等规则。 Privacy，隐私性方面，前面已经说过了，fabric 主要是通过 channel 和 private data 方式来保护，channel 在整个网络中划分了私密的通道，在通道之内的成员，还可以成立各个组织，各个组织之间是通过 private data 来保护的，这里也叫做 collection。 Security &amp;amp; Membership Services，这就是成员的许可机制，已经说过了。 Consensus，这里文章说的主要意思是，现在共识机制就像是算法的同义词，只负责交易的提交，但是在 fabric 里共识体现在方方面面，在提议、背书、排序、验证、提交的整个流程中都在使用。" }, { "title": "Fabric_introduction", "url": "/posts/Fabric_introduction/", "categories": "study", "tags": "", "date": "2018-10-11 00:00:00 +0800", "snippet": "前些天参加了在北京举行的可信区块链峰会，各大公司的落地区块链产品大部分是基于联盟链的，而其中使用最多的就是 hyperledger fabric，而且很多安全方面的公司的产品也是针对 fabric 的，从 fabric 公开的材料中得知，中国已经注册了 50 多家会员单位，算上使用但还没注册的应该更多，可见 fabric 的影响力之大。我虽然之前看了一些文章和视频介绍 fabric，但是理解不深，所以最近开始深入学习下 fabric 的相关知识，就从 官方文档 开始吧。Introduction这段主要介绍了 fabric 为企业级用户设计的一些考量： 成员必须是可识别的 网络应该是许可制的 交易效率高 交易时间短 对于商业交易的隐私性保护Hyperledger Fabric介绍了 fabric 与其他流行的分布式账本和区块链平台的区别。 隶属于 linux 基金会，一个开源的平台，由超过 35 个机构和接近 200 名开发者维护。 模块化和可配置化，适用于银行、金融、保险、健康、人力资源、供应链和数字音乐发布等领域。 是第一个支持智能合约 (chaincode) 的分布式账本，且账本语言支持 Java、Go、Node.js 等流程语言，减少了学习成本。 平台是许可制的，节点间互相了解（虽然可能不完全信任），不匿名。 可插拔的共识机制。比如，CFT (crash fault-tolerant) 和 BFT (byzantine fault tolerant) 可以根据实际场景进行切换。 不需要发行代币，也就省掉了挖矿的消耗，使性能和其他分布式系统相似，且减少了签名相关的风险和攻击。 支持隐私和保密交易Modularity介绍 fabric 中使用的模块： 可插拔的 ordering service，用来排序交易、生成区块和广播给其他节点 可插拔的 membership service，用来验证用户的身份 可选的 peer-to-peer gossip service，用来节点间的提供服务 chaincode 智能合约，比如在 docker 环境内运行，提供隔离。 账本支持一系列的 DBMS (Database Managerment System) 可插拔的背书和验证策略Permissioned vs Permissionless Blockchains这段主要对比了介绍了许可制的优点。没有许可制的区块链中，任何人都可以加入，且是匿名的，为了弥补这种信任的缺失，需要采用类似挖矿和手续费的方式来提供激励。而在许可制的区块链中，区块链的相关操作是在一系列已知、可识别、审核的节点间进行，提供了一定程度的信任基础，所以可以使用类似 CFT 和 BFT 的共识算法。另外在许可制的网络中，节点恶意攻击的可能性降低，因为节点间互相了解，且作恶行为会被记录。Smart Contracts智能合约的三个关键点是： 网络中的智能合约间可以协作 可以动态发布合约 调用合约的应用代码，应该被认为是不可信的或者是恶意的前面讲到了，fabric 支持很多流行语言，不是像 eth 只支持 solidity。eth 只支持 solidity 的原因，作者讲是因为为了得到确定性的结果（支持多语言，可能遇到结果不一致问题），但是会导致学习成本上升。fabric 通过流程的调整解决了这个问题，后面会说。A New Approachexecute-order-validate，是新的交易处理流程，每步骤如下： execute a transaction and check its correctness, thereby endorsing it, order transactions via a (pluggable) consensus protocol, and validate transactions against an application-specific endorsement policy before committing them to the ledgerapplication-specific endorsement policy 指的是，对于每个交易，只有所有背书节点中的一部分来进行背书，这样所有节点就可以并行计算。另外第一个阶段也保证了确定性，因为保证了确定性，所以 fabric 才能很轻松的支持多合约语言（各种合约语言导致的不确定性问题，在第一阶段就解决了）。Privacy and Confidentiality这里讲了在商业上隐私的重要性，并且提出了一些解决隐私问题的方法，比如加密数据、零知识证明等。在 fabric 内使用的是 channel 的方式，channel 连接特定的一些节点，对外不可见。以后还会推出基于零知识证明的私有数据的方式。Pluggable Consensus现阶段 fabric 提供了 基于 Kafka 和 Zookeeper 实现的 CFT共识算法，以后会推出 Raft 和 BFT 的共识算法。Performance and Scalability我们知道，很多联盟链使用的共识算法，对于节点数量比较少的情况，性能都很好。这段主要说的是 1.1.0 版本（现在是 1.3.0）性能又有了很大的提升，比 1.0.0 有了超过两倍的提升。" }, { "title": "PeerCoin白皮书", "url": "/posts/PeerCoin%E7%99%BD%E7%9A%AE%E4%B9%A6/", "categories": "study", "tags": "", "date": "2018-09-29 00:00:00 +0800", "snippet": "Sunny King 是 PoS 的发明人，也是点点币 (PeerCoin) 和质数币 (PrimeCoin) 的创始人。在介绍 PoS 源码之前，先说下自己对 PeerCoin 的白皮书 部分内容的理解。有错误的地方，欢迎留言指正。introductionWe have since formalized a design where proof-of-stake is used to build the security model of a peer-to-peer crypto currency and part of its minting process, whereas proof-of-work mainly facilitates the initial art of the minting process and gradually reduces its significance.在 peercoin 内，重新定义了 coin age (币龄)，我们知道在比特币里，币龄会影响到矿工打包的优先级，但是也仅仅如此，没有其他用处。但是在 peercoin 的 PoS 内，币龄很重要，影响挖矿的难度和奖励。//这个是验证 hash 的代码，这里的 bnTargetPerCoinDay 就是难度值，bnCoinDayWeight 是币龄//所以币龄越大，难度就越低CBigNum(hashProofOfStake) &amp;gt; bnCoinDayWeight * bnTargetPerCoinDay//这个计算奖励（也可以说是利息）的代码，nCoinAge 是币龄，nRewardCoinYear 是一个固定的数//所以币龄越大，挖矿奖励就越高。每年可以得到 1% 的奖励（利息）。int64 nSubsidy = nCoinAge * 33 / (365 * 33 + 8) * nRewardCoinYear;另外在 peercoin 内，同时使用了 PoW 和 PoS，PoW 主要是用于链的初始阶段，为了生成一定量的初始币，有了币才有币龄，才能进行 PoS，往后会被逐渐被 PoS 取代掉。以太坊不也是这样么，现在还是 PoW，也在考虑替换成 PoS。Coin AgeIn order to facilitate the computation of coin age, we introduced a timestamp field into each transaction. Block timestamp and transaction timestamp related protocols are strengthened to secure the computation of coin age.不只是 block 有时间戳，transaction 也有时间戳，这里面提高的提高币龄计算的安全性。在 CoinStake 交易体内的时间戳，一是记录挖矿的时间，二是这个时间是可以在 60 秒范围内变动的，达到 PoW 中 nonce 的作用，挖矿的过程，其实是不断更改时间戳，其实在 PoW 中，由于现在难度越来越高，计算速度越来越快，nonce 的值很快就计算完了，如果还是没有合适的值，现在的做法有两种，一种是更改 CoinBase 中的 extern_nonce，一种是在一定范围内更改时间戳，跟这个做法是类似的。在普通的交易体内，这个时间戳是为了更方便的计算币龄，因为普通交易体的币龄也是有用的，虽然不能给矿工增加奖励，但是可以提高整个块的币龄得分，在有竞争块的时候，币龄得分是选择的依据。Block Generation under Proof-of-StakeThe proof-of-stake in the new type of blocks is a special transaction called coinstake (named after Bitcoin’s special transaction coinbase). In the coinstake transaction block owner pays himself thereby consuming his coin age, while gaining the privilege of generating a block for the network and minting for proof-of-stake. The first input of coinstake is called kernel and is required to meet certain hash target protocol, thus making the generation of proof-of-stake blocks a stochastic process similar to proof-of-work blocks. However an important difference is that the hashing operation is done over a limited search space (more specifically one hash per unspent wallet-output per second) instead of an unlimited search space as in proof-of-work, thus no significant consumption of energy is involved.PoS 用 coinstake 代替了 coinbase，coinstake 包含的内容如下图：这个图里面，可以看到输入包含了两部分，一个是 kernel input，一个是 stake input 组，其实都是未花费的币，但是有所区别： kernel input，这个是特殊的 stake input，它的 hash 需要满足难度目标，是的，不是所有 stake 的币都可以加速挖矿，而是只有一个可以，这个反映了 PeerCoin 鼓励大额交易的思想。与 PoW 的主要区别是，PoW 的 hash 是从一个类似无限空间计算出来的（nonce, extern_nonce），而 PoS 是在一个有限的空间： 每次取出所有未花费的交易中的一个，然后计算有限的次数，这个次数是：上次计算到这次计算的间隔，每 1 秒可以计算一次，每次计算，时间戳减 1，然后检查 hash 值。所以这里是“每个未花费的交易每秒可以计算一次”。 stake input，这个就是普通的未花费的币，虽然不能加速挖矿，但是可以增加奖励。 coinstake 的输出是矿工自己，将币支付给自己，消耗掉了币龄，得到了奖励。Minting based on Proof-of-StakeA mint rate of 1 cent per coin-year consumed is chosen to give rise to a low future inflation rate.矿工手里的币，每年增加 1%，当然不是自己就增加的，而是必须要通过挖矿，消耗掉币龄，获取奖励的方式。这里每个 COIN 的单位是 1000000，每个 CENT 是 10000，每币年的奖励是一个 CENT，所以是 1% 。Main Chain ProtocolThe protocol for determining which competing block chain wins as main chain has been switched over to use consumed coin age. Here every transaction in a block contributes its consumed coin age to the score of the block. The block chain with highest total consumed coin age is chosen as main chain.决定 block 获胜的条件变成了，根据每个 block 的币龄得分，币龄得分来自两个部分： 矿工消耗掉的币龄 block 中所有 transaticon 消耗掉的币龄Checkpoint: Protection of History为了防止整链攻击和双花攻击，PoS 也使用 checkpoint 机制，也是中心化的，但是是每天多次广播，来冻结区块链，防止篡改。Our solution is to modify the coin age computation to require a minimum age, such as one month, below which the coin age is computed as zero.为了防止拒绝服务攻击，PoS 对币龄有限制，所有币龄必须要至少大于一个月，否则就是 0 .Block Signatures and Duplicate Stake ProtocolA duplicate-stake protocol is designed to defend against an attacker using a single proofof-stake to generate a multitude of blocks as a denial-of-service attack. Each node collects the (kernel, timestamp) pair of all coinstake transactions it has seen. If a received block contains a duplicate pair as another previously received block, we ignore such duplicate-stake block until a successor block is received as an orphan block.对于重复的块（kernel 和 时间戳相同），节点会选择忽略。Other ConsiderationsWe modified the proof-of-work mint rate to be not determined by block height (time) but instead determined by difficulty. When mining difficulty goes up, proof-of-work mint rate is lowered. A relatively smooth curve is chosen as opposed to Bitcoin’s step functions, to avoid artificially shocking the market. More specifically, a continuous curve is chosen such that each 16x raise of mining difficulty halves the block mint amount.挖矿的奖励现在不是随区块高度（时间）变化，而是随难度变化，难度越高，奖励越少，这样会使曲线平滑，16倍的难度提升，奖励减半。在比特币里，难度提高，奖励是没有变化的（在每 4 年的衰减期之内），因为难度提高使每个块的产出时间维持稳定，矿工其实并没有从提升设备中收益太多，但是由于公地悲剧，挖矿的成本还是在不断飙升（设备、电力）。而奖励随难度变化也无法解决这个问题，所以这里就只是为了平滑曲线而已。Babaioff et al. (2011) studied the effect of transaction fee and argued that transaction fee is an incentive to not cooperate between miners. Under our system this attack is exacerbated so we no longer give transaction fees to block owner. We decided to destroy transaction fees instead. This removes the incentive to not acknowledge other minter’s blocks. It also serves as a deflationary force to counter the inflationary force from the roof-of-stake minting.这里讨论的是交易费的影响，交易费被认为是导致矿工之间不合作的原因，所以 peercoin 中不再将交易费给矿工，而是直接消耗掉（并不是不再需要交易费），一是解决了矿工之间不合作的问题，二是会产生遏制通货膨胀。一些我的想法在写这篇文章的时候，我已经看过了源码，我有个疑问就是“矿工是否会越来越少”，有这个想法是因为： 矿工挖矿无法收到手续费。 在以后纯 PoS 的时候，挖矿也没有奖励。 矿工手中的币产生的利息，激励着矿工挖矿，但是挖矿很快，币龄消耗得也很快，而且币龄有最小时间限制，那么就是挖过矿之后，很久才能再从挖矿中获得收益。那么是什么激励矿工在没有收益的情况下，继续挖矿，将其他人的交易打包到区块链呢？我能想到的原因是： 矿工会为了自己币不贬值，而自愿无报酬挖矿。但是不是很有说服力（连我自己都说服不了），因为这个和“公地悲剧”是一样的，普通人都不会看很远，都关注眼前的利益，既然挖矿没有收益还费电，那么为啥要挖呢？谁又会考虑到以后都不挖矿导致的币贬值呢？ 矿工基数很大，大到就算矿工只在自己会有收益的时候才挖矿，网络上也一直有矿工在挖矿。参考https://peercoin.net/assets/paper/peercoin-paper.pdf" }, { "title": "PeerCoin_PoS源码", "url": "/posts/PeerCoin_PoS%E6%BA%90%E7%A0%81/", "categories": "study", "tags": "", "date": "2018-09-29 00:00:00 +0800", "snippet": "之前学习了 PeerCoin 的白皮书，这篇就一起看下核心的 PoS 机制，PeerCoin 是 PoS 的鼻祖，所以还是很有代表性。从 CreateNewBlock (src/main.cpp) 开始：创建 coinstake 的主要流程都在 CreateCoinStake (src/wallet.cpp) 内：这里说下手续费我们知道，在比特币里，手续费是支付给矿工的，作为打包的报酬，而矿工自己是不用支付手续费的。而在 PeerCoin 内，手续费是直接消耗掉的，不支付给任何人，包括矿工，而矿工自己也需要支付手续费。至于这么做的目的，有两点： 解决了矿工之间由于互相竞争，导致不合作的问题。 遏制通货膨胀。到这里，整个 PoS 的流程就介绍完了，PeerCoin 是从比特币 fork 出来的，所以很多基础流程是差不多的。那么 PeerCoin 的 PoS 的优势在哪呢，我觉得有如下这些： 加快了区块的产生 降低了能源的消耗 其他一些机制，比如通过去除手续费，来避免矿工间的竞争；对通货膨胀的考虑等。" }, { "title": "以太坊PoS设计", "url": "/posts/%E4%BB%A5%E5%A4%AA%E5%9D%8APoS%E8%AE%BE%E8%AE%A1/", "categories": "study", "tags": "", "date": "2018-09-25 00:00:00 +0800", "snippet": "最近在看 PoS，会陆续更新两篇文章，一篇介绍 PoS 设计思想，一篇看下 PoS 源码。这是第一篇，内容主要来源于以太坊 GitHub 上的《Proof of Stake FAQ》，这篇文章里面 V 神介绍了很多设计思想，很好的一篇文章，但是看起来比较吃力，涉及了很多概念，目前只看完了一部分，就先总结下这部分，这篇文章会不断更新。有理解错误的地方，欢迎留言指正。PoS 介绍权益证明（PoS）是一类应用于公共区块链的共识算法，取决于验证者在网络中的经济权益。一组验证者轮流提议并票决下一个区块，而每位验证者的投票权重取决于其保证金额的大小（即权益）。PoS 的优势 不需要为了保护区块链而消耗大量电力（例如，比特币和以太坊预计每天要在共识机制的电力和硬件上耗费超过100万美元的成本）。 这个是目前 PoW 算法被诟病的一个主要问题，这个问题不只是导致资源浪费，还因为门槛过高，还导致了中心化问题，这个对于区块链来说，是致命的。 由于权益证明避免了高电耗，就没有太多必要为了保持网络中参与者的积极性而发行很多新代币。从理论上来说，甚至有可能变为负发行量，其中一部分交易费被“烧掉（burned）”，因此供应量会逐渐减少。 我觉得为了保持货币价值稳定，供应量多少应该和承载的价值匹配，但是如果资源浪费导致多发行货币，就不应该。 权益证明有助于实现更多采用博弈论机制设计的技术，从而更好地抑制中心化卡特尔式机构的形成，如果这种机构确实形成了的话，也能够阻止它们危害网络。 降低中心化风险，因此规模经济不会造成太大问题。你不会因为负担得起更好的大批量矿机而获得与投入资金不成比例的收益，在PoS中，1000万美元投资带给你的收益就是100万美元投资的整整十倍。 虽然都是有钱，但是一个是投入到了矿机中，一个是投入到了区块链中，这两者有很大不同，投入区块链本身的人更不容易作恶。 能够采用经济处罚，这让发动各种形式的51%攻击所要付出的代价比在工作量证明中高出许多。 扣除保证金 PoS 的两种类型: chain-based proof of stake共识算法在每个时段内伪随机地选择一个验证者（例如，每10秒钟为一个时间段），赋予该验证者出块的权力，新创造的区块必须跟在之前的某个区块（通常是位于最长链的末端的区块）后面。因此，随着时间的推移，大多数区块会填加到同一条区块链上，使之不断增长。 选择一个验证者，出的块就是最终的区块。 BFT-style proof of stake虽然提议区块的权力会随机分配给验证者，但是决定哪一个区块是“合法的”要通过一个多轮过程来完成。每个验证者在每一轮都会给某个特定的区块投出一票，在这个多轮过程的最后，所有（诚实并联网的）验证者会就是否将这个区块添加到链上做出最终决定。要注意的是这些区块可能仍然链接在一起，其关键区别在于对一个区块的共识可以仅限于这个区块本身，与它后面的那条链的长度和大小无关。 选择一些验证者，每个验证者都出区块，经过多轮的投票，选出最终的一个区块。 在 CAP （后面会说）中，工作量证明算法和基于区块链的权益证明算法选择了可用性而非一致性，然而拜占庭容错型共识算法更青睐一致性，Casper 采用的混合模型虽然偏向可用性，却又尽可能地实现一致性。无利害关系在之前的 PoW 和 PoS 算法里，仅有创建的奖励，但是没有惩罚措施，在出现多条区块链相互竞争的情况下，会激励验证者在每条链上都创造区块，以确保获得奖励。 这个在 PoW 中虽然也存在，但是因为分开挖矿分散算力，所以情况不明显。但是在 PoS 就不一样了，这个也导致了“权益粉碎攻击”。在以太坊的设计里，增加了惩罚措施，对同时出块的验证者，扣除部分保证金。 在 PoW 里，惩罚其实也是存在的，只不过是隐性的，矿工必须花费额外的电力并且获得或租用额外的硬件。验证者选择机制和权益研磨在任何基于区块链的权益证明算法中，都需要某种机制，来随机从当前活跃验证者集合中选择能够产生下一个区块的验证者。“权益研磨” 是一种攻击类型，在这种攻击中，验证者通过执行一些计算或者采取某些其他措施使得随机性更偏向他们（也就是说，让他们有更大概率成为产生下一个区块的验证者）。 区块链上使用的是一般是伪随机，随机源取自前一个区块的签名、hash或验证者，但是这些都给了攻击者可乘之机，通过反复生成签名、hash或跳过产生区块，能够控制下一个验证者。 一种解决方式是，让验证者合作产生随机数。Casper 的 51% 攻击这里的 51% 只是个概念，并不是一定是 51% 算力。主要有下面集中攻击形式： 确定性回滚。就是因为同时出现的两个区块导致的分叉。通过社交共识来解决硬分叉。 就是在社交媒体上，大家达成共识，选择一条分叉。。 拒绝区块活性。算力大于 34% 的验证者就可以拒绝添加更多的区块，这会导致永远不会有新的区块被添加。 有两种解决方式：协议自动轮换验证者集合，并处罚旧的验证者；通过硬分叉的方式。 屏蔽攻击。算力大于 34% 的验证者还可以拒绝添加某个他们不喜欢的某些交易类型的区块，可以干涉某些应用等。也有两种解决方式：协议拒绝明显在屏蔽交易的区块；通过硬分叉的方式。 总结下就是，代码识别或者硬分叉解决资本锁定成本这里主要是对比了 PoW 和 PoS，PoW 依赖硬件并且大量消耗能源，所以在考虑到摩尔定律（导致硬件升级）、电费的情况下，投资的很大部分被消耗在这些上面，还是之前说的问题，投资被无意义的浪费掉了，而且门槛越来越高，中心化问题也就越来越严重。而 PoS 是稳定的收益，这点上优势很明显。一些概念 CAP 定理分布式系统的难点是各个节点的状态同步，一个基本定理就是 CAP，即 C、A、P 无法同时做到。 C，Consistency，一致性。在不同节点读到的是一致的数据，也就是各个节点完全同步。 A，Availability，可用性。只要收到请求，服务器就必须给出回应。 P，Partition tolerance，分区容错，指区间通信可能失败。 一般认为 P 总是成立的。那么这个定理实际上说的就是 C 和 A 无法同时做到。 为什么无法做到？要保证一致性，在一个节点写的时候，就必须锁定其他节点，在同步完成之后，才能解锁。但是在锁定期间就无法保证可用性。 FLP 不可能定理在异步环境中（即，即使在正常运行的节点之间，也无法控制网络延时上限），不可能创造出一种算法，在出现单个故障或不诚实的节点之时，确保能在任和特定的有限时间内达成共识。 拉斯维加斯算法蒙特卡罗算法：采样越多，越接近最优解。找到的可能不是最优，但是每次都能找到。拉斯维加斯算法：采样越多，越有可能找到最优解。找到的肯定是最优解。 在共识算法中指，该算法每轮都有一定的概率达成共识，因此在T秒内达成共识的概率会随着T的增加呈指数增长并向1趋近；这实际上是许多成功的共识算法都会使用的“应急之策”。 卡特尔式机构生产同类商品的企业为了获取高额利润，从而达成一个同盟，控制商品产量和价格，使竞争性市场变成了一个垄断市场。 自私挖矿挖到一个区块，并不马上公布，而是继续挖下一个，这样浪费了其他人的算力。如果有人公布了新块，那么他也要马上公布新块，并通过女巫攻击来抢先让他们偷挖的区块得到网络的承认。 需要拉拢其他不诚实的节点，因而需要提供给他们额外的回报，让他们优先确认自己的区块。 通过代码隐藏一个区块也是一个技术门槛 为了第一时间发现别人公布了区块，那么网络必须非常好 这些因素，导致有可能输掉了区块竞争，导致收益不稳定，得不偿失，所以这个还是一个争议问题。 女巫攻击 恶意节点可以分裂出来多个节点，这样冗余备份就没有作用。应对方式： PoW，通俗点就是必须要能干活，需要有计算能力，这样攻击成本就很高。 基于第三方的身份认证 纯分布式的身份认证，每加入一个新节点需要获得当前网络中可靠节点的认证。 公地悲剧 典型的例子是牧场理论： 公共草地上，有一群牧羊人，每一个牧羊人都想要多获利一些，所以某个牧羊人就带了大量的羊来放牧，虽然他知道过度放牧，草地可能会承受不住。但他依然获利了，而后所有的牧羊人都跟进，所以草地牧草耗竭，悲剧因而发生了。关键性是牧羊人获得了所有的利益，但是资源的亏损却是转嫁到所有牧羊人的身上。这种例子在现实中真的是很多。 参考权益证明 FAQ (中文)，https://ethfans.org/posts/Proof-of-Stake-FAQ-new-2018-3-15权益证明 FAQ (英文)，https://github.com/ethereum/wiki/wiki/Proof-of-Stake-FAQsCAP 定理， http://www.ruanyifeng.com/blog/2018/07/cap.htmlFLP 不可能定理，https://yeasy.gitbooks.io/blockchain_guide/content/distribute_system/flp.html蒙特卡洛与拉斯维加斯算法，https://blog.csdn.net/hejun_haitao/article/details/52588423权益证明设计哲学，https://medium.com/@VitalikButerin/a-proof-of-stake-design-philosophy-506585978d51" }, { "title": "以太坊和比特币不同的PoW实现", "url": "/posts/%E4%BB%A5%E5%A4%AA%E5%9D%8A%E5%92%8C%E6%AF%94%E7%89%B9%E5%B8%81%E4%B8%8D%E5%90%8C%E7%9A%84PoW%E5%AE%9E%E7%8E%B0/", "categories": "study", "tags": "", "date": "2018-09-20 00:00:00 +0800", "snippet": "比特币的 PoW 算法，之前的文章中已经提到，以太坊目前使用的共识算法 (Ethash) 虽然也是 PoW，但是与比特币的不同。bitcoin 的 PoW 算法比特币的工作量证明算法，可以用下面这个公式表示:hash_output = sha256(prev_hash, merkle_root, nonce)hash_output 应该不大于 nBits prev_hash，前一个区块的 hash，固定值。 merkle_root，默克尔树的根，每个矿工计算出来的都不一样，但是同一个矿工每次计算都一致。 nonce，如果 hash 计算的结果大于 nBits，那么矿工就会改变 nonce 的值，重新计算 hash。（这里没考虑 extern_nonce）这个算法的实现很简单，很容易理解，很符合共识算法的特点，求解困难验证容易，至今也运行良好，但是慢慢显现出来一些弊端： 求解过程完全依赖 CPU 的计算能力 ，所以出现了很多针对 hash 优化的芯片，即 ASIC。 新的硬件抛弃了通用性，只做 hash，因而大幅提升了求解的速度，为了追上大家的速度，矿场只能升级硬件，这导致大批量的矿机过时，而专用的芯片只能保持一段时间的领先，在大家都升级之后，又变得毫无优势，另外关键的是这种芯片只能用来挖矿，还有其他用途么？ 另外这种硬件很昂贵，普通的家用电脑越来越难与之对抗，算力越来越集中于少数资金雄厚的矿场手中。 普通人目前只能通过加入矿池才有机会。其实这种矿工，根本算不上矿工，因为所有的数据都是由矿池提供的，自己只是计算 hash，相当于这个庞大计算机中的一个 cpu 而已，实际控制权都在矿池，也导致了算力越来越集中。 其实矿池的出现也是因为算法，因为其中变化的变量只有 nonce 一个，其他都是固定的，所以可以没有区块数据，可以不用大内存，甚至可以没有硬盘，只要能满足计算 hash 的需求即可。 另外所需的存储空间也很大，现在 200 多 G，这已经超过大多数手机的存储空间，这也限制了挖矿的人群。 正因为这些问题，才发明了后续的很多算法。ETH 的 PoW 算法，EthashEthash 的前身是 Dagger Hashimoto，而 Dagger Hashimoto 的前身是 Dagger 和 Hashimoto 算法。将前面说的问题总结下，这些也是这些算法要解决的问题： 抵制矿机（ASIC）。 全链数据存储。前面说的矿池的例子。 轻客户端。前面说的手机的例子。我们看下 hashimoto 和 ethash，dagger 包含在了 ethash 里面。1. HashimotoHashimoto I/O bound proof of work，基于 I/O 带宽的工作量证明算法。Hashimoto 由 hash, shift, modulo 组成。直接看下算法代码# hashhash_output_A = sha256(prev_hash, merkle_root, nonce)for i = 0 to 63 do # shift shifted_A = hash_output_A &amp;gt;&amp;gt; i # mode transaction = shifted_A mod total_transactions # I/O txid[i] = get_txid(transaction) &amp;lt;&amp;lt; iend fortxid_mix = txid[0] ⊕ txid[1] … ⊕ txid[63]final_output = txid_mix ⊕ (nonce &amp;lt;&amp;lt; 192)final_output 应该不大于 nBitssha256 的结果不直接与 nBits 比较了，而是作为了中间变量。下面的循环里，调用了 get_txid，这个函数获取对应编号的 transaction 的 id 值，这个 transaction 是从内存或磁盘里取出来的，所以这里是 I/O 操作。然后在将获取到的 id 异或得到最终的 output，将这个与 nBits 比较。这个算法解决了上面说的 1 和 2 的问题，是怎么解决的？首先，求解过程不只是 hash 了，还需要一些读取操作，大量的时间消耗在了 I/O 操作上，所以单纯的 ASIC，只是提高了求 hash 的速度，无法使整体获得很大的提升。那么可以用告诉存储介质来提升速度，确实是，可以制作超高速的内存或者固态硬盘，但是那么容易么？这个目标是现在所有内存或者固态硬盘厂家的目标，现在这么多厂家都没解决的问题，可能短时间做到么？不可能的，那 ASIC 是怎么回事？因为只计算 hash 这个目标和 CPU 厂家的目标是不同的， CPU 厂家的目标是通用性，而不是这一个算法。这个解决了 1 的问题。然后，还是因为 I/O，考虑矿池的情景，大量的矿工因为没有区块链数据，只能通过网络向矿池请求数据，在一次计算中，这种请求就有 64 次，多次计算，这种请求的总数就很客观，矿池为了应付庞大数量的请求必然需要投入非常多的硬件成本，这就不划算了，还不如大家自己保存区块链数据。另外网络延迟也是一种消耗。这个解决了 2 的问题。而且，在这种算法下，矿工需要保存完整的区块链数据，哪怕只少了一个区块，也会导致挖矿成功的概率降低很多。文章中举了一个例子，假设一共 100 个区块，少了 1 块，那么就需要 2 倍的 hash 次数，少了 2 块，就需要 4 倍，这个很好理解，想了解的可以看下 这个文章。2. ethash我们直接看下源码，从入口开始看：Seal，consensus/ethash/sealer.go这个函数主要是创建和子线程的控制，然后我们看下同一个文件的 mine 这个函数：其中的 hashimotoFull，这个名字很熟悉了，前面介绍过 hashimoto 的算法，这里的算法实现和之前的有所区别，主要是这里使用的数据集不再是整个区块链，而是叫做 DAG 的数据集，这个后面再说。hashimotoFull，consensus/ethash/algorithm.go这函数主要是创建了一个闭包函数，传递给了 hashimoto，这个闭包函数在根据传进来的参数，在数据集中选择特定的数据。看下还是在这个文件内的 hashimoto 函数简单介绍下 FNV，FNV 是一种哈希算法，具有高离散性，特别适用于哈希非常相似的字符串，函数如下，还是在这个文件里：到这里整个挖矿的流程就介绍完了。3. DAGethash 与传统 hashimoto 不同的地方，主要是使用了特殊的数据集 DAG，而生成 DAG 需要先生成 cache。这里面有几个基础概念，先介绍下。 epoch。每 30000 个区块，叫做一个 epoch，“一世”，按照 12 秒一个区块的速度，大概是 100 小时。 epochLength = 30000epoch := block / epochLength cache/DAG 的生成周期。因为生成 DAG 很慢，所以在一定周期内，DAG/cache 都是复用的，每个 epoch 重新生成一次。DAG 依赖 cache，cache 依赖 seed，而 seed 只和区块高度有关，所以可以预先生成。 seed := seedHash(c.epoch*epochLength + 1) cache/DAG 的大小。大小是不固定的，epoch 在 2048（区块高度在 61440000）之前，是按照写死的数组里的数来得到 size，在之后是按照一个公式来计算，我们看下代码片段： func (d *dataset) generate(dir string, limit int, test bool) { ... csize := cacheSize(d.epoch*epochLength + 1) dsize := datasetSize(d.epoch*epochLength + 1) ... generateCache(cache, d.epoch, seed) generateDataset(d.dataset, d.epoch, cache) ... func cacheSize(block uint64) uint64 { epoch := int(block / epochLength) if epoch &amp;lt; maxEpoch { return cacheSizes[epoch] } return calcCacheSize(epoch) // 超过 2048 epoch，用这个函数计算} func datasetSize(block uint64) uint64 { epoch := int(block / epochLength) if epoch &amp;lt; maxEpoch { return datasetSizes[epoch] } return calcDatasetSize(epoch) // 超过 2048 epoch，用这个函数计算} func calcCacheSize(epoch int) uint64 { size := cacheInitBytes + cacheGrowthBytes*uint64(epoch) - hashBytes for !new(big.Int).SetUint64(size / hashBytes).ProbablyPrime(1) { // Always accurate for n &amp;lt; 2^64 size -= 2 * hashBytes } return size} func calcDatasetSize(epoch int) uint64 { size := datasetInitBytes + datasetGrowthBytes*uint64(epoch) - mixBytes for !new(big.Int).SetUint64(size / mixBytes).ProbablyPrime(1) { // Always accurate for n &amp;lt; 2^64 size -= 2 * mixBytes } return size} 可以看到，在 2048 epoch (区块高度 61440000) 之前，实际上就是在数组里取值，数组是这样的： 现在 ETH 的区块高度为 6365581，对比下 61440000，这些数想要用完还得很久很久，到 2048 的时候，cache 和 DAG 的大小分别为： DAG:18245220736/8/1024/1024 = 2174.999，大概 2Gcache:285081536/8/1024/1024 = 33.984，大概 33MB 这些说完了，我们先看下生成 cache 的代码，generateCache (consensus/ethash/algorithm.go)接下来看看 dataset 生成的代码，generateDataset (consensus/ethash/algorithm.go)到这生成 cache 和 DAG 的流程就都介绍完了。4. ethash 解决了什么问题我们再回忆下之前说的共识算法要解决的三个问题： 抵制矿机（ASIC）。 全链数据存储。前面说的矿池的例子。 轻客户端。前面说的手机的例子。之前的 hashimoto 解决了，1 和 2，但是因为需要全链数据，所以 3 没解决。ethash，使用了类似 hashimoto 的算法，在保证 1 的基础上，因为 DAG 要比全链数据小很多，所以理论上可以应用在轻客户端上，3 算是解决了，那么 2 呢？首先肯定不是全链数据了，而且从 ETH 的代码里看，默认支持远程挖矿，也就是矿池模式，所以 2 是没解决的，或者说是以太坊不想解决的。那么实际上中心化问题还是有的，但是因为解决了 1 和 3，使普通人可以挖矿，挖矿的整体人群扩大了，所以综合来看，个人觉得还是解决了部分的中心化问题。参考https://www.cnblogs.com/Evsward/p/ethash.htmlhttps://github.com/ZtesoftCS/go-ethereum-code-analysis/blob/master/hashimoto.mdhttps://blog.msiter.com/Hashimoto%20IO%20bound%20proof%20of%20work-20180824.htmlhttps://github.com/ethereum/wiki/wiki/Mining#so-what-is-mining-anyway" }, { "title": "闪电网络和分片", "url": "/posts/%E9%97%AA%E7%94%B5%E7%BD%91%E7%BB%9C%E5%92%8C%E5%88%86%E7%89%87/", "categories": "study", "tags": "", "date": "2018-09-16 00:00:00 +0800", "snippet": "这篇记录下我对闪电网络和以太坊分片技术的理解和一些看法。网络上介绍技术实现的文章很多（详见参考），所以这里不详细讨论实现细节，只是多说下自己的想法。欢迎留言探讨。Lightning Network闪电网络基于微支付通道演化而来，主要包含两个部分 RSMC（序列到期可撤销合约） 和 HTLC（哈希时间锁定合约）。首先闪电网络是用于比特币的，因为要用到类似合约的用法，所以这里先简单说下，比特币的合约。比特币其实是支持合约的，可以看 这篇官网 wiki 介绍。我看完这个介绍的感觉是这很初级，很实用。大家知道比特币网络本身是不支持智能合约的，但是比特币的签名机制可以对输入和输出签名，应用这个，可以实现链下的合约。一般的做法就是，首先构建一个交易体，某个投资人针对这个交易体的自己部分的输入和输出进行签名，然后交给其他人投资人，其他人再增加自己的输入和签名，以此类推，直到这个交易体满足大家的预期，然后就可以将这个交易体广播出去。这其中根据不同的场景有很多细节，这里就不一一说。而 RSMC 就是应用了方式，在链下构建了交易通道，根据需要构建了很多的交易体，交易体对应着链下的交易，也就是一些微交易，最终的交易结果，只有到链下交易通道需要终止的时候，才会最终上到比特币网络，这之前的交易，都不用上链。有个地方设计得很巧妙：每个人手中的终止交易的交易体，如果广播的话，是先把钱给对方，然后自己等待一定区块确认后，才能拿到自己那部分。这个配合废弃旧交易的流程，如果某人作恶，广播旧交易的话，那么另一方可以知道，然后修改交易体，让作恶方一分钱都拿不到，防止欺诈。而 HTLC 就是链下跨通道之间交易，不多说了。闪电网络可能有哪些问题呢？我觉得，首先，这些都是链下交易的，只有最终结果才上链，所以中间流程都是没有记录的，另外链下交易的安全性，也是需要考虑的。其次，流程设计很巧妙，但是也很繁琐，比如互相交换交易体签名，在废弃旧交易体的时候提供私钥，为了防止欺诈还要关注比特币网络的状态。最后，因为需要交换私钥，那么就要求私钥的派生，采用硬化派生，这个就有一定的技术要求，或者钱包支持。sharding其实分片思想来源于传统数据库的分片，从区块链角度来看的话，现在所有的交易，每个全节点都需要全部保存和验证，那么假设每个节点的计算力是 c，交易容量是 \\(O(c)\\)，那么实际整个网络的交易容量也是 \\(O(c)\\)，和单个节点的容量是一样的。那么这样重复的验证，是必须的么？不一定，以 PoW 来说，只要分片后每个片区的节点数量足够多，安全性也是可以保证的，但是和以前相比肯定是降低了。以太坊的分片方案 Casper Pos，这是一个全新的算法，引入了 collation 的概念，每个 collation 包含实际的交易数据，由定期随机选择的 collator 负责打包，每个分片产生一个 CollationHeader，在主网中只包含这些头部。那么性能可以提升多少呢？官方的说法是达到\\(O(c^2)\\)，这个可以这样理解，原来的容量是\\(O(c)\\)，这里包含的是直接的交易数据，现在变成了 CollationHeader，而 CollationHeader 又包含 \\(O(c)\\)的实际交易数据，所以是\\(O(c^2)\\)。这里面 collator 的选择是随机的，从所有在主网合约上注册的候选人里挑选，没有人提前知道自己会被选上，也没有人知道会被选择成为哪个分区的 collator，这可以杜绝一些作恶的场景。这里的核心思想和 DPoS 类似，需要一定数量的保证金，随机委任为生成区块的负责人，有利益相关的人更不可能会作恶（不过真的是这样么？）。分片看着挺不错的，那么会有哪些挑战呢？首先，分片根据地址（或地域，可能会引入更多问题）来划分，且账本分离，那么就不可避免的需要跨分片通信的能力。另外就算跨分片实现可以，也会存在所谓的“火车旅馆问题”。其次，像之前说的，分片导致了每个片区节点数量的降低，那么安全性就会降低，尤其是现在节点数量并不是很客观，进一步的拆分节点，被攻击的可能性就更高些。最后，就是分片使区块链更复杂，越复杂的东西出问题的概率就越高，被攻击的可能性也就越高，就是第一阶段，现在也没落地，大概还需要一年的时间测试，何况第一阶段还不包括跨片交易。感想在看以太坊官方的分片文章的时候，除了分片原理的介绍，里面还有很多其他方面的讨论，比如安全模型、各种攻击形式、抽样随机性等，给我的感觉就是做一个区块链产品，尤其是公链还涉及到资金，架构方面，功能设计只是最基本的，安全和经济模型都是需要着重考虑的，真的是十年磨一剑。参考https://ethfans.org/posts/vitalik-sharding-on-ethereumhttps://ethfans.org/posts/how-to-scale-ethereum-sharding-explainedhttps://ethfans.org/posts/Sharding-FAQhttps://ethfans.org/posts/ethereum-sharding-and-finalityhttps://ethfans.org/posts/making-sense-of-ethereums-layer-2-scaling-solutionshttp://finance.sina.com.cn/blockchain/coin/2018-06-04/doc-ihcmurvh0978930.shtmlhttps://www.zhihu.com/question/46515457" }, { "title": "比特币源码学习-隔离见证", "url": "/posts/%E6%AF%94%E7%89%B9%E5%B8%81%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0-%E9%9A%94%E7%A6%BB%E8%A7%81%E8%AF%81/", "categories": "study", "tags": "", "date": "2018-09-13 00:00:00 +0800", "snippet": "这篇文章介绍下我所理解的 Segregated Witness（隔离见证），隔离见证比较复杂，看了源码和很多介绍的文章，也只能算是初步了解了隔离见证。所以这篇文章陆续会不断更新。有错误的地方，欢迎留言指出。什么是隔离见证及解决什么问题？比特币区块容量限制为 1MB，每笔交易按照 250 字节计算，10 分钟一个区块的话，每秒钟能进行 6 笔交易，这个是非常少的，所以也就陆续出现了很多“扩容”方案。扩容方案总体有三个方向（不算 Segwit2MB 这种折衷方案）： 更改核心代码，增加区块容量，增加到 2MB、8MB等。但是不兼容之前版本，会导致硬分叉。 第二层的方案，即采用“闪电网络”，“侧链”等，将高频交易移出主网，可以不用改变核心代码，不会导致硬或软分叉。 Segwit，这是比特币核心团队提出的方案，在不改变 1MB 上限的前提下，将验证数据（脚本、签名）移出交易体，放到专门的数据结构里。这样由于交易体内存储的数据变少了，虽然上限没变，但是实际可以容纳的交易变多了，也就提高了交易速率，实际上相当于将 1MB 增加到了 3.7MB。关键的是这个虽然改变了核心代码，但是兼容之前版本，不会导致硬分叉的产生。支持增加区块容量的人（其实也曾是比特币核心团队成员，包括中本聪隐退后的负责人 Gavin Andresen）出来成立了 BitcoinClassic 、BitcoinUnlimited，后来硬分叉成为了 BCH ，而 BTC 这边，比特币核心团队也激活了 Segwit 作为扩容方案。哪种更好呢？这个真是争论不休。单从技术来看，我觉得 Segwit 更好一些，一是因为它将“交易状态”和“见证”（签名）分离开了，这种分层思想可以解决很多以后扩展和管理方面的麻烦；二是向前兼容，这是很多产品设计的核心，这也是架构设计最难的地方，也是最体现设计的地方。不过好于不好也不只是技术层面决定的，我是觉得哪种方案支持人数最多，那么它就是大家应该遵守的，这也是区块链这种去中心化的技术的核心，不是么？隔离见证的一些其他优点 杜绝了“无意的可塑性”。这个怎么理解呢？在之前的一篇介绍交易体的文章里，我们知道，交易体是由很多部分组成的，其中任意一部分的变化，都会影响到交易体的 id（hash），其中最有可能变化的是签名，因为在 P2SH 的模式里，如果是 n/m 这种多重签名，那么对同一份交易的有效签名可以有很多种，这就导致了交易的“可塑性”，这种当然不是刻意设计的，所以叫做“无意的可塑性”。这种变化当然是不希望的，因为这就是同一笔交易。Segwit 因为将签名拆分出交易体，所以杜绝了这种情况的发生。 签名数据的传输变为了可选。我们知道轻钱包中，只需要在本地保存区块头即可，验证交易的存在性，只需要下载特定的包含交易的交易体和默克尔树的中间值即可。在以往这样的交易体中包含签名数据，对我们是无用的，因为我们不关心交易的有效性（这是挖矿节点来验证的），我们只关心交易的存在性。Setwit 就进一步减少了轻钱包的存储负担。实现细节1. Transaction ID 及 Commitment因为 witness 的加入，每个交易有了两个 ID: txid，跟以往一样，通过如下数据的 DOUBLE SHA256 得到： [nVersion][txins][txouts][nLockTime] wtxid，新加入，通过如下数据的 DOUBLE SHA256 得到： [nVersion][marker][flag][txins][txouts][witness][nLockTime] marker: 0x00，flag: 0x01，所有 txin 的 witness 数据都存放在 [witness] 内。 所有交易的 wtxid 构成了一棵默克尔树，叶子就是 wtxid，计算出的 witness root hash 成为 commitment 的一部分，保存在 coinbase（挖矿交易）的 scriptPubKey（位于txout）内，commintment 结构如下（至少 38 字节）： 1-byte - OP_RETURN (0x6a) 1-byte - Push the following 36 bytes (0x24) 4-byte - Commitment header (0xaa21a9ed) 32-byte - Commitment hash: Double-SHA256(witness root hash|witness reserved value) 39th byte onwards: Optional data with no consensus meaning2. Segwit 如何工作分两种情况看下区别： P2PKH(Pay-to-Public-Key-Hash) 和 P2WPKH(Pay-to-Witness-Public-Key-Hash) # P2PKH:scriptSig: &amp;lt;signature&amp;gt; &amp;lt;pubkey&amp;gt;scriptPubKey: OP_DUP OP_HASH160 &amp;lt;20-byte hash of Pubkey&amp;gt; OP_EQUALVERIFY OP_CHECKSIG# P2WPKH:scriptSig: (empty)scriptPubKey: 0 &amp;lt;20-byte hash of Pubkey&amp;gt;witness: &amp;lt;signature&amp;gt; &amp;lt;pubkey&amp;gt; scriptSig 变为空（或者其他无效的数据都可以），scriptPubKey 也简化了，前置 0 是 witness version，注意接着的是 20 byte 的数据，以往的 scriptSig 的内容移到了 witness 内。 P2SH(Pay-to-Script-Hash) 和 P2WSH(Pay-to-Witness-Script-Hash) # P2SHscriptSig: 0 &amp;lt;SigA&amp;gt; &amp;lt;SigB&amp;gt; &amp;lt;2 PubkeyA PubkeyB PubkeyC PubkeyD PubkeyE 5 CHECKMULTISIG&amp;gt;scriptPubKey: HASH160 &amp;lt;20-byte hash of redeem script&amp;gt; EQUAL # P2WSHscriptSig: redeemScript / validation failsscriptPubKey: 0 &amp;lt;32-byte hash of redeem script&amp;gt;witness: 0 &amp;lt;SigA&amp;gt; &amp;lt;SigB&amp;gt; &amp;lt;2 PubkeyA PubkeyB PubkeyC PubkeyD PubkeyE 5 CHECKMULTISIG&amp;gt; scriptSig 可以是赎回脚本或其他无效的数据，scriptPubKey 跟上面一样，第一位是 version， 后面是 32 byte 的赎回脚本的 hash，这个是跟上面（20 byte）不同的，是为了区分，witness 里是原来的 scriptSig 的内容。 隔离见证采用了新的 bech32 编码，所以地址有了明显的变化，bc 开头：3. 最后看下源码里 segwit 相关代码先看下 coinbase 中 commitment 的创建，在 GenerateCoinbaseCommitment （validation.cpp）：其中 BlockWitnessMerkleRoot （merkle.cpp）：下面这个函数是 segwit 地址解析，DecodeDestination（key_io.cpp）这几个函数里介绍的结构，前面都已经介绍过。segwit 的想法是很赞，应用在现在的比特币里，总是感觉有些不优雅。比如 witness 的默克尔树的实现，根没有像交易体一样放到区块头里，而是放到了第一个交易 coinbase 内，这样是为了不硬分叉做的妥协，但是也确实使结构不清晰。参考https://bbs.huaweicloud.com/blogs/710256bf476611e89fc57ca23e93a89fhttps://github.com/bitcoin/bips/blob/master/bip-0141.mediawikihttp://www.bcfans.com/xueyuan/baike/15137.htmlhttps://zhuanlan.zhihu.com/p/30930715" }, { "title": "比特币源码学习-生成区块", "url": "/posts/%E6%AF%94%E7%89%B9%E5%B8%81%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0-%E7%94%9F%E6%88%90%E5%8C%BA%E5%9D%97/", "categories": "study", "tags": "", "date": "2018-09-13 00:00:00 +0800", "snippet": "之前学习了隔离见证，这篇主要介绍下生成区块的流程，中间会有部分前面提到的函数。generatetoaddress(mining.cpp) 是 cli 用来给特定地址生成区块的命令，从这个函数开始。其中的 DecodeDestination(key_io.cpp) 之前在看隔离见证的时候，讲过后面部分，下面看下前面部分：得到地址之后，函数里生成了 scriptPubkey，使用的是 GetScriptForDestination(standard.cpp):这个函数没什么东西，主要看下 CScriptVisitor(standard.cpp):接下来就是创建主流程了，位于函数 generateBlocks(mining.cpp)：这里最重要的函数是 CreateNewBlock(miner.cpp)：整个创建区块的流程就这些，其中 addPackageTxs 和 ComputeBlockVersion 涉及到内存池和 BIP9，内容很多，以后单独介绍。整个流程涉及的文件： mining.cpp key_io.cpp standard.cpp miner.cpp" }, { "title": "比特币源码学习-区块验证", "url": "/posts/%E6%AF%94%E7%89%B9%E5%B8%81%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0-%E5%8C%BA%E5%9D%97%E9%AA%8C%E8%AF%81/", "categories": "study", "tags": "", "date": "2018-09-10 00:00:00 +0800", "snippet": "之前学习了区块头验证 和 默克尔树，在区块的验证里都会用到，现在就来看看区块的验证。从 CheckBlock 函数（validation.cpp）开始：除了之前看过的区块头和默克尔根的验证，这里还涉及到两个函数，分别看下：CheckTransaction（tx_verify.cpp）：GetLegacySigOpCount（tx_verify.cpp）：这个函数有个从 txin 统计脚本操作符的过程，我知道在 P2SH (pay to script hash) 中，实际的脚本会存在于 txin 内，但是这个函数其实是计算的非 P2SH 情况，所以这里不是很理解。整体的 check block 的流程就是上面这些，主要设计到如下两个文件： validation.cpp tx_verify.cpp" }, { "title": "比特币源码学习-默克尔树", "url": "/posts/%E6%AF%94%E7%89%B9%E5%B8%81%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0-%E9%BB%98%E5%85%8B%E5%B0%94%E6%A0%91/", "categories": "study", "tags": "", "date": "2018-09-09 00:00:00 +0800", "snippet": "之前学习了区块头验证，今天开始看区块验证，在看到默克尔根验证的地方，代码注释的地方记录了之前的一个缺陷，挺有意思，就先学习下默克尔树，随便说下这个缺陷。默克尔树相关代码都在 merkle.cpp 中，BlockMerkleRoot 和 BlockWitnessMerkleRoot 是两个入口函数。在看 ComputeMerkleRoot 之前，先看下这个函数前的大篇幅注释：这篇注释，主要说明了以前实现方式存在的一个缺陷（CVE-2012-2459）及解决的方式。先说下这个缺陷，这个与 Merkle 的实现方式有有关。我们知道 Merkle 树是一层一层计算，每层两两计算 hash，这样每层都需要是偶数，那么如果数量正好是奇数，怎么办呢？代码会复制最后一个，使其成为偶数，在两两计算 hash。这个会导致一个问题。比如说，我们的 transaction 是 [1,2,3,4,5,6]，这是偶数，两两计算，得到了 [D,E,F]，这时候变成了奇数，代码会复制最后的 F 变成了 [D,E,F,F]，这样又可以计算了，最终算出了根 A。我们再看另一种情况，transaction 原始为 [1,2,3,4,5,6,5,6]，其中 [5,6] 是重复的，这种 transatcion 是特殊构建的，当然是不怀好意，如果一个节点，接受到这样的数据，按照正常的流程计算，得到 [D,E,F,F]，注意这里和刚才说的是一样的，继续计算的话，会得到跟之前一样的根 A。但是因为这样的 transaction 里面有重复，会导致重复花费，在后续的校验里不会通过，所以节点会记录这个根对应的区块是无效的，如果以后再收到原始的没有重复 transaction 的区块，由于根相同，它还是会认为是无效的，甚至后续的所有正确的区块，都无法继续接收。这就是这个缺陷的原因和影响。这个问题现在已经修正了，修正的方式在代码里：默尔克树相关就这些代码，都集中在 merkle.cpp 中。" }, { "title": "比特币源码学习-区块头验证", "url": "/posts/%E6%AF%94%E7%89%B9%E5%B8%81%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0-%E5%8C%BA%E5%9D%97%E5%A4%B4%E9%AA%8C%E8%AF%81/", "categories": "study", "tags": "", "date": "2018-09-08 00:00:00 +0800", "snippet": "最近开始看比特币的源码（版本 0.17），会逐渐更新一些文章。这篇学习下“验证区块头”的流程。从 ProcessNewBlockHeaders（validation.cpp）开始：下面是 AcceptBlockHeader （validation.cpp）函数：这里面主要有两个函数： CheckBlockHeader ，根据当前区块头提供的信息，检查这个区块头本身，即 PoW 是否有效 ContextualCheckBlockHeader，这个就不止根据当前区块的信息了，还要考察当前提供的信息是否有效，时间、checkpoint 等，很多方面，这主要是靠之前区块的信息，所以叫做上下文检查。下面就一个个看下，先是 CheckBlockHeader（validation.cpp） ：CheckProofOfWork （pow.cpp）：ContextualCheckBlockHeader（validation.cpp） ：GetNextWorkRequired（pow.cpp）：MedianTime &amp;lt; 区块的时间戳 &amp;lt;= 现在时间 + 2h，这里的 MedianTime （chain.h）是取之前 11 个区块的中间时间。checkpoint （chainparams.cpp）当前的代码中包含下面这些：CalculateNextWorkRequired（pow.cpp）：到这里区块头的检查和记录就完成了，整个流程涉及到的文件： validation.cpp pow.cpp chainparams.cpp chain.h" }, { "title": "跨链交易-BTC_relay", "url": "/posts/%E8%B7%A8%E9%93%BE%E4%BA%A4%E6%98%93-BTC_relay/", "categories": "study", "tags": "", "date": "2018-09-07 00:00:00 +0800", "snippet": "最近在公司做钱包的项目，产品已接近落地，基于 TEE 和 SE 的，主打安全性。前几天去和一家做 REE 钱包的公司谈合作，这家公司主打的是跨链交易，很感兴趣。之前也听说过跨链交易，但是了解不多，这几天就多学习了下。BTC_relay 是其中一种，在看到 BTC_relay 之前，我也在想如何实现跨链交易，我觉得有几点是必须的： 为了解决互相不信任的问题，要有公正的第三方来确认交易和完成交易。 以太坊或者比特币需要知道对方的网络的情况，来确认交易已经完成。只要可以做到上面的两点，我觉得跨链交易就可以完成。对于第一点，可以用智能合约来实现。智能合约是公开的代码，就会按照预先设定好的规则来处理。正常情况下，确认双方的打款之后，交易就可以达成。如果一方违约，另一方的资金也可以退回。第二点，一直没想明白如何去中心化实现。而 BTC_relay 采用了一种很简单的方式。BTC_relayBTC_relay 采用的是侧链的方式，简单的原理图如下：BTC_relay 在 ETH 的智能合约里，维护了当前比特币网络的所有区块头，类似于比特币的轻节点，这样就具备了验证某笔交易的基础。那么比特币的区块头是如何同步到以太坊的智能合约里的呢？这需要一群特殊的工人，relayer。他们不挖矿，他们的主要工作就是不断将比特币的区块头同步到智能合约里，然后如果有人使用到这些区块头，他们就会收到手续费（ETH）。BTC_relay 主要是为了解决，比特币用户跨链使用 ETH 的 dapp 情景（比如加密猫），但是稍加改动就可以支持钱包的跨链交易，并且这是双向的，比特币换成了以太币，以太币换成了比特币。虽然主导方在 ETH，主要花费手续费的也是 ETH 这边，但是这都是可以通过交易手续费解决掉的。项目的关键点是 relayer，这是项目的 github 主页，最后一次提交是 2017-08-26，有点遗憾。在 issue 里提到了一些原因，这些原因导致了 relayer 的减少。一些想法为了去中心化，使用 relayer 来同步 header，relayer 因为同步的 header 获取手续费，然后合约记录所有的 header 来成为一个轻节点，再通过 BTC 转账方提供的 transaction 和 merkle node 来验证交易，这个逻辑我觉得是没有问题的。如果 relayer 越来越多，交易头更新得及时，那么使用 BTC_relay 的人会越来越多，relayer 的奖励也就越多，这是一个良性循环，相反来看，如果 relayer 不多，就是一个恶心循环。获得手续费的机制和数量非常重要，还有环境，这些决定了 relayer 的数量。" }, { "title": "比特币命令行", "url": "/posts/%E6%AF%94%E7%89%B9%E5%B8%81%E5%91%BD%E4%BB%A4%E8%A1%8C/", "categories": "study", "tags": "", "date": "2018-09-03 00:00:00 +0800", "snippet": "bitcoin-cli 或者 bitcoin core wallet 的 console，支持很多命令，可以获得区块链相关的很多信息，一步步实验和解析下命令的输出。完整的命令列表getwalletinfo获取钱包相关的一些信息。{ &quot;walletname&quot;: &quot;wallet.dat&quot;, &quot;walletversion&quot;: 159900, &quot;balance&quot;: 0.00000000, &quot;unconfirmed_balance&quot;: 0.00000000, &quot;immature_balance&quot;: 0.00000000, &quot;txcount&quot;: 0, &quot;keypoololdest&quot;: 1532923740, &quot;keypoolsize&quot;: 1000, &quot;keypoolsize_hd_internal&quot;: 1000, &quot;paytxfee&quot;: 0.00000000, &quot;hdmasterkeyid&quot;: &quot;a50eea8496c072b54f4f0b2937e3edb7bbc4761d&quot;} balance: 已确认的余额，包括挖矿或者普通交易 unconfirmed_balance: 未确认的余额，确认数为0，普通交易 immature_balance: 未成熟的余额，确认数小于100，只针对挖矿 txcount: 钱包内的 transaction 数量 keypoololdest: 最先生成的 key 的时间戳 keypoolsize: 预生成的 key 数量，外部的 key keypoolsize_hd_internal: 预生成的 key 数量，内部的 key，为 hd 使用 paytxfee: 预设的支付费用 hdmasterkeyid: masterkey 的 Hash160getblockchaininfo获得区块链的信息{ &quot;chain&quot;: &quot;main&quot;, &quot;blocks&quot;: 539741, &quot;headers&quot;: 539741, &quot;bestblockhash&quot;: &quot;0000000000000000000137324acb33a45f61d442cd767420ea934816ac20622c&quot;, &quot;difficulty&quot;: 6727225469722.534, &quot;mediantime&quot;: 1535947766, &quot;verificationprogress&quot;: 0.9999988951741678, &quot;initialblockdownload&quot;: false, &quot;chainwork&quot;: &quot;000000000000000000000000000000000000000003028651fd95478395288650&quot;, &quot;size_on_disk&quot;: 207037656735, &quot;pruned&quot;: false, &quot;softforks&quot;: [ { &quot;id&quot;: &quot;bip34&quot;, &quot;version&quot;: 2, &quot;reject&quot;: { &quot;status&quot;: true } }, { &quot;id&quot;: &quot;bip66&quot;, &quot;version&quot;: 3, &quot;reject&quot;: { &quot;status&quot;: true } }, { &quot;id&quot;: &quot;bip65&quot;, &quot;version&quot;: 4, &quot;reject&quot;: { &quot;status&quot;: true } } ], &quot;bip9_softforks&quot;: { &quot;csv&quot;: { &quot;status&quot;: &quot;active&quot;, &quot;startTime&quot;: 1462060800, &quot;timeout&quot;: 1493596800, &quot;since&quot;: 419328 }, &quot;segwit&quot;: { &quot;status&quot;: &quot;active&quot;, &quot;startTime&quot;: 1479168000, &quot;timeout&quot;: 1510704000, &quot;since&quot;: 481824 } }, &quot;warnings&quot;: &quot;&quot;} blocks &amp;amp; headers: 当前的区块链高度 bestblockhash: 当前的最优块（被确认最多的块）的 hash difficulty: 当前难度值 mediantime: 最后块之前的 11 个块的中间时间，后续有效块的时间不能早于这个中间时间 verificationprogress: 验证的进程，从 0 到 1 chainwork: 按照现在的难度，重新生成目前这么多区块，一共有多少工作量。公式是\\[区块数 * 难度 * [0x01,0001,0001]\\] 来自 What is chainwork? size_on_disk: 所有区块所需的存储空间 pruned: 是否删减区块 softforks: 软分叉是为了升级区块链规则或修复 BUG，不会出现新的币种前面 3 个，BIP34、66、65，是老版本的软分叉，每次升级 version 递增。 后面 2 个，csv、segwit，采用了 BIP9 定义的新的软分叉规则，不再使用 version 递增的方式，而是引入了“版本位”、“开始时间”、“超时”、“状态”等概念。active 就代表已经成功激活。来自 BIP9 都是去中心化的思想，版本是否更新也是由大家决定的 这里有一个 95% 原则，如果一定时间内的所有区块的 95% 使用了这个软分叉的版本，那么就上线这个版本（变为 active），从这时候开始，所有不遵守新版本规则的区块，将不会被矿工们验证通过。getblockhash获取某个区块的 hash&amp;gt;&amp;gt; getblockhash 100000000000c937983704a73af28acdec37b049d214adbda81d7e2a3dd146f6ed09这里获取的是第 1000 个区块的 hashgetblock获取某个区块信息&amp;gt;&amp;gt; getblock 00000000c937983704a73af28acdec37b049d214adbda81d7e2a3dd146f6ed09￼{ &quot;hash&quot;: &quot;00000000c937983704a73af28acdec37b049d214adbda81d7e2a3dd146f6ed09&quot;, &quot;confirmations&quot;: 538753, &quot;strippedsize&quot;: 216, &quot;size&quot;: 216, &quot;weight&quot;: 864, &quot;height&quot;: 1000, &quot;version&quot;: 1, &quot;versionHex&quot;: &quot;00000001&quot;, &quot;merkleroot&quot;: &quot;fe28050b93faea61fa88c4c630f0e1f0a1c24d0082dd0e10d369e13212128f33&quot;, &quot;tx&quot;: [ &quot;fe28050b93faea61fa88c4c630f0e1f0a1c24d0082dd0e10d369e13212128f33&quot; ], &quot;time&quot;: 1232346882, &quot;mediantime&quot;: 1232344831, &quot;nonce&quot;: 2595206198, &quot;bits&quot;: &quot;1d00ffff&quot;, &quot;difficulty&quot;: 1, &quot;chainwork&quot;: &quot;000000000000000000000000000000000000000000000000000003e903e903e9&quot;, &quot;nTx&quot;: 1, &quot;previousblockhash&quot;: &quot;0000000008e647742775a230787d66fdf92c46a48c896bfbc85cdc8acc67e87d&quot;, &quot;nextblockhash&quot;: &quot;00000000a2887344f8db859e372e7e4bc26b23b9de340f725afbf2edb265b4c6&quot;}这里的信息都很熟悉，只说下面两个： weight: 重量，这个是 segwit 引入的概念，用来计算手续费 tx: 这里面是所有的 transaction 的 hash，这里只有一个，是挖矿的getrawtrasaction根据 transaticon 的 hash 来获得原始数据&amp;gt;&amp;gt; getrawtransaction fe28050b93faea61fa88c4c630f0e1f0a1c24d0082dd0e10d369e13212128f3301000000010000000000000000000000000000000000000000000000000000000000000000ffffffff0804ffff001d02fd04ffffffff0100f2052a01000000434104f5eeb2b10c944c6b9fbcfff94c35bdeecd93df977882babc7f3a2cf7f5c81d3b09a68db7f0e04f21de5d4230e75e6dbe7ad16eefe0d4325a62067dc6f369446aac00000000简单解析下，注意啊，都是小端的：01000000, version01, txin num0000000000000000000000000000000000000000000000000000000000000000, 预设值ffffffff, 预设值08, coinbase len04ffff001d02fd04, coinbaseffffffff, sequence01, txout num00f2052a01000000, 50 btc43, script len4104f5eeb2b10c944c6b9fbcfff94c35bdeecd93df977882babc7f3a2cf7f5c81d3b09a68db7f0e04f21de5d4230e75e6dbe7ad16eefe0d4325a62067dc6f369446aac00000000, locktime也可以使用下面的命令来解析decoderawtransaction&amp;gt;&amp;gt; decoderawtransaction 01000000010000000000000000000000000000000000000000000000000000000000000000ffffffff0804ffff001d02fd04ffffffff0100f2052a01000000434104f5eeb2b10c944c6b9fbcfff94c35bdeecd93df977882babc7f3a2cf7f5c81d3b09a68db7f0e04f21de5d4230e75e6dbe7ad16eefe0d4325a62067dc6f369446aac00000000{ &quot;txid&quot;: &quot;fe28050b93faea61fa88c4c630f0e1f0a1c24d0082dd0e10d369e13212128f33&quot;, &quot;hash&quot;: &quot;fe28050b93faea61fa88c4c630f0e1f0a1c24d0082dd0e10d369e13212128f33&quot;, &quot;version&quot;: 1, &quot;size&quot;: 135, &quot;vsize&quot;: 135, &quot;locktime&quot;: 0, &quot;vin&quot;: [ { &quot;coinbase&quot;: &quot;04ffff001d02fd04&quot;, &quot;sequence&quot;: 4294967295 } ], &quot;vout&quot;: [ { &quot;value&quot;: 50.00000000, &quot;n&quot;: 0, &quot;scriptPubKey&quot;: { &quot;asm&quot;: &quot;04f5eeb2b10c944c6b9fbcfff94c35bdeecd93df977882babc7f3a2cf7f5c81d3b09a68db7f0e04f21de5d4230e75e6dbe7ad16eefe0d4325a62067dc6f369446a OP_CHECKSIG&quot;, &quot;hex&quot;: &quot;4104f5eeb2b10c944c6b9fbcfff94c35bdeecd93df977882babc7f3a2cf7f5c81d3b09a68db7f0e04f21de5d4230e75e6dbe7ad16eefe0d4325a62067dc6f369446aac&quot;, &quot;reqSigs&quot;: 1, &quot;type&quot;: &quot;pubkey&quot;, &quot;addresses&quot;: [ &quot;1BW18n7MfpU35q4MTBSk8pse3XzQF8XvzT&quot; ] } } ]}还是上面那些内容，这样看起来或许直观点。dumprivkey &amp;amp; importprivkey这两个可以导出某个地址的私钥，或者将某个私钥导入钱包dumpwallet将钱包的所有私钥和地址导出，开启了 HD 的话，会导出很多其他的命令就不一一介绍了，如果以后发现了有趣的命令，再增加进来。" }, { "title": "RLP和VarInt", "url": "/posts/RLP%E5%92%8CVarInt/", "categories": "study", "tags": "", "date": "2018-08-28 00:00:00 +0800", "snippet": "在区块链的世界里，一方面由于是分布式存储，一样的数据存储在所有的节点里，如果数据中包含无用的字节，那么在所有节点中浪费的总量就很客观，另一方面虽然数字货币的交易速率都不高，但是至少也有每秒几笔（比特币）到几十笔（以太坊），如果每笔交易中都包含一定的无用字节，那么时间长了，就会浪费掉非常大的空间。所以在比特币和以太坊中都有一些为了减少浪费而做的设计。比特币 VarIntVarInt (VI) 在比特币的 transaction 里面使用很多，用来表示后面跟的数据的字节数。在之前的 比特币-交易体 有过一些介绍，为了方便阅读这里再重复下，看下面这张图：这是 transation TxIn 的一部分，里面的 VI 就是 VarInt，VI 的长度根据后面所跟数据的长度来变化，具体的定义：如果数据长度小于 0xFD，那么就只有一个字节长度，本身就代表长度。如果数据长度大于0xFD，那么第一个字节就是一个标示符，代表的是后面跟着的字节是什么类型的 Int，比如 uint16、uint32、uint64，这个 int 是数据的长度。这种设计方式，目前已经为比特币节省了 10 多 G的存储空间。以太坊 RLP（Recursive Length Prefix）以太坊没有比特币那样复杂的交易体，只是一个简单的结构体，RLP 是用来解决结构体的编码问题。注意必须是大端字节序。在 RLP 内也需要表示数据长度，所以也有类似 VarInt 的设计，先总体说下，然后看下代码。1. 概述RLP 只针对两种数据结构，字符串和列表，对于字典数据，以太坊有两种建议的方式，一种是通过二维数组表达键值对，比如[[k1,v1],[k2,v2]...]，并且对键进行字典序排序；另一种方式是通过以太坊文档中提到的高级的基数树 编码来实现。具体编码规则和例子如下：字符串长度为 1 的情况比较特殊，列表的编码实际是个递归的过程，下面会说2. 代码这段代码来自 web3j，编码过程。public class RlpEncoder { // 判断编码类别，字符串还是列表 public static byte[] encode(RlpType value) { if (value instanceof RlpString) { return encodeString((RlpString) value); } else { return encodeList((RlpList) value); } } // 编码流程，字符串和列表都是通过这个函数编码，只不过列表有一个迭代的过程 private static byte[] encode(byte[] bytesValue, int offset) { // 对应于字符串编码，且长度为 1 的情况 if (bytesValue.length == 1 &amp;amp;&amp;amp; offset == OFFSET_SHORT_STRING // OFFSET_SHORT_STRING = 0x80 &amp;amp;&amp;amp; bytesValue[0] &amp;gt;= (byte) 0x00 &amp;amp;&amp;amp; bytesValue[0] &amp;lt;= (byte) 0x7f) { // 直接返回字符串原文 return bytesValue; // 长度小于 55 的情况 } else if (bytesValue.length &amp;lt; 55) { byte[] result = new byte[bytesValue.length + 1]; // offset，字符串是0x80，列表是0xc0 result[0] = (byte) (offset + bytesValue.length); System.arraycopy(bytesValue, 0, result, 1, bytesValue.length); return result; } else { // 得到长度的长度的字节数组 byte[] encodedStringLength = toMinimalByteArray(bytesValue.length); byte[] result = new byte[bytesValue.length + encodedStringLength.length + 1]; // offset，字符串是 0x80，列表是 0xc0，加上 0x37 之后，分别为 0xb7 和 0xf7 result[0] = (byte) ((offset + 0x37) + encodedStringLength.length); System.arraycopy(encodedStringLength, 0, result, 1, encodedStringLength.length); System.arraycopy( bytesValue, 0, result, encodedStringLength.length + 1, bytesValue.length); return result; } } static byte[] encodeString(RlpString value) { return encode(value.getBytes(), OFFSET_SHORT_STRING); } // 根据长度的长度计算字节流，按照大端的顺序写入到字节数组里，选择非 0 的最小段 private static byte[] toMinimalByteArray(int value) { byte[] encoded = toByteArray(value); for (int i = 0; i &amp;lt; encoded.length; i++) { if (encoded[i] != 0) { return Arrays.copyOfRange(encoded, i, encoded.length); } } return new byte[]{ }; } // 根据数值计算出字节流，按照大端顺序写入数组 private static byte[] toByteArray(int value) { return new byte[] { (byte) ((value &amp;gt;&amp;gt; 24) &amp;amp; 0xff), (byte) ((value &amp;gt;&amp;gt; 16) &amp;amp; 0xff), (byte) ((value &amp;gt;&amp;gt; 8) &amp;amp; 0xff), (byte) (value &amp;amp; 0xff) }; } // 将列表迭代编码 static byte[] encodeList(RlpList value) { List&amp;lt;RlpType&amp;gt; values = value.getValues(); if (values.isEmpty()) { return encode(new byte[]{ }, OFFSET_SHORT_LIST); // OFFSET_SHORT_LIST = 0xc0 } else { byte[] result = new byte[0]; for (RlpType entry:values) { result = concat(result, encode(entry)); } return encode(result, OFFSET_SHORT_LIST); // OFFSET_SHORT_LIST = 0xc0 } } // 链接数组 private static byte[] concat(byte[] b1, byte[] b2) { byte[] result = Arrays.copyOf(b1, b1.length + b2.length); System.arraycopy(b2, 0, result, b1.length, b2.length); return result; }}" }, { "title": "比特币-交易体", "url": "/posts/%E6%AF%94%E7%89%B9%E5%B8%81-%E4%BA%A4%E6%98%93%E4%BD%93/", "categories": "study", "tags": "", "date": "2018-08-20 00:00:00 +0800", "snippet": "这次学习下比特币交易体，主要包括交易体的数据结构、签名的几种类型和分别是如何产生的。本人知识有限，如有错误和疏漏，请务必指正，多谢。前言交易体位于区块体内，一个区块体内包含很多笔交易，每笔交易都是一个交易体。为什么想要介绍下交易体的结构呢，有两个方面的原因： 交易体内包含签名，而签名的流程是 TEE 钱包需要保护的内容，有的客户需要我们将整个交易体的打包都放到 TA 内部来做，所以这里需要了解整个流程。 交易体的签名是有很多种类型的，类型不是指签名的算法，而是签名的范围（具体的后面会介绍），这个跟自己很感兴趣的比特币的合约密切相关，自己也打算近期写个相关的文章，所以这也是一个原因。下面的介绍中，先从详细介绍比特币 transaciton wiki 的一张图开始，并会分析下 bitcoinj 的相关部分代码。结构体就是下面这张完整的 transation 的结构图：结构体包含四个部分，版本、TxIns、TxOuts 和 锁定时间，TxIns 里可以包含多个 TxIn，TxOuts 也一样。下面就一部分一部分的分解开看。1. 版本开头的 Version 就是交易体的版本信息，4 个字节，目前是默认的 01，因为是小端顺序，所以是 01000000。2. TxIns 第一个 VI，TxIn 的总数量，这个是可变长度 Int (Var Int)，1 - 9 个字节，主要是看数量的多少 如果数量小于 0xFD，那么就只有一个字节长度，本身就是数量的值。 如果数量大于0xFD，那么第一个字节就是一个标示符，代表的是后面跟着的字节是什么类型的 Int，比如 uint16、uint32、uint64。 这个就是可变长度 Int，主要是为了缩减数据长度，VI 在整个交易体中出现很多次，如果每个 VI 不做可变长度，而是按照 uint64 的 8 字节固定长度，那么每个 VI 最多浪费了 7 个字节，一个交易体中，就算只有一个输入一个输出，也会浪费掉 3 * 7 = 21 字节，一个 block 算包含 1000 条交易体，那么一个 block 浪费掉的字节数为 21 * 1000 = 21K，现在由 50 多万个 block，大概一共会浪费掉 10 多 G。 这样的设计确实重要，比特币中还有一些。 TxOutHash，32 字节，我们知道这次花费的币是来源于上一次交易得到的，所以这里指的是上一次交易的整个交易体的 hash。如果这个交易体对应的是挖矿奖励（区块记录的第一笔交易），那么因为币是凭空产生的，这里没有来源，所以都是 0 。 TxOutIndex，4 字节，每个交易体可能会包含很多个 TxOut，其中一个是这次花费的币的来源，所以这里指的是上一次交易体中这个 TxOut 的索引。和上一个参数一起，就可以唯一确定出来，币的来源。如果是挖矿奖励，这里是 0xffffffff。 ScriptLen，可变长度（VI），代表的是后面的 script 的总长度。 Script，这里有三种类型，Sig&amp;amp;PubKey、单独的 Signature 或 Arbitrary Data（抽象的数据）。一般的交易（Standard TxIn），这里是 Sig&amp;amp;PubKey，也就是签名和公钥。如果花费的是挖矿产生的从未交易过的币（Spend Coinbase），这里是 Signature，就是只提供签名即可，这个跟后面 TxOut 的对应。如果是挖矿奖励，那么这里可以填入挖矿者自定义的一些数据。 Script 是如何生成的，后面会介绍。 Sequence，一个序号，和后面会说的 locktime 配合使用。 一般情况是 locktime 为 0， sequence 是 UINT_MAX，代表交易会被立即执行。 如果 locktime 不为 0，sequence 是 UINT_MAX， 那么交易也会被立即执行。 如果 locktime 不为 0， sequence 不是 UINT_MAX， 那么交易会等到 locktime 代表的时间或者块高度时，才会被执行，而且如果在还未执行的时候，出现 sequence 更大的相同交易，那么这次的交易会被取消掉。 3. TxOuts VI，还是代表的是 TxOuts 的数量 Value，8 个字节，代表的是发送的比特币的数量，单位是 \\(1e^{-8} BTC\\)，也就是一亿分之一比特币（ 1 聪） VI，PkScript 长度。 Script，有两种类型，Recipient Address 和 Recipient Public Key，分别对应于，Standard TxOut Script 和 Coinbase TxOut Script，代表，普通的交易和挖矿的奖励。Script 就是赎回脚本了，里面包含脚本操作符和地址或者公钥信息。4. LockTime锁定时间，这个和之前的 sequence 配合使用。LockTime 可以代表两个意思，一个是时间，一个是块数量签名签名就是指 TxIn 里的 Sig 或 Signature，签名需要两个部分，Key 和 待签的数据，Key 很好确定，就是用户的私钥，数据就有一些麻烦，到底应该签哪些内容呢？是不是整个交易体都需要签呢？交易体多个 TxIn 里的签名，要不要带到签名数据里呢？这里就要引入比特币的另一个概念，签名标示，就直接引用原文了，后面再逐个解释下The SIGHASH flags have two parts, a mode and the ANYONECANPAY modifier:=&amp;gt; 签名标示包含两个部分，模式和 ANYONECANPAYSIGHASH_ALL: This is the default. It indicates that everything about the transaction is signed, except for the input scripts. Signing the input scripts as well would obviously make it impossible to construct a transaction, so they are always blanked out. Note, though, that other properties of the input, like the connected output and sequence numbers, are signed; it&#39;s only the scripts that are not. Intuitively, it means &quot;I agree to put my money in, if everyone puts their money in and the outputs are this&quot;.=&amp;gt; SIGHASH_ALL，这是钱包默认使用的模式，代表交易体内的所有 In 和 Out 我都关心，都不能随便改变，都签上。SIGHASH_NONE: The outputs are not signed and can be anything. Use this to indicate &quot;I agree to put my money in, as long as everyone puts their money in, but I don&#39;t care what&#39;s done with the output&quot;. This mode allows others to update the transaction by changing their inputs sequence numbers.=&amp;gt; SIGHASH_NONE，代表交易体内，我只关心 In， 我只签 In，Out 我不关心，变化了我也不管。这里其他的 sequence 都被设为 0，代表不关心，别人可以更新 sequence。SIGHASH_SINGLE: Like SIGHASH_NONE, the inputs are signed, but the sequence numbers are blanked, so others can create new versions of the transaction. However, the only output that is signed is the one at the same position as the input. Use this to indicate &quot;I agree, as long as my output is what I want; I don&#39;t care about the others&quot;.=&amp;gt; SIGHASH_SINGLE，代表交易体内，我关心 In，并且我还关心位置与我的 In 对应的 Out，这些我签名，其他的 Out 我不关心，另外像　SIGHASH_NONE　一样，别人可以更新 sequence。The SIGHASH_ANYONECANPAY modifier can be combined with the above three modes. When set, only that input is signed and the other inputs can be anything.=&amp;gt;　SIGHASH_ANYONECANPAY，这个是一个附加值，可以附加在所有上述的模式上，额外表示，In 里我只关心自己的，其他的 In 我也不关心，别人都可以改。最开始看这个的时候，搞不明白，这样做有什么意义，不就是签名么，怎么还分别人和我自己，不应该都是我自己的么，我自己都签上就好了。后来看得多了，才知道，这个是为了比特币的合约准备的。在合约里，很多时候，交易体不是一个人完成的，一个人写完了自己的部分，并不直接发送到比特币网络，而是线下发给别人，别人完成自己的部分，然后传给下一个人，类似这样的流程，这就出现了上面说的，我只关心我自己的，或者我只关心 In，这种需求。大家都做好了之后，或者在某个合适的时候，这个交易体才会被真正发送到比特币网络，被矿工收录到区块内，这里后续有机会可以写一篇文章介绍下。知道了签名标示，那么就可以开始介绍签名的具体流程了，这里引用 bitcoinj 的代码，一步步介绍下。 // 这个函数创建的是一个 TxIn，TxIn 内包含签名的创建。 public TransactionInput addSignedInput(TransactionOutPoint prevOut, Script scriptPubKey, ECKey sigKey, SigHash sigHash, boolean anyoneCanPay) throws ScriptException { // Verify the API user didn&#39;t try to do operations out of order. checkState(!outputs.isEmpty(), &quot;Attempting to sign tx without outputs.&quot;); TransactionInput input = new TransactionInput(params, this, new byte[]{}, prevOut); addInput(input); //　hashForSignature，构建了待签名的数据，并求了 hash，主要看看这个函数 Sha256Hash hash = hashForSignature(inputs.size() - 1, scriptPubKey, sigHash, anyoneCanPay); // 将上面求出来的 hash　签名 ECKey.ECDSASignature ecSig = sigKey.sign(hash); TransactionSignature txSig = new TransactionSignature(ecSig, sigHash, anyoneCanPay); 　//　创建完整的脚本并加入到 TxIn 内 if (scriptPubKey.isSentToRawPubKey()) input.setScriptSig(ScriptBuilder.createInputScript(txSig)); else if (scriptPubKey.isSentToAddress()) input.setScriptSig(ScriptBuilder.createInputScript(txSig, sigKey)); else throw new ScriptException(&quot;Don&#39;t know how to sign for this kind of scriptPubKey: &quot; + scriptPubKey); return input; } //　这里构建了待签名的数据 public Sha256Hash hashForSignature(int inputIndex, byte[] connectedScript, byte sigHashType) { try { //　复制了一份 transaction，方便后面的清理等操作 Transaction tx = this.params.getDefaultSerializer().makeTransaction(this.bitcoinSerialize()); for (int i = 0; i &amp;lt; tx.inputs.size(); i++) { //　清理掉所有 TxIn 内的脚本，注意哦，　签名数据是不带已经签好的脚本的 tx.inputs.get(i).clearScriptBytes(); } 　//　这里跟比特币的一个 bug 有关，需要清除掉 OP_CODESEPARATOR connectedScript = Script.removeAllInstancesOfOp(connectedScript, ScriptOpCodes.OP_CODESEPARATOR); 　// 获取当前的 TxIn TransactionInput input = tx.inputs.get(inputIndex); // 将 TxIn 中的脚本，替换为币的赎回脚本（connectedScript） input.setScriptBytes(connectedScript); if ((sigHashType &amp;amp; 0x1f) == SigHash.NONE.value) { // 如果是 NONE 模式，因为不关心 TxOut, 所以去掉所有的 Out tx.outputs = new ArrayList&amp;lt;TransactionOutput&amp;gt;(0); for (int i = 0; i &amp;lt; tx.inputs.size(); i++) if (i != inputIndex) // 将 sequence 设为0，因为不关心 tx.inputs.get(i).setSequenceNumber(0); } else if ((sigHashType &amp;amp; 0x1f) == SigHash.SINGLE.value) { if (inputIndex &amp;gt;= tx.outputs.size()) { return Sha256Hash.wrap(&quot;0100000000000000000000000000000000000000000000000000000000000000&quot;); } // 在 SINGLE 模式下，下面这两行是将除了对应位置的 Out，其他的都清零 tx.outputs = new ArrayList&amp;lt;TransactionOutput&amp;gt;(tx.outputs.subList(0, inputIndex + 1)); for (int i = 0; i &amp;lt; inputIndex; i++) tx.outputs.set(i, new TransactionOutput(tx.params, tx, Coin.NEGATIVE_SATOSHI, new byte[] {})); // 将 sequence 设为0，因为不关心 for (int i = 0; i &amp;lt; tx.inputs.size(); i++) if (i != inputIndex) tx.inputs.get(i).setSequenceNumber(0); } // 如果有 ANYONECANPAY，那么除了当前 TxIn，其他 In 都清零 if ((sigHashType &amp;amp; SigHash.ANYONECANPAY.value) == SigHash.ANYONECANPAY.value) { tx.inputs = new ArrayList&amp;lt;TransactionInput&amp;gt;(); tx.inputs.add(input); } // 序列化，并求 hash ByteArrayOutputStream bos = new UnsafeByteArrayOutputStream(tx.length == UNKNOWN_LENGTH ? 256 : tx.length + 4); tx.bitcoinSerialize(bos); uint32ToByteStreamLE(0x000000ff &amp;amp; sigHashType, bos); Sha256Hash hash = Sha256Hash.twiceOf(bos.toByteArray()); bos.close(); return hash; } catch (IOException e) { throw new RuntimeException(e); // Cannot happen. } }结束比特币的设计确实考虑了很多东西，能稳定运行这么久，与设计者的知识面和用心程度分不开。希望自己以后也能创造出一套可以广为流传的系统。这篇介绍了交易体的结构体和签名，希望对大家有所帮助。" }, { "title": "比特币以太坊白皮书", "url": "/posts/%E6%AF%94%E7%89%B9%E5%B8%81%E4%BB%A5%E5%A4%AA%E5%9D%8A%E7%99%BD%E7%9A%AE%E4%B9%A6/", "categories": "study", "tags": "", "date": "2018-08-19 00:00:00 +0800", "snippet": "之前对比特币和以太坊已经有一些了解，这次重新再看一遍白皮书，主要记录一些以前忽略或者理解不是很深的知识。本人知识有限，如有错误和疏漏，请务必指正，多谢。比特币白皮书1. 默克尔树默克尔树除了比特币、以太坊上的应用，在其他大多数的 P2P 分布式应用中都可见身影，其核心就是将大量数据进行 hash 后增加其分布式索引性能，通过维持一个较小的高效索引进而管理复杂的大量数据（引用自（大鱼）区块链和MerkleTree）。（在 android 校验镜像上也有应用，主要是用的默克尔树的另一个优势，快速重哈希）在 P2P 的场景里（也包括比特币和以太坊等），获取整体数据一般都是从很多个节点获取，这样是为了缓解单个节点压力，也是为了提高获取速度。也是由于这个原因，从每个节点只获取整体数据中的一小块，这样就需要校验数据正确性，因为不是整体数据，所有没法知道整体数据的 hash 值（比如 md5），得想办法单独校验这一小块数据，所以就用到了默克尔树。默克尔树是将一个区块里的所有交易记录，单独 hash， 然后两两 hash 结合再次 hash，依次类推，最后得到一个 Root Hash，看下面的左边的图：如果我是接收者的话，我只需要知道 Root Hash 就可以了，然后我就可以通过 P2P 从任意节点获取任意我需要的交易记录，并且可以验证交易记录的正确性，这是怎么做到的呢？如上面的右边的图所示，比如我想获取 Tx3 这个交易记录，那么节点需要给我回传这些信息： Tx3、Hash2、Hash01。然后我自行计算 Hash3（也就是 Tx3 的 hash），然后与 Hash2 一起计算出 Hash23，然后与 Hash01 一起计算出 Root Hash，最后将这个计算出的 Root Hash 与我事先知道的 Root Hash 对比，如果一致，那么就说明传过来的 Tx3 是正确的。这里有一个问题，如果所有传递过来的 Tx3 和 Hash等都是伪造的，且让这些伪造的数据按照上面的流程，计算出跟实际的 Root Hash 一致的结果，不就可以欺骗别人了么？这样确实可以，但是这样做非常的难，因为这样相当于从 Hash 推导出可以求出这个 Hash 的原文，这是非常难的，哪怕量子计算时代到来。所以运用默克尔树，可以在轻节点（比如钱包节点）上只保留区块的头部信息，在需要验证交易记录的时候，从其他节点获取少量的过程 Hash 即可，这样使钱包的体积非常小，才使在手机上运行成为了可能（包含完整块数据的全节点，现在需要 200 多 G 存储空间）。而且区块头只有大概 80 字节，按照每十分钟生成一个区块的速度，所有区块头的体积每年只会增加 4.2 M，这些数据甚至可以全部装到内存里。以太坊白皮书1. 比特币相关 从技术角度讲，比特币账本可以被认为是一个状态转换系统，该系统包括所有现存的比特币所有权状态和“状态转换函数”。状态转换函数以当前状态和交易为输入，输出新的状态。例如，在标准的银行系统中，状态就是一个资产负债表，一个从A账户向B账户转账X美元的请求是一笔交易，状态转换函数将从A账户中减去X美元，向B账户增加X美元。如果A账户的余额小于X美元，状态转换函数就会返回错误提示。比特币系统的“状态”是所有已经被挖出的、没有花费的比特币的集合。每个UTXO都有一个面值和所有者（由20个字节的本质上是密码学公钥的地址所定义）。（引用自以太坊白皮书） 状态转换系统，我觉得这个词很贴切，因为确实是如这样，不像是普通的货币一样，本身不记名，在谁的手中，就是谁的，比特币不是这样，比特币始终位于区块链上，只不过所有者信息一直在变（可以看这篇比特币-脚本相关）。 比特币的一些应用。利用比特币 first-to-file 原则实现的域名币系统，发行自己的数字货币的彩色币系统，创建新的协议的元币系统。 比特币是可以实现一定程度的智能合约的，比特币的 wiki 上有一篇专门介绍这个，以后争取写一篇相关介绍文章。 比特币的一些限制和缺点。缺少图灵完备性，价值盲，缺少状态，区块链盲。 2. 以太坊开始 以太坊一个很重要的概念是账户，账户分两种，外部账户和合约账户，账户包含四个部分 随机数，用于确定每笔交易只能被处理一次的计数器 账户目前的以太币余额 账户的合约代码，如果有的话 账户的存储（默认为空） 以太坊的消息在某种程度上类似于比特币的交易，但是两者之间存在三点重要的不同。 以太坊的消息可以由外部实体或者合约创建，然而比特币的交易只能从外部创建。 以太坊消息可以选择包含数据。 如果以太坊消息的接受者是合约账户，可以选择进行回应，这意味着以太坊消息也包含函数概念。 幽灵协议(“Greedy Heaviest Observed Subtree” (GHOST) protocol） 如果矿工A挖出了一个区块然后矿工B碰巧在A的区块扩散至B之前挖出了另外一个区块，矿工B的区块就会作废，这样会白白浪费掉算力，且没有对网络安全做出贡献。 从另一个方面讲，如果A是一个拥有全网30%算力的矿池而B拥有10%的算力，A将面临70%的时间都在产生作废区块的风险而B在90%的时间里都在产生作废区块。因此，如果作废率高，A将简单地因为更高的算力份额而更有效率，区块产生速度快的块链很可能导致一个矿池拥有实际上能够控制挖矿过程的算力份额。 由于这两个原因，幽灵协议提出了“叔区块”的概念，以叔区块为新块确认做出贡献的废区块也会得到 87.5% 的奖励，“侄区块”会获得 12.5% 的奖励，最多下探到第五层。 以太币是持续发币的，每年有 0.26 × 发售总量的币被挖出。注意这里是发售总量，不是发行总量，发售总量是以太币最开始发售的数量。因此从长期来看，货币供应增长率是趋于零的。另外由于有丢失或者死亡等原因，币的遗失和增发最终来看会达到一个平衡。其实比特币也虽然发行总量是固定的，但是也不一定会导致通货紧缩，所以这里我觉得都可以 比特币的一个问题是越来越中心化，这个从两个方面来看，一是由于矿池的出现，大部分连接矿池的电脑没有完整的区块链数据，是依靠矿池来提供验证，所以其实是矿池控制了所有的矿机，计算力很集中，二是比特币完整区块链现在已经 200 多G，还在以每年大概 50 G的速度增长，这样会导致，对矿机的硬盘容量要求越来越高，慢慢只有大型组织才能负担的起这样庞大的硬件消耗，计算力只会越来越集中。 以太坊有这方面的措施，首先以太坊会通过挖矿算法迫使每个矿工成为一个全节点，其次全节点无需存储完整的区块链历史，只需要存储状态。 MPT不同于比特币的默克尔树，以太坊使用 MPT(Merkle-PatricaTrie) 来存储核心数据的校验数据。 以太坊一共使用了三个 MPT 对象，state Trie、tx Trie、receipt Trie，分别对应账户状态、交易状态、收据状态。 上个点说的以太坊用来解决存储数据越来越大的方法，其中一个是只存储状态，就是指的这三个状态，但是需要注意的是，这些状态只能用来校验数据是否正确，但是并不是数据本身，也就是说，如果全节点上只有这些状态，那么他还是需要从别的包含完整数据的全节点获取数据，才能进行交易的验证。这种只包含状态的全节点，应该算是比特币的全节点和轻节点的中间状态。 一些感想比特币虽然引入了区块链和比特币本身，但是核心还是比特币本身，虽然比特币也支持有限的合约，也有一些诸如染色币、域名币、元币的应用，但是这些应用其实都脱离开了比特币本身，要么需要特殊客户端，要么就是一条新链或新协议。以太坊是作为应用平台出现的，原生就可以支持应用的开发，而且功能完善，这是以太坊的很大优势。以太坊属于比特币的升级版本，并不能算是完整的创新，很多理念还是来源于比特币，虽然在很多方面有优化，比如尝试解决算力集中问题，存储空间大的问题等。不过并不能算是真正解决了这些问题，只能说是优化。我觉得现在无论是比特币还是以太坊，下面这些问题都还是核心问题，如果不能妥善解决，还是会影响到以后的发展： 算力越来越集中 存储空间越来越大 性能低下这样来看，对于联盟链来说，只有存储空间的问题，或许目前来看联盟链是个更好的发展方向。" }, { "title": "比特币-钱包备份和恢复", "url": "/posts/%E6%AF%94%E7%89%B9%E5%B8%81-%E9%92%B1%E5%8C%85%E5%A4%87%E4%BB%BD%E5%92%8C%E6%81%A2%E5%A4%8D/", "categories": "study", "tags": "", "date": "2018-08-10 00:00:00 +0800", "snippet": "这篇学习下钱包备份和恢复相关的知识。本人知识有限，如有错误和疏漏，请务必指正，多谢。前言之前 HD 钱包的文章介绍过钱包密钥的派生过程，主密钥（包括链码）是由熵、助记词、种子一步步推导出来的，而子密钥都是根据主密钥确定性派生出来的，所谓确定性就是说只要提供子密钥的路径，无论派生多少次，派生出来的密钥都是一样的。那么其实对于用户来说，只要保留好熵、助记词、种子、主密钥（包括链码）中的任何一样都可以恢复出自己的钱包。但钱包一般只会提供助记词或主密钥给客户来备份，助记词要好过主密钥，有两点： 助记词顾名思义，可以方便用户记忆，一般是 12 个单词，抄写到纸上之类的，也比较不容易出错。而主密钥（包括链码）就是 64 字节的不直观的数据而已。 助记词可以推导出所有其他的要素，比如熵、种子、主密钥，但是主密钥无法推导出其他要素，因为主密钥是单向 hash 得到的，虽然不影响推导子密钥（也就不影响使用钱包），但是永远无法知道比如助记词是什么，不方便。这里就有一个困惑很久的问题。我们知道 HD 钱包是分层的，而且是可以无限派生的，我们每个人都有很多的密钥，虽然密钥是确定性的，但是如果只提供助记词或主密钥，钱包是如何知道，我们之前一共派生并使用过多少密钥呢？毕竟得知道这些，才能算出我们的资产。另外一个问题是，钱包一般还提供了备份到文件的功能，这种之前说的有什么区别，到底备份了哪些东西？看完下面的介绍，相信大家跟我一样，对这两个问题，就会有比较清晰的解答。正文这里会粘贴一些代码，这些代码来自bitcoinj，是一个 java 实现的比特币协议代码。备份钱包到文件到底备份些什么其实备份钱包到文件，还是备份了很多东西的，这些东西能够加快恢复钱包的速度。代码在后面了，主要的备份内容就是下面这些： 网络ID 、钱包描述和版本 所有本钱包的交易记录 key 相关内容，包括：种子、助记词、所有的key、对应的路径等 一些脚本，与公钥对应，主要用来从全节点获取交易信息用。 最后一个块的 hash scrypt 参数、Signer、Tags key rotation time 信息，与比特币安全相关 /** * Converts the given wallet to the object representation of the protocol buffers. This can be modified, or * additional data fields set, before serialization takes place. */ public Protos.Wallet walletToProto(Wallet wallet) { Protos.Wallet.Builder walletBuilder = Protos.Wallet.newBuilder(); walletBuilder.setNetworkIdentifier(wallet.getNetworkParameters().getId()); // 网络id if (wallet.getDescription() != null) { walletBuilder.setDescription(wallet.getDescription()); // 钱包描述 } for (WalletTransaction wtx : wallet.getWalletTransactions()) { Protos.Transaction txProto = makeTxProto(wtx); walletBuilder.addTransaction(txProto); // 交易记录 } walletBuilder.addAllKey(wallet.serializeKeyChainGroupToProtobuf()); // 所有key相关 for (Script script : wallet.getWatchedScripts()) { Protos.Script protoScript = Protos.Script.newBuilder() .setProgram(ByteString.copyFrom(script.getProgram())) .setCreationTimestamp(script.getCreationTimeSeconds() * 1000) .build(); walletBuilder.addWatchedScript(protoScript); // 脚本 } // Populate the lastSeenBlockHash field. Sha256Hash lastSeenBlockHash = wallet.getLastBlockSeenHash(); // 最后一块区块信息 if (lastSeenBlockHash != null) { walletBuilder.setLastSeenBlockHash(hashToByteString(lastSeenBlockHash)); walletBuilder.setLastSeenBlockHeight(wallet.getLastBlockSeenHeight()); } if (wallet.getLastBlockSeenTimeSecs() &amp;gt; 0) walletBuilder.setLastSeenBlockTimeSecs(wallet.getLastBlockSeenTimeSecs()); // Populate the scrypt parameters. KeyCrypter keyCrypter = wallet.getKeyCrypter(); // scrypt 参数 if (keyCrypter == null) { // The wallet is unencrypted. walletBuilder.setEncryptionType(EncryptionType.UNENCRYPTED); } else { // The wallet is encrypted. walletBuilder.setEncryptionType(keyCrypter.getUnderstoodEncryptionType()); if (keyCrypter instanceof KeyCrypterScrypt) { KeyCrypterScrypt keyCrypterScrypt = (KeyCrypterScrypt) keyCrypter; walletBuilder.setEncryptionParameters(keyCrypterScrypt.getScryptParameters()); } else { // Some other form of encryption has been specified that we do not know how to persist. throw new RuntimeException(&quot;The wallet has encryption of type &#39;&quot; + keyCrypter.getUnderstoodEncryptionType() + &quot;&#39; but this WalletProtobufSerializer does not know how to persist this.&quot;); } } if (wallet.getKeyRotationTime() != null) { long timeSecs = wallet.getKeyRotationTime().getTime() / 1000; walletBuilder.setKeyRotationTime(timeSecs); // rotation time } populateExtensions(wallet, walletBuilder); for (Map.Entry&amp;lt;String, ByteString&amp;gt; entry : wallet.getTags().entrySet()) { Protos.Tag.Builder tag = Protos.Tag.newBuilder().setTag(entry.getKey()).setData(entry.getValue()); walletBuilder.addTags(tag); // Tags } for (TransactionSigner signer : wallet.getTransactionSigners()) { // do not serialize LocalTransactionSigner as it&#39;s being added implicitly if (signer instanceof LocalTransactionSigner) continue; Protos.TransactionSigner.Builder protoSigner = Protos.TransactionSigner.newBuilder(); protoSigner.setClassName(signer.getClass().getName()); protoSigner.setData(ByteString.copyFrom(signer.serialize())); walletBuilder.addTransactionSigners(protoSigner); // Signer } // Populate the wallet version. walletBuilder.setVersion(wallet.getVersion()); // 钱包版本 return walletBuilder.build(); }所以，就是基本备份了钱包的所有用户数据。。钱包交易越多，密钥越多，备份后的钱包就越大。一般备份钱包到文件的时候，需要用户输入一个密码，钱包数据是用这个密码加密过的，恢复钱包的时候，也需要提供这个密码。备份后的钱包文件，最好离线保存起来。这种钱包很好恢复，因为包含了所有的数据，包括所有的密钥和密钥对应的路径等，只需要原样载入数据库就可以。那么在比如只提供助记词的时候，钱包是如何恢复的？钱包如何知道密钥数量只知道助记词的情况下，钱包会先计算出来种子，然后根据种子来生成根密钥， 并按照索引递增的方式，推导出一定数量的子密钥，这个数量一般是预设的，比如 30 个子密钥。当然这些子密钥可能不足或者超过了用户所有使用过的密钥数量，这都没关系。然后钱包将这些密钥对应的地址发送给全节点来获取所有相关的交易数据，因为交易数据的输出脚本部分都会包含地址，所以可以通过对比地址来知道有哪些交易是与这些地址相关的。 为什么钱包不自己搜索所有的交易数据？因为钱包是轻节点，只保留了区块链的一部分，没有所有的块数据，只能依靠全节点来检索数据。 全节点是什么？保有一份完整的、最新的区块链拷贝的节点都叫做全节点。全节点能够独立自主地校验所有交易，而不需借由任何外部参照。常见的有 Reference Client、Full Block Chain Node、Solo Miner。 涉及到用户隐私保护，钱包发送给全节点的地址信息，是通过 bloom 处理过的，是不完整的信息，全节点会发回所有包含这些信息的交易数据，然后钱包根据完整的地址来筛选信息，得到自己相关的信息。然后全节点将这些信息发送回钱包，钱包就知道了这些地址中哪些是使用过的，因此就知道了两个信息，①对应的哪些密钥是使用过的，② 各个地址中有多少余额。如果预生成的密钥，都已经使用过，那么就说明所有的密钥可能不止这些，那么钱包会继续生成一批密钥，然后再将这些密钥对应的地址发送给全节点来获取信息，直到预生成的密钥，没有全部使用为止。通过这个过程，钱包就知道了用户所有的密钥和余额信息。参考：bitcoinj 代码实现" }, { "title": "比特币-脚本相关", "url": "/posts/%E6%AF%94%E7%89%B9%E5%B8%81-%E8%84%9A%E6%9C%AC%E7%9B%B8%E5%85%B3/", "categories": "study", "tags": "", "date": "2018-08-02 00:00:00 +0800", "snippet": "这次来学习比特币脚本相关的概念。本人知识有限，如有错误和疏漏，请务必指正，多谢。前言在介绍脚本之前，先简单说下自己对比特币所有权的理解。经常所说的比特币钱包，容易引起一种误解，觉得比特币就像是现实的硬币一样，我们从一堆硬币里，抓一把硬币放到我们自己的钱包里，这样硬币就从那堆硬币中分离开了，他们有了新的位置（在我们的钱包里），硬币本身是不记名的，在我钱包里的硬币就是我的。但是其实比特币不是这样的。比特币其实更像是房产，比如开发商开发了一片住宅，我从开发商手中买了其中的几套房子，我将所有房子都换了锁，房子属于我，我可以使用房子，但是房子的位置没有变，还是在原来的地方。我也可以将房子转卖给其他人，他们换了锁，房子属于了他们，我的钥匙再也打不开房门，但是房子还是在那个地方。或者我也可以将房子分成隔间来卖，每个隔间都有自己的门锁，卖出去的隔间就换成买家自己的锁，他们就拥有这个隔间。开发商就是矿工，他们的劳动创造了住宅，也就是比特币。我从开发商那买到了房子并换了锁，就是比特币交易到了我的名下。我拆分房子成隔间再卖给其他人，就是相当于比特币的交易和找零，买家获得了我的房子的一部分。但是在这些过程中，房子始终在原来的位置上，也就是在区块链上，只是所有权在不断变更而已，这不像是现实的硬币的概念，比特币并没有被拿走。既然比特币没有被拿走放到固定的钱包里，大家的比特币都在一个地方，那么比特币就得记名，要不就无法分辨比特币到底是谁的。。这个记名机制，其实就是后面要说的脚本实现的，未花费的比特币就是 UTXO， UTXO 都包含一段脚本，这个脚本明确了币的所有权，这些信息都记录在了区块链上，无法篡改。脚本1. 介绍先截取一段《精通比特币》中关于比特币脚本的介绍比特币交易脚本语言，也称为脚本，是一种基于逆波兰表示法的基于堆栈的执行语言。如果这让您听起来似乎在胡言乱语，很有可能是您没学习过1960年的编程语言的缘故。脚本是一种非常简单的语言，这种语言被设计为能在有限的硬件上执行，这些硬件类似简单的嵌入式设备，如手持计算器。它仅需最少的处理即可，而且不能做许多现代编程语言可以做的事情。当涉及可编程的钱时，这是它的一个基于深思熟虑的安全特性。比特币脚本语言被称为基于栈语言，因为它使用的数据结构被称为栈。栈是一个非常简单的数据结构，它可以被理解成为一堆卡片。栈允许两类操作：入栈和出栈。入栈是在栈顶部增加一个项目，出栈则是从栈顶部移除一个项目。脚本语言通过从左至右地处理每个项目的方式执行脚本。数字（常数）被推送至堆栈，操作符向堆栈推送（或移除）一个或多个参数，对它们进行处理，甚至可能会向堆栈推送一个结果。例如，OP_ADD将从堆栈移除两个项目，将二者相加，然后再将二者相加之和推送到堆栈。条件操作符评估一项条件，产生一个真或假的结果。例如，OP_EQUAL从堆栈移除两个项目，假如二者相等则推送真（表示为1），假如二者不等则推送为假（表示为0）。比特币交易脚本常含条件操作符，当一笔交易有效时，就会产生真的结果。简单来说，脚本就是一种编程语言，来实现一些逻辑。比特币的脚本是图灵非完备的，由于本身的刻意限制，只能实现一些基础功能，但是更安全。比特币脚本语言包含许多操作，但都故意限定为一种重要的方式——没有循环或者复杂流控制功能以外的其他条件的流控制。这样就保证了脚本语言的图灵非完备性，这意味着脚本的复杂性有限，交易可执行的次数也可预见。脚本并不是一种通用语言，施加的这些限制确保该语言不被用于创造无限循环或其它类型的逻辑炸弹，这样的炸弹可以植入在一笔交易中，通过引起拒绝服务的方式攻击比特币网络。受限制的语言能防止交易激活机制被人当作薄弱环节而加以利用。2. 使用前面说了脚本是为了证明币的所有权，那么现在就来看看如何证明。举个转账的例子，比如我给你转账，我知道你的地址，那么我就将比特币的所有权转给你的地址，注意，这里是地址，因为我只知道你的地址。但是其他人也知道你的地址啊，如果别人要用这笔钱怎么办？就得验证这个地址是你所有才行。这里就用到了脚本。首先在转账给你的时候，在这些比特币上我加上了锁，这个锁只能由地址的所有者才能打开，这个锁就是锁定脚本。在你使用这个地址上的比特币的时候，你需要提供你是地址所有者的证明，这个证明包含你的公钥和一段你的私钥的签名数据，这个证明就是解锁脚本。注意啊，只有在你需要使用地址上的比特币的时候，才需要提供证明。解锁脚本 + 锁定脚本，构成了一个完整的验证脚本，区块链上的矿工在打包数据的时候，验证交易的合法性的时候，就会执行这个验证脚本（执行过程可以看文尾的链接），如果验证脚本的返回真，那么交易就是合法的。这种交易脚本类型是 P2PKH (Pay-to-Public-Key-Hash)，需要先验证公钥的 hash（上图中的PubKHash，也就是比特币地址），比特币网络上的大多数交易都是这种交易。3. 另一种交易脚本 P2SH主要是用于多重签名的复杂脚本交易，将锁定脚本中的 PubKHash 替换为了 ScriptHash，主要是因为多重签名的时候，在锁定脚本中，会出现多个公钥 hash，会使锁定脚本很长，这有很多弊处。更改为 P2SH 的优点： 在交易输出中，复杂脚本由简短电子指纹取代，使得交易代码变短。 脚本能被编译为地址，支付指令的发出者和支付者的比特币钱包不需要复杂工序就可以执行P2SH。 P2SH将构建脚本的重担转移至接收方，而非发送方。 P2SH将长脚本数据存储的负担从输出方（存储于UTXO集，影响内存）转移至输入方（仅存储于区块链）。 P2SH将长脚本数据存储的重担从当前（支付时）转移至未来（花费时）。 P2SH将长脚本的交易费成本从发送方转移至接收方，接收方在使用该笔资金时必须含有赎回脚本。参考精通比特币2" }, { "title": "比特币-核心客户端编译和运行", "url": "/posts/%E6%AF%94%E7%89%B9%E5%B8%81-%E6%A0%B8%E5%BF%83%E5%AE%A2%E6%88%B7%E7%AB%AF%E7%BC%96%E8%AF%91%E5%92%8C%E8%BF%90%E8%A1%8C/", "categories": "study", "tags": "", "date": "2018-07-30 00:00:00 +0800", "snippet": "如何下载、编译和运行 bitcoin 核心客户端。这是完整节点客户端，有安装版和编译版，适合于普通用户和开发者。安装版在 bitcoin 官网 下载，包括 windows、macOS、linux、ubuntu 等版本，下载后安装即可。编译版 clone bitcoin 源码并切换版本 $ git clone https://github.com/bitcoin/bitcoin.git$ cd bitcoin &amp;amp;&amp;amp; git tag...v0.16.1v0.16.1rc1v0.16.1rc2v0.16.2v0.16.2rc1v0.16.2rc2 这里列出的是所有发型版本，rc 后缀的是预发行版本，可以用来测试。没有后缀的是稳定版本，可以直接在产品上运行。这里选择现在最新的 0.16.2 版本 $ git checkout v0.16.0 安装相关依赖 $ sudo apt-get install -y autoconf libtool pkg-config libboost-all-dev libssl-dev libevent-dev 安装 berkeleyDBberkeleyDB 是一款嵌入式数据库, bitcoin 使用它的特定版本berkeleyDB-4.8.30作为钱包数据库.  $ sudo apt-get install software-properties-common$ sudo add-apt-repository ppa:bitcoin/bitcoin$ sudo apt-get update$ sudo apt-get install libdb4.8-dev libdb4.8++-dev 编译 $ ./autogen.sh$ ./configure $ make$ sudo make install install 会将 bitcoind 和 bitcoin-cli 安装到 /usr/local/bin/ 下 运行 $ bitcoind -daemon -datadir=&amp;lt;datadir&amp;gt; 需要很长一段时间来同步所有数据，现在所有数据超过了 200G。 通过命令行使用 RPC-API 接口所有 RPC-API 接口定义可以查看官方的wiki，也可以通过 help 来查看。 当你第一次运行 bitcoin-cli 时，它会提醒你用一个安全密码给 JSON-RPC 接口创建一个配置文件 : $ bitcoin-cli helperror: Could not locate RPC credentials. No authentication cookie could be found, and RPC password is not set. See -rpcpassword and -stdinrpcpass. Configuration file: (/home/cooli7wa/.bitcoin/bitcoin.conf) 在你喜欢的编辑器中编辑配置文件并设置参数，将其中的密码替换成 bitcoind 推荐的强密码。不要使用出现在这里的密码。在 ~/.bitcoin 目录下创建一个名为 bitcoin.conf 的文件，然后输入用户名和密码: $ rpcuser=cooli7wa$ rpcpassword=2XA4DuKNCbtZXsBQRRNDEwEY2nM6M4H9Tx5dFjoAVVbK $ bitcoin-cli xxx 停止 bitcoind $ bitcoin-cli stop 安装 bitcoin-qt $ sudo apt-get install libqt5gui5 libqt5core5a libqt5dbus5 qttools5-dev qttools5-dev-tools libprotobuf-dev protobuf-compiler libqrencode-dev 重新执行一遍配置和编译: $ ./autogen.sh$ ./configure$ make 启动图形界面 $ ./src/qt/bitcoin-qt -datadir=&amp;lt;datadir&amp;gt; 同步同样需要很多时间 " }, { "title": "比特币-密钥与地址", "url": "/posts/%E6%AF%94%E7%89%B9%E5%B8%81-%E5%AF%86%E9%92%A5%E4%B8%8E%E5%9C%B0%E5%9D%80/", "categories": "study", "tags": "", "date": "2018-07-30 00:00:00 +0800", "snippet": "这篇文章一起学习下比特币密钥和地址的由来。本人知识有限，如有错误和疏漏，请务必指正，多谢。比特币秘钥比特币的签名算法是 ECDSA (Elliptic Curve Digital Signature Algorithm)， 即椭圆曲线签名算法，是数字签名算法一种，基于椭圆曲线离散对数问题的难解性。大家平时经常接触到的可能是 RSA 算法，是基于极大整数因数分解的难解性。这两种算法的秘钥生成、签名流程、签名的形式都是不同的。1. 私钥如何产生在 HD 钱包里，私钥分两种，一种是主私钥，其他的姑且都叫做其他私钥。主私钥和其他私钥的产生方式不同，具体的在 比特币-分层确定性钱包 里已经介绍，这里不再多说。私钥的 HEX 格式是 32 个字节，这是私钥的其中一种格式，还有一些其他的格式，比如： Hex-compressed、WIF、WIF-compressed，这个在后面会说。2. 公钥如何产生公钥的产生，要用到特定的椭圆曲线。比特币和以太坊使用的都是 secp256k1 （每种椭圆曲线的特定参数是不一样的），曲线是这个样子：曲线的参数 G 是固定的，是曲线上的一个点，公钥（K）是将私钥（k）与 G 点相乘得到的曲线上的另一点。\\(K = k * G\\)， 公钥 K，被定义为一个点，\\(K = (x, y)\\)生成点 G 的倍数 kG，也就是将 G 相加 k 次，在椭圆曲线中，点的相加等同于从该点画切线找到与曲线相交的另一点，然后映射到 x 轴。从私钥得到公钥很容易，但是从公钥反推到私钥，就非常得难。3. 公私钥的格式公私钥都有很多格式，最主要的是他们分为压缩和非压缩格式。先说公钥：公钥是在椭圆曲线上的一个点，由一对坐标（x，y）组成。公钥通常表示为前缀04紧接着两个256比特的数字。其中一个256比特数字是公钥的x坐标，另一个256比特数字是y坐标。前缀04是用来区分非压缩格式公钥，压缩格式公钥是以02或者03开头。比如这个公钥：x = F028892BAD7ED57D2FB57BF33081D5CFCF6F9ED3D3D7F159C2E2FFF579DC341Ay = 07CF33DA18BD734C600B96A72BBC4749D5141C90EC8AC328AE52DDFE2E505BDB非压缩格式，就是 04 + x + y：04F028892BAD7ED57D2FB57BF33081D5CFCF6F9ED3D3D7F159C2E2FFF579DC341A07CF33DA18BD734C600B96A72BBC4749D5141C90EC8AC328AE52DDFE2E505BDB因为我们知道，一个公钥是一个椭圆曲线上的点(x, y)，而椭圆曲线实际是一个数学方程，曲线上的点实际是该方程的一个解。因此，如果我们知道了公钥的 x 坐标，就可以通过解方程 y2 mod p = (x3 + 7) mod p 得到 y 坐标。这种方案可以让我们只存储公钥的 x 坐标，略去 y 坐标。而因为同一个 x 可能对应与两个 y，直观来看，就是 y 可能位于 x 轴的上面或者下面，y 就是有正负的。在素数p阶的有限域上使用二进制算术计算椭圆曲线的时候，y 坐标可能是奇数或者偶数，分别对应前面所讲的 y 值的正负符号。为了区分 y 坐标的两种可能值，我们在生成压缩格式公钥时，如果 y 是偶数，则使用 02 作为前缀；如果 y 是奇数，则使用 03 作为前缀。所以上面的例子中，因为 y 为奇数，压缩公钥就是 03 + x：03F028892BAD7ED57D2FB57BF33081D5CFCF6F9ED3D3D7F159C2E2FFF579DC341A为什么需要压缩格式的公钥？因为当交易的发生的时候，比特币所有者必须提供签名和公钥信息，这些信息会记录到区块的每笔交易内，因此使用压缩的公钥可以减少比特币交易的字节数，从而可以节省那些运行区块链数据库的节点磁盘空间。再说下私钥：压缩格式私钥，指的不是私钥本身压缩，而是说，这把私钥只能用来导出压缩格式的公钥。因为如前面所说，压缩格式的公钥和非压缩格式的公钥，长度是不同的，这会导致比特币地址不同，所以当使用压缩格式的公钥的时候，导出的私钥，就一定要标明是压缩格式的。其实压缩格式的私钥，比非压缩的私钥，还要在结尾多出一个字节（01），用来表示这个私钥是压缩格式，所以压缩格式的私钥，其实有点误导的意思。Base58Check 编码的 WIF 格式（后面会讲）的私钥，非压缩以5开头，压缩以K/L开头。同样的私钥，不同的格式：4. 比特币地址从公钥到比特币地址的生成过程，看下面这个图就很清晰了：在比特币中，大多数需要向用户展示的数据都使用 Base58Check 编码（这种编码后的格式就是 WIF，Wallet Import Format），可以实现数据压缩，易读而且有错误检验。Base58Check 编码中的版本前缀是数据的格式易于辨别，编码之后的数据头包含了明确的属性。这些属性使用户可以轻松明确被编码的数据的类型以及如何使用它们。上图中的 Base58check 的编码的详细流程，可以看下面这个图：Base58Check 版本前缀和编码后的结果，可以看下面这张表：如表，比特币地址是 1 开头，私钥是 5/K/L 开头。就这样了，有什么问题，欢迎留言。参考链接：精通比特币" }, { "title": "比特币-分层确定性钱包", "url": "/posts/%E6%AF%94%E7%89%B9%E5%B8%81-%E5%88%86%E5%B1%82%E7%A1%AE%E5%AE%9A%E6%80%A7%E9%92%B1%E5%8C%85/", "categories": "study", "tags": "", "date": "2018-07-28 00:00:00 +0800", "snippet": "这篇文章，参考《精通比特币》，加入了一些自己的理解和注释。先从钱包开始写，是因为自己正在做钱包，这里是钱包的入口。本人知识有限，如有错误和疏漏，请务必指正，多谢。什么是非确定性钱包非确定性指的是，无法从一个私钥推导出另一个私钥，也就是私钥是完全随机生成的。比特币为了保护交易隐私，要求地址不能重复使用（也就是每次交易之后，改变地址），因为每个地址对应一对公私钥，所以变更地址，就是变更公私钥对，所以钱包里就存在大量的私钥，Just a Bunch of Keys (一堆私钥)，简称 JBOK。为什么从非确定性(随机)钱包,过渡到确定性钱包每个私钥都对应着一笔资金，所有私钥都得妥善保存，但是在非确定钱包里因为私钥间是没有关联的，所以所有私钥都需要备份，而且因为私钥在不断产生，需要经常备份，要不就有可能丢失私钥。正是因为管理的麻烦，所以催生了确定性钱包，确定性钱包里的私钥是可以由主私钥一层层推导出来的，所以只需要保存好主私钥即可。分层确定性钱包的生成过程HD 钱包需要一个种子来产生主密钥，然后逐层产生子密钥，在知道种子的情况下，每层的密钥中的每一个都可以确定下来，所以叫确定性钱包。而种子的产生需要助记词，助记词的产生是根据 BIP0039 来做的（BIP0039在另一篇文章里介绍），流程可以看下图：熵是由 CSPRNG （密码学安全的伪随机数生成器）产生，就是一个随机数。需要注意的是，熵和助记词可以互相转化，但是助记词到种子是个单向的过程，种子到主私钥也是个单向过程链码是用来给这个过程引入看似的随机数据的，使得索引不能充分衍生其他的子密钥。因此，有了子密钥并不能让它发现自己的相似子密钥，除非你已经有了链码（这个导致了硬化子密钥的产生）。最初的链码种子（在密码树的根部）是用随机数据构成的，随后链码从各自的母链码中衍生出来。索引为0，就是生成第0个子密钥。分层确定性钱包的一些特性首先子私钥不能从非确定性（随机）密钥中被区分出来。因为衍生方程是单向方程，所以子密钥不能被用来发现他们的母密钥。子密钥也不能用来发现他们的相同层级的姊妹密钥。分层确定性钱包的一个很有用的特点就是可以不通过私钥而直接从公共母钥匙派生出公共子钥匙的能力。这就给了我们两种去衍生子公共钥匙的方法：或者通过子私钥，再或者就是直接通过母公共钥匙。这个方法真的非常重要，使钱包的使用非常灵活，比如可以在某个应用或者服务器上部署一个公钥，然后应用或者服务器可以自行扩展公钥来进行业务，但是因为没有私钥，服务器和应用无法使用这个地址上的比特币。但是这种灵活性也有弊端，就是在丢失任意一个子私钥的情况下，那么所有母私钥和其他子私钥都可以推导出来，看下图：深绿色的是原本就知道信息，如果额外知道了浅绿色的子私钥信息，那么就可以反向推导出来父私钥（减去“左256 bits”），那么所有其他的子私钥也就知道了。这种推导可以一直延续到主私钥！所以为了应对这种风险，HD钱包使用一种叫做 hardened derivation 的硬化衍生方案。这就“打破”了母公共钥匙以及子链码之间的关系。这个硬化衍生方程使用了母私钥去推到子链码，而不是母公共钥匙。这就在母/子顺序中创造了一道“防火墙”——有链码但并不能够用来推算子链码或者姊妹私钥。强化的衍生方程看起来几乎与一般的衍生的子私钥相同，不同的是是母私钥被用来输入散列方程中而不是母公共钥匙，如下图：这样就是牺牲掉了灵活性，保证了安全性。那就不要灵活性了么？其实可以兼顾灵活性和安全性。因为钱包是分层的，所以可以在前面层上使用硬化衍生，而在后面层上使用正常衍生，一般是在 account 的下一层 change 层开始使用正常衍生，这样就算丢失密钥，也只是丢失当前 account 的所有密钥，不会影响到其他 account。BIP0044 中指定了包含5个预定义树状层级的结构：m / purpose’ / coin_type’ / account’ / change / address_index分别是：目的地（默认44‘），货币种类（0’是比特币），账户，地址类型（0是接收地址，1是找零地址），地址这里的\\(&#39;\\)，代表是硬化衍生，所以就是从第 4 层开始使用的是正常衍生。一些例子：参考链接：精通比特币" }, { "title": "关于最近的AI和区块链学习", "url": "/posts/%E5%85%B3%E4%BA%8E%E6%9C%80%E8%BF%91%E7%9A%84AI%E5%92%8C%E5%8C%BA%E5%9D%97%E9%93%BE%E5%AD%A6%E4%B9%A0/", "categories": "essay", "tags": "", "date": "2018-07-27 00:00:00 +0800", "snippet": "之前一直在工作之余学习深度学习，大概有半年的时间了，看了很多教程（推荐 吴恩达 在 coursera 上的系列教程），也接触到了深度学习的很多领域，比如图像、语音、文本等。这些领域都有很多有意思的应用，也都很有价值，但我还是对图像最感兴趣，所以打算先深入了解下图像识别相关。前段时间研究了 YOLO 的论文，并做了一些小实验，跳一跳。识别的速度和分类的准确度都很好，但是框的准确度不是很高，这个导致了在某些很小的方块的时候，跳得不准。我觉得这与标记的准确度有关，但是这也说明了一点， YOLO 过度依赖人的标记准确程度了，其实训练集中的标记我修正过很多次，已经算是很细心了。也是一部分出于这个原因，最近打算研究下 RPN。不知道基于 region 的算法，能否解决这个问题？最近我也优化了 LabelImage 这个工具，因为之前标记跳一跳图像的时候，发现不是很好用。精确标记的时候，需要经常缩放和移动图像，原来缩放需要组合键，并且无法鼠标拖动图像，另外选择分类的时候，需要点击很多次。这些地方都进行了优化，还有一些其他的方面，可以在这里下载用用看，还在完善文档。另外公司打算做自己的数字货币钱包，基于 TEE 技术，我负责这块，所以这一个多月，工作时间大多在学习区块链相关知识，目前已经开始着手编写代码，以后会逐渐更新一些文章。就这样吧，得多吃点 DHA 了。" }, { "title": "用yolo来玩跳一跳", "url": "/posts/%E7%94%A8yolo%E6%9D%A5%E7%8E%A9%E8%B7%B3%E4%B8%80%E8%B7%B3/", "categories": "play", "tags": "", "date": "2018-07-03 00:00:00 +0800", "snippet": "前一阵子看完了 YOLO 的 1-3 论文（YOLO1-3学习总结)，也用官方提供的 weights 跑了下，效果确实不错。这段时间一直在合计自己训练个模型，最后选择了微信的跳一跳。为什么选跳一跳呢，因为它简单，用来学习模型再好不过。跳一跳里方块的种类比较少，样子也变化不大，这样标记和训练起来都可以节省很多时间。记得跳一跳刚出来的时候，同事就弄了一个辅助工具来玩，当时跳了800多分，很是惊讶，这也行！现在上网查查，很多辅助工具已经可以跳到2万分以上，我真是拜了。但是下载代码看，都不是使用的深度学习，一般是直接分析图片像素（因为跳一跳简单）或者使用 opencv，所以能借鉴的地方不多。没做之前，我给自己定的目标是1000分就行了，并不奢望能达到上万分，毕竟实现方式不同，后来实际的结果虽然确实没有做到上万分，但是已经远超1000分。下面就一步步介绍下自己的整个实践过程，最后会附上代码和训练集的下载地址。获取数据数据需要是游戏截图，但是边玩边截图，有点麻烦，所以我是先录像，然后在从录像里面截取图片。手机录像工具有很多，我使用的是“录屏精灵”，免费是标清 1280 * 720，还可以，唯一的问题是录时间长了，自己莫名奇妙就退出了，害我白录了很多次。为了提高训练的效果，录像要尽量覆盖高分和低分的场景，因为高分的时候，有些方块更小，互相之间的距离也不一样，这些场景尽量在训练集中都要出现。但是前期手打还是比较难的，我是最多只打到了100多分（见笑），所以我的训练集中，大部分的数据是集中在0-150之间的，不过这样也没关系，在训练出模型之后，模型要比你打得好多了，很容易就打上高分，出现识别有问题的情况的时候，再手动截图，增加到训练集即可。当然，你也可以下载一个别人的辅助工具来玩，然后录像。从录像中截取图片这里，我写了工具，仓库地址，tiao_tools/video_to_image.py。PRE_FRAME_KEY = 44 # &#39;,&#39;，前一帧NEXT_FRAME_KEY = 46 # &#39;.&#39;，后一帧PAUSE_KEY = 32 # &#39; &#39;，暂停SAVE_IMG_KEY = 47 # &#39;/&#39;，保存图片EXIT_KEY = 27 # &#39;esc&#39;，离开save_image_path = os.path.abspath(&#39;./&#39;) + &#39;/img/&#39; # 保存图片地址video = &#39;/home/cooli7wa/project/pycharm/tiaotiao/video/20180617123132.mp4&#39; # 录像地址设置好保存图片和录像的地址，然后运行工具就可以，到想要截图的时候，“空格” 暂停，然后 “,” 或 “.”，向前或向后微调帧，按 “/” 来保存当前图片。我是最开始选择了大概500张图片，后续陆续增加到了550多张。标记图片标记图片的工具很多，我使用的是 labelImg，支持 windows、linux 和 macOS。工具很不错，使用起来很方便，尤其是会显示当前鼠标位置的横竖轴线。使用之前，需要更改下 data/predefined_classes.txt，改成你自己的类别名，更改好的 仓库地址对于跳一跳来说，没必要给每个不同的方块都分一个类，我只分了3个类，分别为 chessman、box_score 和 box_normal，chessman 是黑色的小人，box_score 代表是可以额外得分的方块，box_normal 是普通的方块。其实后来发现，box_score 也是没什么用的，因为受限于很多方面的因素，跳的速度没法很快，在方块上等待的时间，基本都超过了额外得分需要等待的时间。所以，这里分两个类也是可以的，标记起来会更快。对于标记图片，有一些需要注意的地方： 标记方块的上表面即可。距离依据上表面的中心计算的。 小人我是标记的整个人，虽然有用的只是底面中心，但是这个比较好计算，只要将小人方框的最下边框的中心位置，上移一点，即是小人的底面中心。 标记的框，最好紧贴物体的边缘。不要画很大的一个框，离实际边缘很远，也不要画到物体内部去。 标记的框，尤其不要左右或者上下间隙不同。 对于不完整的方块，比如少露出一个角，我的一般是看露出的比例，如果超过50%的画，就要标记上。当然这时候标记的框就不是正好在四个顶点上了。这里多说一点，我最开始的时候，没有标记这种不完整的方块，因为我觉得游戏里不会出现需要跳到不完整的方块上这种情况，其实虽然不多，但是确实有，如果训练集里这种都没标注的画，模型遇到这种情况会找不到目标方块。 标记完所有的图片，最好多检测几次，不要出现漏标，或者标错的情况。 框标记的越完美，越符合统一的标准，训练的结果一般也就越好。标记完的图片大概这个样子：标记完的图片，会生成一个 xml 文件，记录的是标记的框的位置和分类信息。训练前的准备工作训练我是直接使用的官方的 darknet 框架。 将 xml 文件转换为 darknet 需要的文件，仓库地址，tiao_tools/voc_label_tiao.py这个为了适配跳一跳，相对原版有一些改动 classes = [&quot;box_normal&quot;, &quot;box_score&quot;, &quot;chessman&quot;] # 自己的分类信息prop = 1.0 # 这个是 train 和 test 的训练集的划分比例，训练模型，不需要 test 数据集，而且我们的数据比较少，所以这里我设置为了1，也就是全都划分为 train 数据。 python voc_label_tiao.py &amp;lt;image folder path&amp;gt; 运行之后，会给每个图片生成一个 txt 文件，并且生成一个 train.txt 文件，记录了所有图片的位置，后面训练会用到。 darknet 框架内有一些需要配置，更改好的仓库地址。下载后都得重新编译下，记得开启 GPU、CUDNN、OPENCV。 // 新建 cfg/tiao.data，路径替换成自己的路径classes= 3train = /home/cooli7wa/project/pycharm/tiaotiao/img/train.txtvalid = /home/cooli7wa/project/pycharm/tiaotiao/img/test.txtnames = data/tiao.namesbackup = /home/cooli7wa/project/yolo/darknet_pjreddie/backup // 复制 cfg/yolov3-voc.cfg 为 cfg/yolov3-tiao.cfg，并更改如下内容[net]# Testing# batch=1 训练时，注释掉# subdivisions=1 训练时，注释掉# Training batch=64 训练时，开启，如果提示oom（显存不够），可以改小点 subdivisions=16 训练时，开启，如果显存不够，可以改大点...learning_rate=0.001burn_in=1000#max_batches = 50200 这个是总的迭代次数，我们不用训练那么久，所以改小点max_batches = 3000policy=steps#steps=40000,45000 这个控制学习速率的衰减，也改小点steps=2000,2500scales=.1,.1...[convolutional]size=1stride=1pad=1#filters=75 filters一共有3处，根据 anchor_num*(5+class_num)计算出来，我的是24filters=24activation=linear [yolo]mask = 0,1,2anchors = 10,13, 16,30, 33,23, 30,61, 62,45, 59,119, 116,90, 156,198, 373,326#classes=20 这种classes一共有3处，我是3个类别，所以这里都改成3classes=3num=9jitter=.3ignore_thresh = .5truth_thresh = 1#random=1 这个是扩展数据集用的，对于跳一跳可以不用开，可以减少显存的使用random=0 // 新建 data/tiao.nameschressmanbox_storebox_normal // 更改 examples/darknet.c，指向 cfg/tiao.data@@ -437,7 +437,7 @@ int main(int argc, char **argv) char *filename = (argc &amp;gt; 4) ? argv[4]: 0; char *outfile = find_char_arg(argc, argv, &quot;-out&quot;, 0); int fullscreen = find_arg(argc, argv, &quot;-fullscreen&quot;);- test_detector(&quot;cfg/coco.data&quot;, argv[2], argv[3], filename, thresh, .5, outfile, fullscreen);+ test_detector(&quot;cfg/tiao.data&quot;, argv[2], argv[3], filename, thresh, .5, outfile, fullscreen); } else if (0 == strcmp(argv[1], &quot;cifar&quot;)){ run_cifar(argc, argv); } else if (0 == strcmp(argv[1], &quot;go&quot;)){ 训练在 darnet 目录，执行下面这个命令./darknet detector train cfg/tiao.data cfg/yolov3-tiao.cfg如果你是增量训练，也就是有之前训练的 weights，那么可以使用下面这个命令（weights 名字替换下）./darknet detector train cfg/tiao.data cfg/yolov3-tiao.cfg backup/yolov3-tiao.backup如果你想保留训练过程中的输出到日志文件，可以这样./darknet detector train cfg/tiao.data | tee train_log.txt这样得到的 train_log.txt 里包含训练过程中的 loss 信息，可以用来绘制 iter-loss 图，仓库地址，tiao_tools/plot_curve.py3000次的迭代，1060 和 i5-8300H 的笔记本，大概需要6个小时，最后 loss 在 0.5 左右，Avg IOU 在 80% - 90%，详细的看下图，这是最近一次训练，第3000次迭代的结果，作为参考吧。关于这个输出，有几点需要注意下： nan，训练到10000次，nan 也一直存在，但是不影响实际使用，只要 nan 不出现在类似下面这种 loss 的输出里，就没关系，训练过程还是正常的。 3000: 0.515394, 0.459542 avg, 0.000010 rate, 6.533828 seconds, 192000 images IOU，这个反应的是预测时候的框的准确度，我尝试过很多方式，这个值一直没有稳定在 90% 以上。 Class，是分类的准确度，因为我们类别很少，这个正确率很高 Obj，这个是识别出物体的概率，越高越好 No Obj，这个是没有物体的时候，识别出问题的概率，越低越好 .5R，这个是 50% IOU 的时候的召回率，越高越好 .75R，这个是 75% IOU 的时候的召回率，同样越高越好 训练中，在 backup 目录里，1000 次迭代以下时，每 100 次迭代，生成一个 weights 文件，1000 次以上时，每 100 次迭代，更新 yolov3-tiao.backup 文件。全部 3000 次训练完，会生成 yolov3-tiao_final.weights 文件。开始游戏darknet 用来训练不错，速度很快，而且显存占用小，但是是用 c 写的，跳一跳游戏逻辑相关的代码，还是 python 写起来方便，所以这里使用的是 keras-yolo3 的框架，这个是 yolov3 的 python 实现。keras-yolo3 提供工具可以将 darknet 训练出来的 weights 和 cfg 文件，转换成自己需要的 yolo.h5 文件。仓库地址 convert.py# 将 darknet 的 cfg 文件和训练出来的 weights， 复制到根目录，然后执行python convert.py yolov3-tiao.cfg yolov3-tiao_final.weights model_data/yolo.h5仓库的代码相对于原版，针对跳一跳进行了一些修改，并增加了 tiao.py，这个是主要的游戏代码。WAIT_AFTER_JUMP = 3000 # 跳后等待的时间，主要是等待游戏中下一个方块，落平稳CAPTURE_FOLDER = &#39;/home/cooli7wa/Desktop/tmp_image/&#39; # 保存临时图片的位置CAPTURE_FILE = CAPTURE_FOLDER + &#39;screen.png&#39; # 截屏的保存路径IMG_PATH = &#39;/home/cooli7wa/project/pycharm/tiaotiao/img/&#39; # 遇到无法识别的物体，自动保存图片的路径PRESS_PARAM = 1.375 # 按压时间参数，距离×此参数=按压时间CHESS_CENTER_CORRECT = 22 # 小黑人底线到底部中心的修正距离INVALID_BOX_DISTANCE = 20 # 无效的目标方块的判定范围RESTORE_NUM = 50 # 临时保存图片的数量RESTART_GAME_POS = [600, 1700] # 重新开始按键的位置，这个根据手机的分辨率设置下主要流程如下： 通过 adb 命令截屏手机，并将截图下载到本地 调用 yolo 来识别图片中的物体，找到小黑人和目标方块的中心位置 计算中心位置的距离，并转换为按压时间 通过 adb 命令按压手机屏幕来控制小人跳跃 等待下一个方块落稳，回到步骤1如果无法识别到目标方块或者小黑人或死掉了，那么会自动保存当前截图到指定目录，然后重新开始游戏。一些效果图： 关于成绩这套代码最高打到过 9000 多分，死掉的原因都是因为方块的识别位置有些偏差，从上面的图像就可以看出来，这个导致了中心距离算得不够准，在遇到距离很远，方块又很小的时候，就有可能会掉下来。我尝试过哪些方式改进： 增加迭代。曾经迭代到10000次，也没什么效果 修改 cfg 使用更密的 grid。一般更密的网格，在识别很小的物体的时候，有效果，但是在这里，没什么效果。 可以用这个工具来画上网格看看，tiao_tools/draw_grid.py，就会发现其实现在的13、26、52已经很密了。 使用更小的学习速率。这个是有效果的，尤其是到迭代后期的时候，loss 基本维持在很低的水平，一个更小的学习速率，可以使 loss 更低一些。 重新制作 anchor box。很多人推荐这个，我使用 k-means 聚类算法将所有标记的框分为了9类，然后像 yolo 的做法一样，从小到大排序分为了3组，重新训练整个模型，但是没有什么效果。 增大输入图片的尺寸。从 416×416 修改为 608*608，没什么效果。 检查图片的标记。这个最后说，是因为这个是最有用的，标记位置不准的问题，很大的原因是因为训练用例的标记没有很完美，有漏标记、错标记或者标记偏的情况。如果可以再仔细检查下所有的标记，并且耐心调整标记的位置，那么成绩应该可以再高很多。 上面说的这些，一些没什么效果，但是很可能并不是这个方法有问题，而是对于跳一跳来说没什么效果而已。这个游戏，因为图像简单，我觉得影响最大的还是训练集的标记准确度。另外提醒一点，现在这种打出来的分数，微信都给屏蔽掉了，是提交不到服务器的，就当学习或者自娱自乐吧。代码和数据地址文中出现的和这里列出的都是基于原版修改过的代码，感谢原作者。 git@github.com:cooli7wa/keras-yolo3.git git@github.com:cooli7wa/labelImg.git git@github.com:cooli7wa/darknet.git数据地址： weights、cfg、train_log：https://pan.baidu.com/s/1wl7Dmc_IUWin363z6Dqr7g 带标记的训练集：https://pan.baidu.com/s/1lgTwV6t-MWo2DutK0B28Tw最后附上一段游戏视频，祝大家开心。" }, { "title": "YOLO论文学习", "url": "/posts/YOLO%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0/", "categories": "study", "tags": "", "date": "2018-06-12 00:00:00 +0800", "snippet": "这篇文章逐步介绍YOLOv1-v3，水平有限，如有错误，欢迎指正。一些基本概念 Object Detection 目标检测，包含目标的分割和识别，不只是标记出目标在图片中的位置，还要识别出目标的类别。 R-CNN ( Region CNN ) 一种目标检测算法，一般的 R-CNN 流程如下： 将一张图像生成多个候选区域 对每个候选区域，使用深度网络提取特征 特征送入SVM 分类器，判断目标分类 对候选框进行修正，得到最终的结果 mAP 这个是作者衡量模型性能的一个主要指标。\\[Precision_c = \\frac {N(TruePositive_{c})} {N(TotalObjects_{c})}\\]\\[AveragePrecision_c = \\frac {\\sum Precision_c} {N(TotalImages_c)}\\]\\[MeanAveragePrecision = \\frac {\\sum AveragePrecision_c} {N(Classes)}\\] YOLO 的特点YOLO 是 you only look once 的缩写，YOLO 从影像输入到输出预测结果仅靠一个 CNN 网络来实现，这点与 R-CNN 很不同。正因为此，YOLO 是一个实时的目标检测算法，可以达到 45 帧/秒（Fast YOLO 可以达到 155 帧/秒），可用在视频实时检测上。优点 快速，可以实时检测。 背景误检率（background errors）低。 通用性强。YOLO 的基本思想YOLO 将输入图像分成 \\(S*S\\) 个格子，如果某个物体的中心坐标落入到某个格子中，那么这个格子就负责检测出这个物体。 这点在图像预处理上体现为，一个物体可能占据了多个格子，但是只有物体中心所在的格子才被标注为“存在物体”。YOLOv1 每个格子预测5个数值，分别为 x, y, w, h, confidence x, y: 物体的中心相对于格子边界的位置 w, h: 物体的长宽相对于整个图像的比例 confidence: 格子检测到物体的置信度，作者定义的公式为 \\(Pr(Object)*IOU^{truth}_{pred}\\)，如果不存在物体，整个值应为0，如果存在物体，那么这个值应为预测 box 和真实框的 box 的 IOU。 这里需要说明的是，这个confidence的公式，是作者的预期，在实际模型中，是没有这个公式的 另外在 YOLOv3 中，模型不再直接预测 x,y,w,h 每个格子预测只一个分类，采用下面这个公式\\[Pr(Class_i|Object)*Pr(Object)*IOU^{truth}_{pred}=Pr(Class_i)*IOU^{truth}_{pred}\\] 这点与 YOLOv3 也不同，在 v3 中，每个格子的每个 box都可以单独预测一个分类，这样可以检测出重叠的物体 因此模型一共输出 \\(S*S*(B*5+C)\\) 个数值 **在 YOLOv2 YOLOv3 中为 \\(S*S*B*(5+C)\\) ** 网络设计 整体参考GoogLeNet，24个卷积层和2个全连接层 注意这里有两个全连接层，在v2开始，取消了全连接层，使用卷积代替 训练流程 作者先取了模型中的前20个卷积层，然后在后面增加了一个平均池化层和一个全连接层，组成了一个新的网络，用1000分类的 ImageNet 来训练网络，作者在 TitanX 上训练了一个礼拜。。 然后删除掉新增的池化层和全连接层，使用原本的网络，前20层的参数保持训练后的参数，后4层和2个全连接的参数随机初始化。并且为了效果更好，将输入从224*224提升到了448*448。 这里也是正因为有全连接层，所以在更改了输入图像的大小的时候，全连接层的参数就没法使用了，也就必须删除掉 loss loss 作者有些特殊设计，作者为包含物体和不包含物体的 confidence loss 增加了权重，包含物体的权重更大，这样不包含物体的 loss 对整体 loss 的影响就小了，这很好理解，因为不含物体的格子太多了，而包含物体的格子很少，我们应该增加少的格子的权重，况且这部分格子我们更关心。 另外一些细节 作者在第一个全连接层后使用了 dropout 层，并且使用了数据扩展，为了防止过拟合。 从 YOLOv2 开始，作者使用 Batch Normalization，因为其有正则作用，所以作者去掉了 dropout 层 一些不足 因为每个格子只能预测一个分类的两个 box，所以在离得很近的物体上，效果不好。在YOLOv3中增加了 box，而且每个 box 可以有不同类别，这点有所改善，但不能说解决 泛化性能不好。这个我觉得还好 一个小的 error，在大的 box 和小的 box 上对 IOU 的影响不同，但是由于 loss 的定义方式，无法体现出来。这个导致了 YOLO 会犯定位错误的问题。这个在 YOLOv3 上，好像也没解决 剩下的是一些对比数据，YOLO 综合来看还是非常牛的，并且得益于全连接层，使 YOLO 可以看到整张图像，Background 的错误大概只有 Fast R-CNN 的 1/3。 不能理解的是，v2 开始作者就取消了全连接层，那么 YOLO 如何能解决 Background 的问题？ 后来的论文里，也没再提 Background 的错误率问题 YOLOv2在这篇论文里，作者介绍了 YOLOv2 和 YOLO9000 两个模型。这两个模型相比于 YOLOv1 都做了一些改进，YOLO9000 上还提出了一种将分类和目标检测数据集混合在一起训练的方式，可以充分利用庞大的分类数据集，让 YOLO 可以检测更多的物体。YOLOv1 相对于 R-CNN 来说，有标错位置多和召回率低的问题，所以作者主要针对这两个方面进行优化。 Batch NormalizationBN 以标准化每层的数据分布，优化深层网络的训练，并且有一定的正则效果。 作者使用 BN，获得了mAP 2%的提升，并且因此去掉了dropout 层。 High Resolution Classifier YOLOv1 的时候，在转换模型为目标检测的同时，将输入图像从 224*224 提升为 448*448，这使模型需要同时适应分类到目标的转换和像素的提升。 在 YOLOv2 和 9000 中，作者在转换之前，先进行了10epoches 的 448*448 的分类训练，然后才转到目标检测训练，这获得了 mAP 4% 的提升。 Convolutional With Anchor Boxes 作者借鉴 R-CNN 加入了 anchor boxes，anchor boxes 是 预先选择好的一系列 boxes，这个不同于 YOLOv1 中的 boxes。 去掉了全连接层，输入图像从 448 削减到 416，因为作者认为大的物体一般位于图像的中间，所以作者希望输入图像有一个中心点，即像素是奇数。 分类也是每个 box 均有了（YOLOv1 是每个格子一个分类），所以现在模型的输出为 \\(S*S*B*(5+C)\\) 。 这些改动使 mAP 小幅下降，从 69.5 下降到 69.2，但是召回率从 81% 上升到 88%。 Dimension Clusters 在选择anchor boxes的时候，作者没有手动选择，而是使用的是k-means聚类，分析train set中的所有boxes，然后进行聚类。 作者没有使用殴几里距离，因为对于大boxes会产生更多的error，因为关心的是IOU，所以作者这里使用的是如下这个公式：\\[d(box, centroid) = 1 - IOU(box, centroid)\\] 考虑到模型复杂度和高的查全率，作者选择了 5 这个点（“拐点”应该是 3，作者选择的这个点位于“拐点”附近）。 Direct location prediction 作者使用一种新的位置预测方式（YOLOv3中延续了这个方式），这种方式将x, y的取值范围限制在0和1之间，这样中心点就只能位于此格子中，这样使训练前期的稳定性提高。具体的看 YOLOv3 中的介绍。 这种处理方式，可以借鉴 Fine-Grained Features（细粒度特征） 为了提升小物体的检测效果，通过取前面层的输出，并加到现有层上，预测出的 13*13 包含了更多的细粒度特征。提升了 1% 的性能。 Multi-Scale Training 因为现在没有了全连接层，模型实际上对于输入图像的大小不敏感，为了增加鲁棒性，作者多了多尺寸的训练，尺寸范围从 320 到 608，每10个 epoches 改变一次。 这种方式虽然对输入图像不敏感，但是输出敏感，也就是实际的 y 根据输入图像大小不同，得变化。另外在 YOLOv3 中不再使用了 关于融合detection和classification数据 我觉得，这部分才是这篇论文的重点。下面说的都是 YOLO9000 中特有的技术了 During training we mix images from both detection and classification datasets. When our network sees an image labelled for detection we can backpropagate based on the full YOLOv2 loss function. When it sees a classification image we only backpropagate loss from the classification specific parts of the architecture. When it sees a classification image we only backpropagate classification loss. To do this we simply find the bounding box that predicts the highest probability for that class and we compute the loss on just its predicted tree. 这里将两个数据集的数据融合到了一起，如果取到的是分类的数据，那么就只训练模型中分类部分的参数，找到所有boxes中预测为当前分类的最高可能性，用这个来计算error，并更新网络。如果取到的是目标数据，那么就正常训练整个模型。 但是由于分类和目标的数据标签不一样，不是排他的，所以作者没有使用softmax而是使用了sigmoid，而且引入了WordTree 下面的是我关于WordTree和9000模型的理解： To compute the conditional probabilities our model predicts a vector of 1369 values and we compute the softmax over all sysnsets that are hyponyms of the same concept, see Figure 5.原来ImageNet1000模型在计算分类的时候，都是直接使用的softmax，来做1000分类预测，但是这里不是了，这里也使用了softmax，不过只是对当前子类做的，看下两图 这里wave,snow,cloud都是同一个子类的，softmax是针对这3个类别来做的。 When our network sees a detection image we backpropagate loss as normal. For classification loss, we only backpropagate loss at or above the corresponding level of the label. For example, if the label is “dog” we do assign any error to predictions further down in the tree, “German Shepherd” versus “Golden Retriever”, because we do not have that information. To do this we simply find the bounding box that predicts the highest probability for that classand we compute the loss on just its predicted tree. 这两个可以一起说。作者的算法中，对于分类图像，找到所有boxes中预测为当前分类的最高可能性的那个box，在这个box的分类计分中，只计算WordTree中标签同层和上层的loss，而不管下层的，比如标签为dog，只需要计算同层的fish,cat和上层的animal,artifact,natualobject,phenomenon，不用计算“German Shepherd”“Golden Retriever”这种。 对于目标检测图像，就按照正常的loss算法来计算。 We traverse the tree down, taking the highest confidence path at every split until we reachsome threshold and we predict that object class. Performance degrades gracefully on new or unknown object categories. For example, if the network sees a picture of a dog but is uncertain what type of dog it is, it will still predict “dog” with high confidence but have lower confidences spread out among the hyponyms.这两个一起说。 在预测类别的时候，因为模型得到的是包含树中所有节点值的一个向量，那么怎么找出可能性最大的类别呢？ 肯定不是直接计算所有节点的绝对概率，然后选最大的。因为作者计算分类的绝对概率的时候，是按照树的分支来一步步乘条件概率得到的，那么就是分支越深，绝对概率就会越小，这样的话，根节点一定是概率最大的。 所以这里肯定需要一个搜索策略，作者使用的策略是贪婪算法，每次都找最大可能性的分支，不断找下去，直到达到一个终止条件（threshold）。 这种处理方式的好处就是，比如如果图片是一个之前从没见过的狗的分类，那么可能系统找到dog这个分类，就不继续往下找了，那么就预测这个图片为dog。 The corresponding WordTree for this dataset has 9418 classes. ImageNet is a much larger dataset so we balance the dataset by oversampling COCO so that ImageNet is only larger by a factor of 4:1. 这里提到的是由于ImageNet比COCO大很多，所以当融合这两个数据集的时候，COCO的数据占比就非常小，为了平衡比例，作者对COCO数据集使用了过采样，使比例达到4:1. Using this dataset we train YOLO9000. We use the base YOLOv2 architecture but only 3 priors instead of 5 to limit the output size. 作者将boxes从5个减少到3个，这个可能是因为原来每个box预测的是20分类，现在是9000+分类，如果再使用5个box，参数就会太多。 Conversely, COCO does not have bounding box label for any type of clothing, only for person, so YOLO9000 struggles to model categories like “sunglasses” or “swimming trunks”. 这里提到的是YOLO9000，在检测没见过的衣服的效果比没见过的动物的效果要差很多，是因为COCO数据集中，关于衣服的数据很少。 ​ YOLOv3 During training we use sum of squared error loss. If the ground truth for some coordinate prediction is ^ t* our gradient is the ground truth value (computed from the ground truth box) minus our prediction: ^ t* t*. This ground truth value can be easily computed by inverting the equations above. 这里是从 YOLOv2 继承来的。 模型的输出是\\(t_x, t_y, t_w, t_h\\)，计算loss时，换算真实box的这四个参数，和模型得出数值计算均方差。 （但是这里在求bx by的时候，在sigmoid之后使用均方差会使模型非凸，容易陷入局部最小值，查看了一些别人的代码，这里有的使用的是交叉熵） \\(b_x = \\sigma(t_x) + c_x\\)这种表达式，由于\\(\\sigma\\)取值0-1，所以限制了物体中心点的预测范围（在grid内部），相比于RPN中的中心点可以位于任意位置，这样使模型训练变得简单、稳定。 Each box predicts the classes the bounding box may contain using multilabel classiﬁcation. We do not use a softmax as we have found it is unnecessary for good performance, instead we simply use independent logistic classiﬁers. During training we use binary cross-entropy loss for the classpredictions. This formulation helps when we move to more complex domains like the Open Images Dataset [7]. In this dataset there are many overlapping labels (i.e. Woman and Person). Using a softmax imposes the assumption that each box has exactly one class which is often not the case. A multilabel approach better models the data. 作者这里对于输出的“存在物体”和“物体分类”的结果，都采用的sigmoid来计算，而没有使用softmax，而且使用交叉熵作为loss函数。 这么做的原因是，作者发现一是使用softmax没发现性能提升，二是在一些复杂的数据集的时候，数据的标签不能保证是独立的，比如女人和人，是非互相独立的，而softmax总是假设标签相互独立，所以不适合。 YOLOv3 predicts boxes at 3 different scales. Our system extracts features from those scales using a similar concept to feature pyramid networks [8]. From our base feature extractor we add several convolutional layers. The last of these predicts a 3-d tensor encoding bounding box, objectness, and class predictions. In our experiments with COCO [10] we predict 3 boxes at each scale so the tensor isN*N*[3*(4+ 1+ 80)] for the 4 bounding box offsets, 1 objectness prediction, and 80 class predictions. 作者这里使用了3个scale，就是作者同时预测了13*13，26*26，52*52三种grid，不同大小的grid，适用于检测不同大小的物体，三种grid预测出的所有box集合在一起，然后通过non_max_suppression来找到最好的box。 We still use k-means clustering to determine our bounding box priors. We just sort of chose 9 clusters and 3 scales arbitrarily and then divide up the clusters evenly across scales. On the COCO dataset the 9 clusters were: (10 × 13), (16 × 30), (33 × 23), (30 × 61), (62 × 45), (59 × 119), (116 × 90), (156 × 198), (373 × 326). 这里作者针对不同的scale，准备了不同的anchor boxes。boxes的选择方式，还是采用的K-means。 先取出数据集中的一些图像的一些boxes，然后进行聚类，得到9个boxes，然后将这9个boxes，划分为3scale。 darknet53模型 这个是作者用来训练分类任务的模型，借鉴于VGG，在物体检测任务里，作者使用了分类任务的训练参数，在上图模型上，删除了最后3层，加入了一些额外的卷积层，然后在此基础上训练物体检测任务。 分类任务，由于有大量的标记数据，可以训练出能够准确分类的模型。物体检测任务在此基础上继续训练模型的检测物体的能力，只需要少量的物体检测数据，这样可以同时达到准确分类和检测物体的目的。这也是一种转移学习。 数据集中的数据，是如何转换成训练数据的？ 数据集中的数据给出的是物体的分类和物体框的左上右下角的坐标，单位是像素。 先将物体框的顶角坐标转化为中心点坐标和长宽。 然后将坐标和长宽，转化为图像总长宽的比例系数。 计算loss的时候，再将比例系数，转化为t。 论文下载 yolov1: https://arxiv.org/abs/1506.02640 yolov2&amp;amp;9000: https://arxiv.org/abs/1612.08242 yolov3: https://arxiv.org/abs/1804.02767代码下载 darknet这个是官方的框架，用 c 写的，支持 GPU 训练。 参考官网的介绍，https://pjreddie.com/darknet/yolo/ 一些感想之前看吴恩达的深度学习教程的时候，就了解了 YOLO 的基础思想和实现方式，但是当自己从头开始看论文，才发现自己很多地方其实是一知半解。模型从v1到v3的逐步优化过程，很有借鉴意义，尤其是 YOLO9000 的融合多数据集的部分，真是让人惊讶。如有问题欢迎提问，如有错误欢迎指正。" }, { "title": "一种思路解决patch失败的问题", "url": "/posts/%E4%B8%80%E7%A7%8D%E6%80%9D%E8%B7%AF%E8%A7%A3%E5%86%B3patch%E5%A4%B1%E8%B4%A5%E7%9A%84%E9%97%AE%E9%A2%98/", "categories": "tools", "tags": "", "date": "2018-05-18 00:00:00 +0800", "snippet": "经常需要在 android 的 fullsource 上合入大量代码，一般提供的代码包都是新旧代码和 patch，代码一般不是基于要合入的 android 制作的，所以就是 patch 的上下文一般有所不同，没法直接使用 patch 工具，一般大家都是使用 Beyond Compare 等工具来人工对比合入。这样做的缺点是，如果代码量很大，容易漏掉文件，或者人工疏忽合错代码。最近公司基础流程中需要加入多平台 fullsource 验证流程，由于平台很多，验证很频繁，采用原来的方式来做的话，合代码时间太长，还容易出现疏忽。所以，就在想如何将这部分工作自动化。patch的问题patch命令可以将来自于源文件的改修，同步到目标文件中。patch命令会根据patch文件的上下文信息，在目标文件中找到合适的位置，插入patch。但是如果源文件和目标文件的上下文有些差别，那么经常会遇到patch失败的情况。比如原文件和patch如下：# a.caabbcc# b.caabbddcc# patch文件diff.patch（diff -U10 a.c b.c &amp;gt; diff.patch）--- a.c 2018-05-14 16:33:23.133303425 +0800+++ b.c 2018-05-14 16:33:51.484964501 +0800@@ -1,3 +1,4 @@ aa bb+dd cc比如将a.c改成下面这样，看看还能否成功：# a.caabbcc# 运行提示&amp;gt;&amp;gt; patching file a.c&amp;gt;&amp;gt; Hunk #1 succeeded at 1 with fuzz 2.# 打完patch的a.caaddbbcc可以看到，本应该在bb后面的dd，现在跑到bb前面去了其实这种情况，在我们使用patch的时候，经常遇到，因为源文件和目标文件有些空行区别是很正常的事情再比如下面这种情况，在bb和cc后面，各加了一个空格，然后在bb前面插入了一些空行：# a.c， bb和cc后面各加了一个空格aabb cc # 运行结果patching file a.cHunk #1 succeeded at 1 with fuzz 2.# 打完patch的a.caddbb cc 可以看到，这样打得也不对这种情况也经常出现，patch同样处理不了这种情况。但是其实上述的情况都应该是可以避免的，毕竟实际代码没有什么本质的区别。一种思路既然这些空格和空行，都不是我们关心的，而且对代码逻辑也没有实际的影响，那么我们就可以想办法忽略掉他们的影响，找到真正的插入patch的位置。一种方式 先确定一个行数，作为我们需要对比的上下文，比如3，也就是我们选择上下各3行 将patch中待插入代码的上下各3个非空行，提取出来，注意哦，这里只要非空行 将这6行合并在一起，去掉换行和所有空格、TAB，做出一个字符串s_origin 从上到下扫描目标代码，对每6个非空行，进行一次第三步的操作，然后将结果和s_origin对比下，得到一个相似度百分比。 将相似度最大的6行作为待插入的区域，将待插入代码插入到这6行中的第3行后面。原理很简单吧，实际操作起来效果也不错，上面提到的patch问题，都可以解决掉。有一点没有解释，如何计算相似度百分比？如果大家很熟悉相关的算法，那么就按自己的方式处理就好了。我是不太熟悉，幸运的是，python默认库中就有这种函数，代码如下：import difflibdef compute_diff(str1, str2): return difflib.SequenceMatcher(None, str1, str2).ratio()这种处理方式，是我最开始使用的，虽然大部分效果不错，但是有时也会出现一些问题。出现问题的原因是，这种方式，是将6行数据整体来做比较的，有时前3行完全一致，但是后3行差距比较大，这时计算出来的相似度就不会很高，而有些是前3行和后3行都一些不同，但是不同的地方差距不是很大，这样计算出来的相似度就比上一种情况要高，程序默认选择了后种。那么这种选择好不好呢？从实践中来看，这种选择并不好，因为大家在自己合patch的时候，就会体会到，如果发现一个位置，前几行是完全一致的，而后几行有些区别，或者后几行完全一致，前几行有些区别，那么在这点插入代码是比前后都有些不同的位置点插入更为合适的。所以另一种方式 将patch中的非空6行，分为前后各3行 对这前后各3行，分别做出来s_origin_before和s_origin_after 从上到下扫描目标代码，对每6个非空行，分为前后各3行，分别与s_origin_before、s_origin_after进行相似度对比，得到两个相似度百分比，prop_before，prop_after 将这两个百分比转换成分数，百分比越接近100%，那么分数应该越高，这里不是线性的。 根据两个分数，计算平均值，选择平均值最大的点，插入代码。同样解释下： 为什么转换成分数的时候，不用线性的函数？因为可以这样考虑，如果线性的话，我们假设1%对应的是1分的话，那么100%（前）和30%（后），会得到130分，而80%和80%会得到160分，这不是我们希望的，我们希望的是如果有一方达到100%相似，我们就要很倾向于这个点。 所以我这里使用的函数是下面这个： math.exp(1 / (1.5 - prop)) - math.exp(1 / 1.5) 可以看到，在1（也就是100%相似）的时候，大概可以得到5这个很大的分数。 这个函数，实际使用起来效果还不错，大家也可以选择别的函数。 ​ 到目前为止，我在项目中使用的就是上面这种方式，在前一种方式出错的地方，现在这种方式都可以正确处理，效果还不错，大幅提高了patch的可用性，也为节省了大量的时间。最后提到一点，在实际使用中，为了保险起见，建议对所有不是100%匹配的地方，都进行下人工check，毕竟以防万一。到这就差不多了，主要是提供一种解决类似问题的思路，希望对大家有用。代码地址：https://github.com/cooli7wa/script_github/tree/master/mine/shell/android_helper（find_best_place.py）" }, { "title": "彩色编译", "url": "/posts/%E5%BD%A9%E8%89%B2%E7%BC%96%E8%AF%91/", "categories": "tools", "tags": "", "date": "2018-05-07 00:00:00 +0800", "snippet": "将终端的输出彩色化！经常使用ubuntu系统来编译android等，make（或mm，mmm）的时候默认的终端输出都是灰色，所以很难一眼发现错误位置，也容易忽略一些警告，所以做了这个工具来方便排错。这个工具很小巧、灵活，而且不影响正常的使用习惯。效果图代码位置及安装https://github.com/cooli7wa/script_github/tree/master/mine/shell/ccolor使用下面的命令来安装：./install.sh注意事项 适用于ubuntu。别的linux操作系统并没测试过。 工具安装，会更改.bashrc文件，在其中嵌入ccolor这个函数，并且为常用的命令创建了alias（别名），比如make命令，实际调用到的是ccolor make，这样ccolor就可以处理make的输出，将其输出为不同的颜色。 因为.bashrc重启生效，所以在install之后，需要重新启动下终端。使用说明 默认关键字和颜色，如下： 关键字 颜色 warning 黄色 error 红色 failed 红色 fail 红色 note 蓝色 警告 黄色 错误 红色 附注 蓝色 undefined reference 红色 更改keywords变量，可以自定义关键字和颜色 如果影响到某些make命令的执行效果，比如弹出菜单之类的，可以屏蔽掉 更改except变量，可以屏蔽掉命令 可以设置哪些命令调用彩色输出，默认开启彩色输出的命令如下：gcc, g++, make, mm, mmm如果想增加其他命令，比如myout，可以在最后，增加： alias myout=&quot;ccolor myout&quot; ​ " }, { "title": "gerrit REST API 使用方法", "url": "/posts/gerrit_REST_API/", "categories": "tools", "tags": "", "date": "2018-05-04 00:00:00 +0800", "snippet": "公司使用repo管理多个project，使用gerrit作为代码审核。为了方便为gerrit上的多个project创建同名分支，减少错误和遗漏，需要做一个自动化工具。查阅了一些资料，有两种方式可以做，SSH和REST APISSH就是走的很熟悉的SSH协议，使用这种方式，前提是gerrit账号有配置当前机器的rsa pubkey（gerrit网页，Settings -&amp;gt; SSH Public Keys）。使用方式很简单，比如我想在test这个project下，基于master创建一个叫abc的分支：# &#39;ssh&#39; -p &amp;lt;port&amp;gt; &amp;lt;host&amp;gt; &#39;gerrit create-branch&#39; &amp;lt;PROJECT&amp;gt; &amp;lt;NAME&amp;gt; &amp;lt;REVISION&amp;gt;ssh -p 29418 cooli7wa@mygerrit.com.cn gerrit create-branch test abc master-p 29418，是gerrit默认的ssh 端口cooli7wa，替换成你的gerrit的账号名mygerrit.com.cn，替换成你的gerrit网址如果在~/.ssh/config中配置过默认的用户名，就可以省略掉cooli7wa@，比如这样Host mygerrit.com.cn Hostname 192.168.8.248 Port 29418 User cooli7wa这样就可以了，在Branches里，可以看到刚创建的分支这种方式简单，但是也有弊端，SSH可以支持的命令有限，比如“删除分支”就没有。。全部支持的命令，可以看这里所以这个还不能满足我们的需求。REST APIREST API是走的HTTP协议，使用GET、PUT、POST、DELETE等常见的HTTP命令，比SSH支持的命令多太多了，我们最关心的删除分支，自然包括在内。既然使用的是HTTP协议，在某些操作的时候（比如PUT、DELETE），自然需要提供账号和密码，发送给gerrit。但是这里有几个容易掉的坑：1. 关于账号和密码大家在登录gerrit的时候，需要输入账号和密码，在使用REST API的时候，理所当然的认为也应该是这个，其实不然。账号是这个账号没错，但是密码不是。我们公司的服务器是gerrit+apache，可能大多数公司都是类似这样的组合，平时输入的账号和密码，其实是apache来验证的，与gerrit无直接的关系，自然不能用在gerrit的REST API上，应该使用HTTP Password，这密码在这里：如果密码是空的，就点下面的那个Generate Password按键，这个密码记录下来，以后会用2. 关于gerrit endpoint默认的REST endpoint（比如”/projects/test/branches/”），是匿名的，也就是说，如果直接通过HTTP协议访问这个endpoint，在gerrit看来，你是匿名访问的，那么权限就会有限（这个看gerrit的权限配置）。需要在前面增加”/a/”，也就是”/a/projects/test/branches/”，这样才是鉴权的endpoint，所谓鉴权，就是需要提供账号和密码，也就是上面说的。3. 关于gerrit端口这个也与gerrit+apache的组合有关，访问gerrit的网址（比如mygerrit.com.cn）时，其实不是直接访问到gerrit，而是先访问到apache，apache再转发到真实的gerrit的端口上，这时gerrit服务才能接收到信息。而REST API中使用的，应该是gerrit的真实地址，也就是网址+gerrit的端口，比如我们的gerrit实际的端口是8081，那么这个地址就应该是mygerrit.com.cn:8081（这里真是掉坑无数，我和同事都先后掉到这里，而且我也没在gerrit的官方文档中看到关于这里的提示，网上也没看到有人提示这里）4. 关于HTTPDigestAuth 和 HTTPBasicAuth最后一点需要提醒的是，HTTP协议里，账号和密码由两种鉴权方式，一种是Digest，一种是Basic，应该用哪种取决于gerrit的配置（gerrit/etc/gerrit.config）如果这里配置了，gitBasicAuth = true，那么应该使用Basic，否则就使用Digest我们公司默认是Digest好了，这几点都介绍完了，可以开始用了，下面的例子都以创建和删除分支为例，其他更多的命令可以查看官方文档。这里给出两种实现方式，一种是使用curl，一种是使用python的pygerrit库curl实现# 创建分支# revision，是基于的分支或者commit id# 注意URL中的/a/curl -u cooli7wa:WRach3vif7r8cpYn5Gc4QQUySFFBqi9u3oBlWtYTmQ --digest -X PUT http://mygerrit.com.cn:8081/a/projects/test/branches/abc -H &quot;Content-Type: application/json&quot; -d &#39;{&quot;revision&quot;:&quot;master&quot;}&#39;# 删除分支curl -u cooli7wa:WRach3vif7r8cpYn5Gc4QQUySFFBqi9u3oBlWtYTmQ --digest -X DELETE http://mygerrit.com.cn:8081/a/projects/test/branches/abcpython实现from pygerrit2.rest import GerritRestAPIfrom requests.auth import HTTPDigestAuth, HTTPBasicAuthGERRITE_USER = &quot;cooli7wa&quot;GERRITE_PWD = &quot;WRach3vif7r8cpYn5Gc4QQUySFFBqi9u3oBlWtYTmQ&quot;GERRITE_URL = &quot;http://mygerrit.com.cn:8081/&quot;GERRITE_URL_CREATE_BRANCH = &quot;/projects/test/branches/abc&quot;auth = HTTPDigestAuth(GERRITE_USER, GERRITE_PWD)rest = GerritRestAPI(url=GERRITE_URL, auth=auth)# 创建分支ret = rest.put(GERRITE_URL_CREATE_BRANCH)# 删除分支#ret = rest.delete(GERRITE_URL_CREATE_BRANCH)print(ret)" }, { "title": "Neural Networks And Deep Learning Chap4", "url": "/posts/Neural_Networks_And_Deep_Learning_Chap4/", "categories": "study", "tags": "", "date": "2018-02-28 00:00:00 +0800", "snippet": "原文地址Almost any process you can imagine can be thought of as function computation. Of course, just because we know a neural network exists that can (say) translate Chinese text into English, that doesn’t mean we have good techniques for constructing or even recognizing such a network.Two caveats First, this doesn’t mean that a network can be used to exactly compute any function.Rather, we can get an approximation that is as good as we want. By increasing the number of hidden neurons we can improve the approximation. 准确的拟合无法做到，但是通过提升中间层神经元的数量，足够精度的拟合总是可以做到 The second caveat is that the class of functions which can be approximated in the way described are the continuous functions. 因为神经网络是根据输入计算的连续函数，所以对于非连续的函数，不能很好得拟合 However, even if the function we’d really like to compute is discontinuous, it’s often the case that a continuous approximation is good enough. 即使函数不是连续的，神经网络计算出的连续的拟合，经常也是足够好的 Summing up, a more precise statement of the universality theorem is that neural networks with a single hidden layer can be used to approximate any continuous function to any desired precision.Universality with one input and one output\\[\\sigma(wx+b), \\ \\sigma(z)=1/(1+e^{z})\\]这个图形的特点是，w控制图形的“胖瘦”，w和b共同控制图形的中心点位置（s=-b/w）。如果只改变b，那么图形平移。这张图可以看出来，中间层的，每两个神经元构成了输出图像的一个矩形，这两个神经元的s（圆内数值）相差的值，就是矩形的宽度，h控制的矩形的高度，这样就可以拟合任意图形，这类似于积分。Many input variables类似于平面的情况Extension beyond sigmoid neurons作者这里介绍了另外一种样式的激活函数，图形如下：对应的输出图形为：这种激活函数，通过调节w，也可以达到step function，所以这种来拟合也没有问题。但是自己在实验的时候，没有特殊操作过w和b（比如，按照作者所言，w需要非常一个数），也可以拟合得很好：而且将第一层的W打印出来，也并不是很大的数：[[ 1.43674046e-01 -1.20247200e-01 -7.25370944e-02 1.19405329e-01 -1.38694555e-01 -4.56957966e-02 -6.28167158e-03 1.19999368e-02 -5.14546297e-02 -2.37682499e-02 1.44006923e-01 1.42787874e-01 ...]]所以作者所说的step function的情况，应该是比较好理解的一种情况，但是实际上神经网络不需要严格限制w和b，也可以通过别的方式，来拟合多项式像relu这种线性激活函数，是无法拟合多项式的，因为无法提供非线性的特征Fixing up the step functions在之前，都假设是一个完美的step function，但是实际上，不会很完美，如作者所画，这里面由failure window这种window可以通过手段减小，但是不会消失，虽然不会消失，但是对拟合的影响可以控制。这里还是上面的问题，在自己实验里，这里的step function并不存在，说明神经网络并不是一定通过step这种方式来进行的拟合。ConclusionAlthough the result isn’t directly useful in constructing networks, it’s important because it takes off the table the question of whether any particular function is computable using a neural network. The answer to that question is always “yes”. So the right question to ask is not whether any particular function is computable, but rather what’s a good way to compute the function.universality tells us that neural networks can compute any function; and empirical evidence suggests that deep networks are the networks best adapted to learn the functions useful in solving many real-world problems." }, { "title": "Neural Networks And Deep Learning Chap6", "url": "/posts/Neural_Networks_And_Deep_Learning_Chap6/", "categories": "study", "tags": "", "date": "2018-02-25 00:00:00 +0800", "snippet": "原文地址Introducing convolutional networks Local receptive fields局部采样 shared weights分享权重To see why this makes sense, suppose the weights and bias are such that the hidden neuron can pick out, say, a vertical edge in a particular local receptive field. That ability is also likely to be useful at other places in the image. And so it is useful to apply the same feature detector everywhere in the image.分享权重为什么会有用？因为一般来说一套w和b，对应于获取图像的某一种特征，那么这种特征应该是全图像都需要的，所以对于图像的各个局部，都需要一样的权重。The network structure I’ve described so far can detect just a single kind of localized feature. To do image recognition we’ll need more than one feature map. And so a complete convolutional layer consists of several different feature maps一个正常的卷积层应该包含多个feature，也就是多组参数。A big advantage of sharing weights and biases is that it greatly reduces the number of parameters involved in a convolutional network.下面比较的就是卷积和全连接的参数数量，卷积可以有效缩减参数数量：If we have 20 feature maps that’s a total of 20×26=520 parameters defining the convolutional layer.这是卷积的参数数量That’s a total of 784×30 weights, plus an extra 30 biases, for a total of 23,550 parameters.这是全连接的参数数量对比520和23550，全连接的参数数量大概是卷积的40倍。 pooling max-pooling L2 pooling Convolutional neural networks in practiceThat’s a satisfying point of view, but gives rise to a second question. The output from the previous layer involves 20 separate feature maps, and so there are 20×12×12 inputs to the second convolutional-pooling layer. It’s as though we’ve got 20 separate images input to the convolutional-pooling layer, not a single image, as was the case for the first convolutional-pooling layer. How should neurons in the second convolutional-pooling layer respond to these multiple input images? In fact, we’ll allow each neuron in this layer to learn from all 20×5×5 input neurons in its local receptive field. More informally: the feature detectors in the second convolutional-pooling layer have access to all the features from the previous layer, but only within their particular local receptive field这里每个神经元，要连接所有input的20个feature的5*5区域However, across all my experiments I found that networks based on rectified linear units consistently outperformed networks based on sigmoid activation functionsrelu要全面好过sigmoid这些一步步提高了测试成绩 增加额外的卷积和池化层 改用relu激活函数 扩展数据集 增加全连接层神经元数量，或者增加一个全连接层（效果不明显） 使用dropout 使用多个网络，共同决定分类（作者的意思是，这种方式其实是一种阻碍，且效果不明显）Why we only applied dropout to the fully-connected layersthe convolutional layers have considerable inbuilt resistance to overfitting. The reason is that the shared weights mean that convolutional filters are forced to learn from across the entire image. This makes them less likely to pick up on local idiosyncracies in the training data. And so there is less need to apply other regularizers, such as dropout.卷积已经有正则的效果了，因为卷积由于权值共享，实际上学习的是整个图像，这样过拟合的现象就比较不容易发生。Why are we able to train?上章的最后的问题是，如何避免梯度消失和爆炸这里的答案是，我们现在也没解决，只是多做了一些优化(1) Using convolutional layers greatly reduces the number of parameters in those layers, making the learning problem much easier;(2) Using more powerful regularization techniques (notably dropout and convolutional layers) to reduce overfitting, which is otherwise more of a problem in more complex networks;(3) Using rectified linear units instead of sigmoid neurons, to speed up training - empirically, often by a factor of 3-5;(4) Using GPUs and being willing to train for a long period of time. making use of sufficiently large data sets (to help avoid overfitting); using the right cost function (to avoid a learning slowdown); using good weight initializations (also to avoid a learning slowdown, due to neuron saturation); algorithmically expanding the training data.Recent progress in image recognition增加一些扰动，可能就无法识别，比如：The existence of the adversarial negatives appears to be in contradiction with the network’s ability to achieve high generalization performance. Indeed, if the network can generalize well, how can it be confused by these adversarial negatives, which are indistinguishable from the regular examples? The explanation is that the set of adversarial negatives is of extremely low probability, and thus is never (or rarely) observed in the test set, yet it is dense (much like the rational numbers), and so it is found near virtually every test case.这种扰动为什么正则没有用处？实际上，这种扰动在测试和学习集中，出现的几率非常低，所以没有学习到，而常规的正则解决不了这样问题，这种或许智能通过人为扩展数据集才行。Other approaches to deep neural nets Recurrent neural networksIndeed, a neuron’s activation might be determined in part by its own activation at an earlier time.Or perhaps the activations of hidden and output neurons won’t be determined just by the current input to the network, but also by earlier inputs.神经元本身会受到之前的激发状态的影响或者受到之前的输入的影响they’re particularly useful in analysing data or processes that change over time. Such data and processes arise naturally in problems such as speech or natural language, for example.用来识别有时间流逝的数据或者进程，比如谈话或自然语言。 Long short-term memory units (LSTMs)One challenge affecting RNNs is that early models turned out to be very difficult to train, harder even than deep feedforward networks. The reason is the unstable gradient problem discussed in Chapter 5. Recall that the usual manifestation of this problem is that the gradient gets smaller and smaller as it is propagated back through layers. This makes learning in early layers extremely slow. The problem actually gets worse in RNNs, since gradients aren’t just propagated backward through layers, they’re propagated backward through time.在RNN内，梯度不稳定的问题，更加明显，因为梯度不只是通过层来传播，还有时间。LSTM的加入会对这个有所帮助 Deep belief nets, generative models, and Boltzmann machinesDBNIn this, a generative model is much like the human brain: not only can it read digits, it can also write themA second reason DBNs are interesting is that they can do unsupervised and semi-supervised learning. reinforcement learning加强学习，比如学习如何打游戏，下面是作者推荐的两篇文章这个我以后可能会用到http://www.cs.toronto.edu/~vmnih/docs/dqn.pdfhttps://www.nature.com/articles/nature14236On the future of neural networks Intention-driven user interfaces Machine learning, data science, and the virtuous circle of innovation The role of neural networks and deep learningWill neural networks and deep learning soon lead to artificial intelligence?it’s too early to say" }, { "title": "Neural Networks And Deep Learning Chap5", "url": "/posts/Neural_Networks_And_Deep_Learning_Chap5/", "categories": "study", "tags": "", "date": "2018-02-25 00:00:00 +0800", "snippet": "原文地址Why are deep neural networks hard to train?前一章有介绍，任意函数都可以通过2层神经网络来求解，但是并不是说任何问题都用两层就好了。因为“可以做到”并不一定是“最好的”。作者的观点是，当使用2层神经网络来解决问题的时候，需要的神经元的数量可能是多层网络的指数倍，设计的困难程度也比多层复杂得多。Deep circuits thus can be intrinsically much more powerful than shallow circuits.如果我们都用深层网络呢？做之前的手写识别（之前是2层），会不会提升很多？作者实验的是不会，甚至没有提升。原因是前层和后层，总有在stick的，没有真正在学习，学习速率差距很大。后面作者会阐述问题原因（本章）和解决办法（下一章）。The vanishing gradient problem vanishing gradient problem当增加层数（每层都是30神经元）的时候，准确率有时还会下降这里有问题，因为按照常理来说，哪怕新增的层，没有什么可以学习的了，那么也就是什么都不做，也至少应该和原来的准确率一致，但是现象确实有时会下降。作者使用\\(\\frac{\\partial C}{\\partial b}\\)来指示梯度情况，这个实际上是\\(\\delta\\)，因为实际上w的偏导，也与\\(\\delta\\)有关，不过是多了一个a，所以这个来衡量梯度情况是可行的。可以看出来，越前面的层，梯度越小，这个就是梯度消失问题首先需要确定是这个梯度消失问题，是不是个问题？因为从导数来看，导数越小，说明越接近正确值，这样不是更好？其实不是，因为这里面的图片都是刚开始训练时候的数据，而最开始的参数都是随机取的，而随机取的参数基本不会是合适的参数，所以前面的层来说，不合适的参数会导致，图像得很多特征没有被正确获取到，如果梯度很小，训练很慢得话，那么肯定是有问题的。 exploding gradient problem这种是梯度爆炸，就是与上面的对立的情况，前面层的梯度特别大，后面的反而很小作者暂时没有细说What’s causing the vanishing gradient problem? Unstable gradients in deep neural nets由于我们使用的是标准正态分布的方式初始化变量，所以一般情况下（68.2%）, |w|&amp;lt;1，σ′(z)&amp;lt;1/4，所以|wjσ′(zj)|&amp;lt;1/4所以这就是梯度消失的原因。如果这里的不是&amp;lt;1/4，而是远大于1，那么就是梯度爆炸了这两都是梯度不稳定，The unstable gradient problem，所有层的学习速率有着很大的不同。这里是用的一种很简答的模型（只有一个神经元每层），对于正常的模型，也是类似的情况。要知道一点，这个问题，作者是用sigmoid做例子，但是不只是sigmoid有这种问题sigmoid是比较倾向于出现梯度消失的问题，而其他的激活函数，可能有别的问题Other obstacles to deep learning除了梯度问题，还有其他的问题，其中一个是关于sigmoid的In particular, they found evidence that the use of sigmoids will cause the activations in the final hidden layer to saturate near 0 early in training还有其他的，不详细记录了" }, { "title": "Neural Networks And Deep Learning Chap3", "url": "/posts/Neural_Networks_And_Deep_Learning_Chap3/", "categories": "study", "tags": "", "date": "2018-02-24 00:00:00 +0800", "snippet": "原文地址 a better choice of cost function, known as the cross-entropy cost function four so-called “regularization” methods (L1 and L2 regularization, dropout, and artificial expansion of the training data)，L1 L2正则化、dropout、虚假扩展数据 a better method for initializing the weights in the network a set of heuristics to help choose good hyper-parameters for the networkThe philosophy is that the best entree to the plethora of available techniques is in-depth study of a few of the most important.对现在过多的技术，最好的方式了，深入研究几个最重要的技术The cross-entropy cost functionloss函数是均方差，激活函数是sigmoid的时候\\[\\frac{\\partial C}{\\partial w}=(a-y)\\sigma&#39;(z)x=a\\sigma&#39;(z)\\]\\[\\frac{\\partial C}{\\partial b}=(a-y)\\sigma&#39;(z)=a\\sigma&#39;(z)\\]当output接近0或1的时候，函数很平坦，梯度就很小，学习很慢Introducing the cross-entropy cost function\\[c=-\\frac{1}{n}\\sum_{x}[ylna+(1-y)ln(1-a)]\\]什么样的交叉熵可以做为损失函数？ 函数结果是非负的 当实际输出结果和期待的结果接近的时候，函数输出应该接近0（假设a=y=0或a=y=1）从上面这两点看，交叉熵是合适的交叉熵为什么可以防止训练速度慢？\\[\\frac{\\partial C}{\\partial w_{j}}=\\frac{1}{n}\\sum_{x}x_{j}(\\sigma(z)-y)\\]\\[\\frac{\\partial C}{\\partial b}=\\frac{1}{n}\\sum_{x}(\\sigma(z)-y)\\]\\(\\sigma(z)-y\\)就是error，所以error越大，学习速率就越大，这个符合人学习的特点。另外交叉熵的cost/epoch曲线，更加陡峭什么时候使用交叉熵？如果激活函数是sigmoid的话，那么损失函数就应该是交叉熵，因为可以放置训练过慢的问题。对于其他的激活函数，并没说。\\[C=-\\frac{1}{n}\\sum_{x}\\sum_{j}[y_{j}lna^{L}_{j}+(1-y_{j})ln(1-a^{L}_{j})]\\]这个公式和别的地方的函数里用的不一样，这里是两项相加，都当成了概率均方差并不是总是会造成学习速率慢的问题，当最后一层的神经元是线性的（也就是没有sigmoid的激活函数），这时的偏导数就不会有这个问题。所以来看，学习速率慢的问题，貌似只在sigmoid配合均方差的时候出现。所以均方差也不是用在哪里都不合适。Using the cross-entropy to classify MNIST digitscross-entropy对比quadratic cost原文784, 30, 1030, 10，0.5 95.42 -&amp;gt; 95.49784, 100, 1030, 10，0.5 96.59 -&amp;gt; 96.82error: 3.41 -&amp;gt; 3.18，下降了1/12，下降很多了自己784, 30, 1030, 10，0.5 95.19 -&amp;gt; 95.36784, 100, 1030, 10，0.5 96.51 -&amp;gt; 96.63需要注意的是，这里没有仔细调参，所以通过这个结果直接说，cross-entropy比quadratic要好，不严谨。不过作者也说，实际上确实要好为什么关注损失函数？the more important reason is that neuron saturation is an important problem in neural nets主要原因是，神经元的饱和度是一个特别重要的问题，这里值得花力气来研究，这个也应该是深度学习的主要方面。What does the cross-entropy mean? Where does it come from?\\[\\sigma&#39;(z)=\\sigma(z)(1-\\sigma(z))\\]这个是从\\(\\sigma(z)=\\frac{1}{1+e^{-z}}\\)求导而来\\[\\frac{\\partial C}{\\partial w_{j}}=\\frac{1}{n}\\sum_{x}x_{j}(\\sigma(z)-y)\\]从上面的公式可以看出来，x也是影响学习速率的一个主要因素，当x接近0的时候，速率也很慢Softmaxsoftmax可以用来解决学习速率慢的问题？这里的softmax并不是我理解的那样，只是一个归一化的处理过程。文章这里的softmax是作为一个激活函数来用的，类似于sigmoid。当cost为\\(c\\equiv -lna^{L}_{y}\\)（这个叫做log-likelihood cost）时，可以推到出来下面的公式：\\[\\frac{\\partial C}{\\partial b^{L}_{j}}=a^{L}_{j}-y_{j}\\]\\[\\frac{\\partial C}{\\partial w^{L}_{jk}}=a^{L-1}_{k}(a^{L}_{j}-y_{j})\\]从这里可以看出来，这个与sigmoid+crossentropy是一样的所以说这个也解决了速率慢的问题。w的证明过程如下：\\[\\frac{\\partial C}{\\partial z^{l}_{j}}=\\frac{\\partial C}{\\partial a}\\frac{\\partial a}{\\partial z^{l}_{j}}=-\\frac{1}{a^{l}_{j}}\\frac{e^{z^{j}_{l}}\\sum-(e^{z^{j}_{l}})^{2}}{\\sum^{2}}=..=\\frac{e^{z^{j}_{l}}-\\sum}{\\sum}=a^{l}_{j}-1=a^{l}_{j}-y_{j}\\]log-likelihood cost，可以作为损失函数，因为还是那么几点 非负的，当\\(a^{L}_{y}\\)为1的时候，cost就是0，也就是第y个神经元的输出与期待1相符，没有偏差 当实际输出结果和期待的结果接近的时候，函数输出应该接近0 当不接近的时候，输出应该远离0 注意\\(c\\equiv -lna^{L}_{y}\\)这里的y，是指输出层第几个神经元的输出，比如对于mnist，如果这里要计算图像与7的偏差，那么这里的y就是7The fact that a softmax layer outputs a probability distribution is rather pleasing. In many problems it’s convenient to be able to interpret the output activation \\(a^{L}_{j}\\) as the network’s estimate of the probability that the correct output is j.softmax的输出可以理解为是一个概率分布，是\\(a^{L}_{j}\\)是j的概率。a network with a sigmoid output layer, the output activations \\(a^{L}_{j}\\) won’t always sum to 1.sigmoid的输出，并不是相加总为1You can think of softmax as a way of rescaling the \\(z^{L}_{j}\\), and then squishing them together to form a probability distribution.sigmoid使z成为一个概率分布现在知道了两种组合： sigmoid + crossentropy softmax + log-likelihoodOverfitting and regularization这里的测试，是按照下面的比例来做的，train_data只有1000，因为数据少了，epoch设置为了400，其他没变&amp;gt;&amp;gt;&amp;gt; import mnist_loader &amp;gt;&amp;gt;&amp;gt; training_data, validation_data, test_data = \\... mnist_loader.load_data_wrapper()&amp;gt;&amp;gt;&amp;gt; import network2 &amp;gt;&amp;gt;&amp;gt; net = network2.Network([784, 30, 10], cost=network2.CrossEntropyCost) &amp;gt;&amp;gt;&amp;gt; net.large_weight_initializer()&amp;gt;&amp;gt;&amp;gt; net.SGD(training_data[:1000], 400, 10, 0.5, evaluation_data=test_data,... monitor_evaluation_accuracy=True, monitor_training_cost=True)这里有个问题 从accuracy来看，overfitting是从280epoch开始的 从cost on test data来看，overfitting是从15epoch开始的那么哪个才是真正的overfitting起始点？From a practical point of view, what we really care about is improving classification accuracy on the test data, while the cost on the test data is no more than a proxy for classification accuracy. And so it makes most sense to regard epoch 280 as the point beyond which overfitting is dominating learning in our neural network.文中的观点是，280才是，但是这个观点有点草率，后面作者也会说这里正确率是100%，那就是说，网络记住了整个训练数据，而不是理解避免overfitting的方式： 时刻关注在测试集（验证集）上的正确率，如果正确率停止增长，那么就停止训练。但是严格来说，这个并不能真正的识别overfitting，因为有时是训练集和测试集同时停止了增长。但这个应该可以防止overfitting的发生。（这里作者也在说，判断何时算是overfitting，这个应该谨慎，因为训练过程中，有时就会发生一段时间内正确率不上升的情况，这种类似平坦的地形，但是过后，就又会开始上升。） 增大数据量 正则化（后面会说）这里关于数据集的划分方面，是这样定的 train validation，在这个数据集上来测试调整过的超参，并选择其他的超参 test，在这个数据集上来最终测试网络，这个算是最终的测试集Regularization减少网络的尺寸，也是一种减少过拟合的方式，但是这个我们一般不会采用，但是这是一种思路，要知道。weight decay or L2 regularization，是一回事交叉熵的L2：\\[C=-\\frac{1}{n}\\sum_{xj}[y_{j}lna^{L}_{j}+(1-y_{j})ln(1-a^{L}_{j})]+\\frac{\\lambda}{2n}\\sum_{w}w^{2}\\]\\(\\frac{\\lambda}{2n}\\)，\\(\\lambda&amp;gt;0\\)，叫做正则参数，n是样本的数量，后面会讨论如何选择正则参数注意这里只有w，没有b，原因如下： Empirically, doing this often doesn’t change the results very much At the same time, allowing large biases gives our networks more flexibility in behaviour in particular, large biases make it easier for neurons to saturate, which is sometimes desirable，饱和是我们希望的？均方差的L2：\\[C=\\frac{1}{2n}\\sum_{x}\\left \\|y-a^{L}\\right \\|^{2}+\\frac{\\lambda}{2n}\\sum_{w}w^{2}\\]通用形式的L2：\\[C=C_{0}+\\frac{\\lambda}{2n}\\sum_{w}w^{2}\\]Large weights will only be allowed if they considerably improve the first part of the cost function.？？这里不明白\\[\\frac{\\partial C}{\\partial w}=\\frac{\\partial C_{0}}{\\partial w}+\\frac{\\lambda}{n}w\\]\\[\\frac{\\partial C}{\\partial b}=\\frac{\\partial C_{0}}{\\partial b}\\]\\[b\\rightarrow b-\\eta \\frac{\\partial C_{0}}{\\partial b}\\]\\[w\\rightarrow w-\\eta \\frac{\\partial C_{0}}{\\partial w}-\\frac{\\eta\\lambda }{n}w=(1-\\frac{\\eta \\lambda }{n})w-\\eta \\frac{\\partial C_{0}}{\\partial w}\\]后来的测试里面，使用正则都要好过没使用正则的结果为什么L2正则可以减少overfitting，而且得到更好的结果？ 对于L2正则来说，L2减小了w，w越小，每次迭代w的变化就越小，这样即使样本少，数据不均衡，平均不完全，也可以减小陷入到局部最优解的可能性 如果样本多的话，那么学习的目标就多，那么取平均值之后，陷入到少样本的局部最优解里的可能性就小，这是为什么多样本，可以减小overfitting 如果样本多，而且使用L2，那么效果自然就更好Why does regularization help reduce overfitting?One point of view is to say that in science we should go with the simpler explanation, unless compelled not to.正则可以抵抗noise下面的两个例子，再说的是，正则产生的帮助，是无法简单解释。。A network with 100 hidden neurons has nearly 80,000 parameters. We have only 50,000 images in our training data. It’s like trying to fit an 80,000th degree polynomial to 50,000 data points. By all rights, our network should overfit terribly. And yet, as we saw earlier, such a network actually does a pretty good job generalizing.我们的输入是50000个点，模型的参数缺有80000个，所以这样看的话，应该会过拟合严重，但是实验的结果却是一个好的结果，这里是一个无法解释的地方。“the dynamics of gradient descent learning in multilayer nets has a ‘self-regularization’ effect”梯度下降，好像自带正则效果Other techniques for regularization L1 regularization\\[C=C_{0}+\\frac{\\lambda}{n}\\sum_{w}\\left | w \\right |\\]\\(\\frac{\\partial C}{\\partial w}=\\frac{\\partial C_{0}}{\\partial w}+\\frac{\\lambda}{n}sgn(w)\\), sgn(w), w&amp;gt;0:+1 w&amp;lt;0:-1\\[w\\rightarrow w&#39;=w-\\frac{\\eta \\lambda }{n}sgn(w)-\\eta \\frac{\\partial C_{0}}{\\partial w}\\]L0：计算非零个数，用于产生稀疏性，但是在实际研究中很少用，因为L0范数很难优化求解，是一个NP-hard问题，因此更多情况下我们是使用L1范数L1：计算绝对值之和，用以产生稀疏性，因为它是L0范式的一个最优凸近似，容易优化求解L2：计算平方和再取平均数，L2范数更多是防止过拟合，并且让优化求解变得稳定很快速（这是因为加入了L2范式之后，满足了强凸）。L1和L2的区别是，L1使用的固定值的衰减，而L2使用的是w的比例衰减，所以在w比较大的时候，L2衰减更快，在w小的时候，L1衰减更快。The net result is that L1 regularization tends to concentrate the weight of the network in a relatively small number of high-importance connections, while the other weights are driven toward zero.L1倾向于保存比较大的w，而其他的w趋于0。L1产生稀疏性，因为很多小w，被衰减为0。稀疏性的好处是可解释性，即根据非零系数所对应的基的实际意义来解释模型的实际意义，而且可以缩减数据量另外L1，需要注意w=0的点，因为\\[\\left | w \\right |\\]在这点是不可导的，在实际使用的时候，需要额外处理。 dropout每个batch是一个循环 恢复之前dropout的神经元 随机砍掉一半的神经元 正向反向传播，更新参数有一个地方需要注意，由于实际算出来的参数是只有一半的中间神经元，当做评估的时候，需要使用所有的神经元，所以训练时候得到的参数，应该除以2.为什么dropout可以生效？The reason is that the different networks may overfit in different ways, and averaging may help eliminate that kind of overfitting.And so the dropout procedure is like averaging the effects of a very large number of different networks. The different networks will overfit in different ways, and so, hopefully, the net effect of dropout will be to reduce overfitting.因为dropout相当于产生了很多不同模型的网络，每个网络都可能过拟合成不同的方式，而dropout平均化了这些方式，所以可以生效。Dropout has been especially useful in training large, deep networks, where the problem of overfitting is often acute.当网络大且深的时候，防止过拟合就越来越重要 artificially increasing the training set size两个不同的算法AB，可能发生的情况是，在数据集X上，A要好，在数据集Y上，B要好，所以如果有人问，是A好还是B好，那么应该反问，你选择哪个数据集？手写数字识别 基础情况：98.4 加入一些基础扩展，比如旋转等：98.9 加入一个特殊的随机的晃动（模拟手写时候的晃动）：99.3It’s fine to look for better algorithms, but make sure you’re not focusing on better algorithms to the exclusion of easy wins getting more or better training data.寻找好的算法，也找寻找好的数据，一个好的数据，会使达到好成绩变得简单地多。Weight initialization\\[z=\\sum_{j}w_{j}x_{j}+b\\]这个公式，当1000个input，其中500个1,500个0，w和b都采用标准正态分布来初始化，那么，z符合N(0,501)的正态分布，这个正态分布就很平，导致的结果就是，z远大于1或者远小于-1的可能性很大，就是|z|取大值的可能性很大。这时候，如果激活函数是sigmoid的话，那么\\(\\sigma(z)\\)就很接近1，也就是饱和了，学习速率就很低（这个应该是说的是sigmoid的情况了）。We addressed that earlier problem with a clever choice of cost function. Unfortunately, while that helped with saturated output neurons, it does nothing at all for the problem with saturated hidden neurons.之前的更换损失函数从均方差到交叉熵，只是解决了输出层的饱和问题，没法解决中间层的饱和问题！如何避免这个问题呢？可以在初始化的时候，将标准正态分布，换成\\(N(0,1/\\sqrt{n_{in}})\\)，b还是使用的标准正态分布，因为这个有人实验证明过，没有啥影响。实验结果，可以看出来，虽然最后结果一样，但是上升速度要快，并且提前达到最后的96精度However, in Chapter 4 we’ll see examples of neural networks where the long-run behaviour is significantly better with the 1/nin‾‾‾√1/nin weight initialization. Thus it’s not only the speed of learning which is improved, it’s sometimes also the final performance.初始化的值，并非只是影响学习速率，也会影响到最终的结果。L2 regularization sometimes automatically gives us something similar to the new approach to weight initialization.L2正则和优化初始化值的方式，比较相似，都是减小参数Handwriting recognition revisited: the codesave和load使用的是json，也可以使用pickle作者的意思是使用json，可以方便在以后更改代码之后，load数据但是实际上pickle也可以做到这点，以前看过类似的做法How to choose a neural network’s hyper-parameters?开启monitor，这个很重要 Broad strategy 可以从10个分类，减少到两个分类，比如先尝试分类0和1，这样会减少时间 可以减少网络层数，也可以减少时间，后续再增加层数 可以减少训练的数据量，虽然这个可能造成overfitting等，但是现在不是避免这个的时候，现在主要是想测试参数。 前期的时候，能够快速回馈是最重要的事，模型一定要简单，数据也要简单，要记住。 Learning rate0.25在最小点附近有震荡，0.025又太慢了最好的办法是比如前20epochs使用0.25，后面10个epochs使用0.025如何一步步选择rate？首先rate，按照作者的经验是，与训练本身的速率有关，所以作者用的是cost作为判断标准，而其他的参数，比如mini-batch，层数等等，都是用的valiadate数据集的准确率作为判断标准。 先选择一个经验值，比如0.01或者0.1 找到上限值。看训练的前几个epoch，如果cost在下降，那么尝试加大rate，如果上升或者震动，那么就减少rate，这样可以找到一个上限值 用上限值训练，观察图，如果在最低点附近震荡的话，那么就尝试减少一点rate 直到找到一个既迅速又不震荡的rate Use early stopping to determine the number of training epochs在测试集或者验证集上，如果一段时间正确率不上升，就停止训练。这个可以简化epoch的选择，并且减少overfitting。但是在前期，作者并不建议使用这种方案，作者是希望overfitting发生，然后使用正则来处理。如何实现？A better rule is to terminate if the best classification accuracy doesn’t improve for quite some time.比如，从no-improvement-in-ten开始，需要知道的是，有时模型就是会平坦一段时间，这段时间内没有正确率上升，所以这个判断的标准，要根据实际情况来定。 Learning rate schedule学习速率递减，怎么递减？一种方法是，当验证集的正确率下降的时候开始减小学习速率，可以按照 a factor of two or ten（也就是减小到原来的1/2或者1/10），直到减小到1/1024（or 1/1000），就停止减小。需要知道的是，跟early stop一样，这个也增加了额外的参数，也就是增加了需要调整的地方，在前期的时候，没必要这么做，后期追求分数的时候，再这么做。 The regularization parameter, λ最开始的时候，λ=0.0即可，先调整η ,η 调整到一个合适的数值之后，开始调整 λ，可以按照10倍的速率增加或者减少，当确定好 λ之后，再回过头去继续调整η。 How I selected hyper-parameters earlier in this book Mini-batch sizeonline_learning(batch为1)，会导致梯度有问题，但是这个不是一个很要紧的事，因为如下：It’s as though you are trying to get to the North Magnetic Pole, but have a wonky compass that’s 10-20 degrees off each time you look at it. Provided you stop to check the compass frequently, and the compass gets the direction right on average, you’ll end up at the North Magnetic Pole just fine.那是不是online_learning，是最好的选择了呢？因为既可以达到最优点，也频繁得更新参数，不是很好？但是有一点需要注意，因为batch size =1，那么10000个样本的epoch就必须循环10000次，而且没有用到矩阵计算的优势，这样就很慢，学习时间就会长。如果batch size太大，那么参数更新就不够频繁，也不好。所以需要综合起来考虑。所幸的是，batch_size与其他参数的没啥关系，所以只需定下其他参数，单独优化这个就好。所以应该是在最优点一致的情况下，选择一个最大的batch size，使训练时间最短。 Automated techniques自动优化参数 Summing up有一些paper介绍了如何选择参数选择参数这个问题，现在也没有解决，没有一个统一的方式So your goal should be to develop a workflow that enables you to quickly do a pretty good job on the optimization, while leaving you the flexibility to try more detailed optimizations, if that’s important.所以就是定下一个自己的流程，这个流程应该可以做出一个比较好的参数，然后再细致得调整。Other techniquesVariations on stochastic gradient descent（变种随机梯度下降） Hessian techniqueIntuitively, the advantage Hessian optimization has is that it incorporates not just information about the gradient, but also information about how the gradient is changing. momentumOther approaches to minimizing the cost functionAs you go deeper into neural networks it’s worth digging into the other techniques, understanding how they work, their strengths and weaknesses, and how to apply them in practice.Other models of artificial neuron tanh，具体公式不记录了，这个与sigmoid类似，只是output从0,1到了-1,1一般认为tanh要比sigmoid要好，因为sigmoid由于是非负的，所以对一个神经元来说，所有的w同时上升或者下降，这个不太贴合实际。而tanh，就可以避免这个问题。但是现在实验起来，tanh比sigmoid又没有多少进步或者进步很小。 rectified linear neuron（就是relu）image recognition上面用的比较多，并且比较有效。这种激活函数，没有sigmoid类似函数的饱和问题。network2 code 增加了cross-entropy 增加了权重衰减 增加了Save load 增加了monitor # 均方差0.5*np.linalg.norm(a-y)**2# 交叉熵np.sum(np.nan_to_num(-y*np.log(a)-(1-y)*np.log(1-a))) # nan_to_num，如果是nan则换为0，如果是infinite，则换为inf " }, { "title": "Neural Networks And Deep Learning Chap2", "url": "/posts/Neural_Networks_And_Deep_Learning_Chap2/", "categories": "study", "tags": "", "date": "2018-02-23 00:00:00 +0800", "snippet": "原文地址Warm up: a fast matrix-based approach to computing the output from a neural networkThe two assumptions we need about the cost function两个假设： \\[C=\\frac{1}{2n}\\sum_x \\left \\| y(x)-a^{L}(x) \\right \\|^{2}​\\] the cost function can be written as an average \\(C=\\frac{1}{n}\\sum_{x}C_{x}\\)over cost functions Cx for individual training examples, x.\\[C_{x}=\\frac{1}{2}\\left\\| y-a^{L} \\right\\|^{2}\\] the cost is that it can be written as a function of the outputs from the neural network \\[C_{x}=\\frac{1}{2}\\left \\| y-a^{L}\\right \\|^{2}=\\frac{1}{2}\\sum_{j}(y_{j}-a_{L}^{j})^{2}\\] 总体的Cost，依赖于所有的输入x（对于）和每个输入x的所有的output aThe Hadamard product, s⊙t# 与矩阵乘法不同# 这个的写法是a*b# 矩阵乘法的写法是np.dot(a, b)The four fundamental equations behind backpropagation一些基础点： \\[\\delta_{j}^{l}\\equiv \\frac{\\partial C}{\\partial z_{j}^{l}}\\] the error in the \\(j_{th}\\) neuron in the \\(l_{th}\\) layeradd a little\\(\\bigtriangleup z_{j}^{l}\\), so output \\(\\sigma(z_{j}^{l}) \\rightarrow\\sigma(z_{j}^{l}+\\bigtriangleup z_{j}^{l})\\) 最终的output为\\(\\frac{\\partial C}{\\partial z_{j}^{l}}\\bigtriangleup z_{j}^{l}\\) \\(z_{j}^{L}\\)并不是神经元输出，\\(\\sigma(z_{j}^{L})\\)才是第一个基础公式(An equation for the error in the output layer)：下面的推导是针对output层来说\\[\\delta_{j}^{L}=\\frac{\\partial C}{\\partial z_{j}^{L}}\\rightarrow \\sum_{k}\\frac{\\partial C}{\\partial a_{k}^{L}}\\frac{\\partial a_{k}^{L}}{\\partial z_{j}^{L}}\\rightarrow \\frac{\\partial C}{\\partial a_{j}^{L}}\\frac{\\partial a_{j}^{L}}{\\partial z_{j}^{L}}\\rightarrow \\frac{\\partial C}{\\partial a_{j}^{L}}\\sigma&#39;(z_{j}^{L})\\]Of course, the output activation \\(a_{k}^{L}\\) of the \\(k^{th}\\) neuron depends only on the weighted input \\(z_{j}^{L}\\) for the \\(j^{th}\\) neuron when k=j.现在讨论的是输出层的状态，所以\\(a_{j}\\)（也就是\\(\\sigma\\)）只与\\(z_{j}\\)有关。\\[C=\\frac{1}{2}\\sum_{j}(y_{j}-a_{j}^{L})^{2} \\rightarrow \\partial C/\\partial a_{j}^{L}=(a_{j}^{L}-y_{j})\\]\\[(y_{j}-a_{j}^{L})(y_{j}-a_{j}^{L})&#39;= (a_{j}^{L}-y_{j})\\]\\[\\delta^{L}=\\bigtriangledown_{a}C\\odot \\sigma &#39;(z^{L})\\]当loss是均方差的时候，可以化简为下面这个\\[\\delta^{L}=(a^{L}-y)\\odot \\sigma &#39;(z^{L})\\]这里的⊙需要注意下，这个不是一般的矩阵乘法第二个公式（An equation for the error δlδl in terms of the error in the next layer）\\[\\delta^{L}=((w^{l+1})^{T}\\delta ^{l+1})\\odot \\sigma &#39;(z^{L})\\]推导的过程文章下面介绍很详细第三个公式（An equation for the rate of change of the cost with respect to any bias in the network）\\[\\frac{\\partial C}{\\partial b^l_j}=\\delta _j^l\\]第四个公式（An equation for the rate of change of the cost with respect to any weight in the network）\\[\\frac{\\partial C}{\\partial w_{jk}^{l}}=a_{k}^{l-1}\\delta_{j}^{l}\\rightarrow \\frac{\\partial C}{\\partial w}=a_{in}\\delta _{out}\\]A nice consequence of Equation (32) is that when the activation \\(a_{in}\\) is small, \\(a_{in}\\)≈0, the gradient term ∂C/∂w will also tend to be small.In this case, we’ll say the weight learns slowly, meaning that it’s not changing much during gradient descent.当\\(a_{in}\\)很小的时候，参数的学习很慢so the lesson is that a weight in the final layer will learn slowly if the output neuron is either low activation (≈0) or high activation (≈1). In this case it’s common to say the output neuron has saturated and, as a result, the weight has stopped learning (or is learning slowly).当使用sigmoid激活函数的时候，如果神经元的输出接近0或1的时候，学习也很慢Summing up, we’ve learnt that a weight will learn slowly if either the input neuron is low-activation, or if the output neuron has saturated, i.e., is either high- or low-activation.Fully matrix-based approach to backpropagation over a mini-batch这是一种提升速度的方式，在tensorflow里面，应该已经在使用了。可以试试在tensorflow里面，这个速度快多少？写了一份代码，跑起来使用GPU+tensorflow，速度也没有提升太多（比如10倍）0:04:00.804773 原来的0:02:45.739480 现在的代码：mnist_tensor_mine/src/tensor_network.pyIn what sense is backpropagation a fast algorithm?如之前视频里讲解的一样，反向传播，将需要计算量大为缩减，与正向传播基本计算量相等。但是这个是有前提的，就是现在计算的是，大量的输入，一个输出（比如就是是否符合预期），反向传播是适用的，但是有的模型是，一个输入，大量的输出，这时，正向传播才是合适的Backpropagation: the big picture转换成tensorflow，之后的测试情况有一些准备过程： 参照这篇文档来做，数据格式需要处理Batch_size的影响 Batch_Size 太小，算法在 200 epoches 内不收敛。 随着 Batch_Size 增大，处理相同数据量的速度越快。 随着 Batch_Size 增大，达到相同精度所需要的 epoch 数量越来越多。 由于上述两种因素的矛盾， Batch_Size 增大到某个时候，达到时间上的最优。 由于最终收敛精度会陷入不同的局部极值，因此 Batch_Size 增大到某些时候，达到最终收敛精度上的最优。" }, { "title": "Neural Networks And Deep Learning Chap1", "url": "/posts/Neural_Networks_And_Deep_Learning_Chap1/", "categories": "study", "tags": "", "date": "2018-02-22 00:00:00 +0800", "snippet": "原文地址perceptrons（感知器）1950s-1960s by scientist Frank Rosenblatt数学模型：所有权重参数为w1,w2…，thresholdthreshold，Dropping the threshold means you’re more willing to go to the festival.与权重b是同一种意思，一个表示偏好的权重感知机的输入只是0或1，输出也是0或1两处简化\\[\\sum_{j} w_{j}x_{j} \\equiv w\\cdot x\\]\\[b \\equiv -threshold\\]这个bias代表的是how easy it is to get the perceptron to fireperceptron的两种用途： a method for weighing evidence to make decisions compute simple logical functions, depend on NAND gateperceptrons能实现与非门，但是并不是仅仅的只是实现另一种集成电路，与传统的基于与非门的集成电路不同的是，perceptrons能够通过自动调节weights和biases，来自动学习解决问题sigmoid neuronsIn fact, a small change in the weights or bias of any single perceptron in the network can sometimes cause the output of that perceptron to completely flip, say from 00 to 11.sigmoid：Sigmoid neurons are similar to perceptrons, but modified so that small changes in their weights and bias cause only a small change in their output.perceptrons的问题是，它是阶梯变化的，不线性，微小的变化不能很好的表现出来。sigmoid输出0-1之间的任意数，σ(w⋅x+b)\\[\\sigma (z) \\equiv \\frac{1}{1+e^{-z}}\\]\\[\\sigma (z) \\equiv \\frac{1}{1+e^{-\\sum_{j}w_{j}x_{j}-b}}\\]sigmoid的一个比perceptrons不方便的地方是，output不是0或1，而是一个介于0-1之间的数，就指示一个对错的百分比而已perceptrons和sigmoid的一些特性 perceptrons的w b乘以任意正数c，不影响输出，sigmoid就会有影响 实际上sigmoid如果乘以c，且c趋于正无穷，那么sigmoid与perceptrons就一样了MLPs：multilayer perceptrons我们一般使用的是feedforward网络，也就是只有前进没有后退的还有一种是Recurrent neural nets，这里包含loops，这个loops持续一段被限制的时间，不是一直循环下去，神经元本身会受到之前的激发状态的影响或者受到之前的输入的影响，Recurrent neural nets，更贴近人类的大脑手写数字处理output 10和4，为什么会导致识别准确率有区别？我个人的理解是，hidden层是基于第一层输入图像的提取结果，那么这个结果是基于图像的。而4这种方式，是基于数字的，在这层来做不合适。在练习的部分，有提到，如果加入第三层的话，那么这层可以使用4output，因为相当于是将之前的10个数字的概率，这种基于数字的提取成另外一种数字，这样成功率应该比较高。梯度递减Why not try to maximize that number directly, rather than minimizing a proxy measure like the quadratic cost?我的理解是，如果只关注结果的数字的最大化的话，那么做了些许的改变w和b，对预测结果来说，可能根本就没有变化，那么就不知道怎么继续提高分数了，这就是说这个不是一个平滑的方法。而如果关注的是所有结果的数字的均方差的最小化的话，这是一个平滑的方法，可以进行学习。均方差：\\[\\frac{1}{2n}\\sum \\left \\| y(x)-a \\right \\|^{2}\\]梯度下降：\\[\\Delta C \\approx \\bigtriangledown C\\cdot \\Delta v\\]\\[\\bigtriangledown C\\equiv (\\frac{\\partial C}{\\partial v_{1}},...,\\frac{\\partial C}{\\partial v_{m}})^{T}\\]\\[\\Delta v=-\\eta \\bigtriangledown C\\]stochastic gradient descent，随机梯度递减\\[\\bigtriangledown C = \\frac{1}{n} \\sum_{x} \\bigtriangledown C_{x}\\]In practice, to compute the gradient ∇C we need to compute the gradients ∇Cx separately for each training input, x, and then average them, ∇C=1n∑x∇Cx一种极端的mini-batch为1的训练，这种应用于online on-line incremental learning，就是随机梯度下降，类似于人类学习的过程创建网络模型从文章提供的git下载的代码，在3.5下使用需要更改几个地方参考这篇 patchclass Network(object): def __init__(self, sizes): self.num_layers = len(sizes) self.sizes = sizes self.biases = [np.random.randn(y, 1) for y in sizes[1:]] self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]这里使用的是随机高斯分布（正态分布）来初始化weights和biases测试下隐含层的神经元数量的影响：# Epoch 29: 9384 / 10000# train time: 0:03:17.5953027# Epoch 29: 9418 / 10000# train time: 0:03:28.126904net = network.Network([784, 20, 10])net.SGD(training_data, 30, 10, 3.0, test_data=test_data)# Epoch 29: 9503 / 10000# train time: 0:04:12.572446net = network.Network([784, 30, 10])net.SGD(training_data, 30, 10, 3.0, test_data=test_data)# Epoch 29: 9575 / 10000# train time: 0:05:39.106396net = network.Network([784, 50, 10])net.SGD(training_data, 30, 10, 3.0, test_data=test_data)# Epoch 29: 8537 / 10000# train time: 0:07:48.387790# Epoch 29: 9599 / 10000# train time: 0:07:44.628575net = network.Network([784, 80, 10])net.SGD(training_data, 30, 10, 3.0, test_data=test_data)从结果看，隐含层神经元越多，测试结果一般越好，但是花费时间越多。隐含层神经元越多，意味着从input层获取到的Feature的种类就越多，对于预测越有帮助，但是由于神经元越多，参数也就越多，所以花费时间长。最后一次实验（对应中间层是80），结果不稳定，低的时候只有85%，可能是陷入了局部最优解，但是20的情况，我测试多次也没有出现不稳定的情况。所以看来，模型越复杂，精度可能会越好，但是越有可能不稳定（陷入局部最优、过拟合、梯度爆炸/消失等）。测试下batch-size的影响：# Epoch 29: 8932 / 10000# train time: 0:04:17.280716net = network.Network([784, 20, 10])net.SGD(training_data, 30, 1, 3.0, test_data=test_data)# Epoch 29: 9384 / 10000# train time: 0:03:17.5953027net = network.Network([784, 20, 10])net.SGD(training_data, 30, 10, 3.0, test_data=test_data)# Epoch 29: 9376 / 10000# train time: 0:03:11.990982net = network.Network([784, 20, 10])net.SGD(training_data, 30, 20, 3.0, test_data=test_data)# Epoch 29: 9383 / 10000# train time: 0:02:57.594158net = network.Network([784, 20, 10])net.SGD(training_data, 30, 50, 3.0, test_data=test_data)# Epoch 29: 9270 / 10000# train time: 0:03:08.102759net = network.Network([784, 20, 10])net.SGD(training_data, 30, 100, 3.0, test_data=test_data)从结果看，随着size增加，整体来看，相同epoch的时间在下降，精度也有下降趋势。所以这么看的话，size应该有个最优值，在这点上，训练时间和精度会达到最优。另外一个现象是，在Size=1的情况下，模型没有收敛到应该有的水平。可能是由于，只根据一个样本来更新参数，整体波动比较大的原因Toward deep learningIn the early days of AI research people hoped that the effort to build an AI would also help us understand the principles behind intelligence and, maybe, the functioning of the human brain. But perhaps the outcome will be that we end up understanding neither the brain nor how artificial intelligence works我们人类的目的是想建立一个AI来帮助我们理解智能背后的原理，比如人类的大脑，但是讽刺的是，实际的结果是，我们造出来了AI，但是发现我们既不理解大脑也不理解AI。MINIST数据集到图像对于这本书里的数据集，可以在mnist_loader.py里，加入如下代码def restore_image(): from PIL import Image tr_d, va_d, te_d = load_data() for i, img in enumerate(tr_d[0][:1]): a = np.reshape(img, (28, 28)) new_img = Image.fromarray(a, &#39;L&#39;) new_img.show()if __name__ == &quot;__main__&quot;: restore_image()fromarray的mode可以参考下表取到的图片（这个不是数字，应该是这个图片有预处理，是从pickle里拿出来的）对于MNIST的真正数据集，train-images-idx3-ubyte，用如下方法提取from PIL import Imageimport numpyimport gzipdef _read32(bytestream): dt = numpy.dtype(numpy.uint32).newbyteorder(&#39;&amp;gt;&#39;) return numpy.frombuffer(bytestream.read(4), dtype=dt)[0]def extract_images(f): with gzip.GzipFile(fileobj=f) as bytestream: magic = _read32(bytestream) if magic != 2051: raise ValueError(&#39;Invalid magic number %d in MNIST image file: %s&#39; % (magic, f.name)) num_images = _read32(bytestream) rows = _read32(bytestream) cols = _read32(bytestream) buf = bytestream.read(rows * cols * num_images) data = numpy.frombuffer(buf, dtype=numpy.uint8) data = data.reshape(num_images, rows, cols, 1) return datawith open(&#39;train-images-idx3-ubyte.gz&#39;, &#39;rb&#39;) as f: images = extract_images(f) for i in range(10): img = Image.fromarray(numpy.reshape(images[i], (28,28)), &#39;L&#39;) img.save(&#39;/home/cooli7wa/Desktop/%s.png&#39;%i)" } ]
